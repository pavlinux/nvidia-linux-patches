diff -ur a/kernel/common/inc/nv-mm.h b/kernel/common/inc/nv-mm.h
--- a/kernel/common/inc/nv-mm.h	2016-11-16 02:53:45.000000000 +0300
+++ b/kernel/common/inc/nv-mm.h	2017-01-27 03:45:03.774116776 +0300
@@ -57,7 +57,7 @@
                                              struct page **pages,
                                              struct vm_area_struct **vmas)
         {
-            unsigned int flags = 0;
+            unsigned int flags = 0u;
 
             if (write)
                 flags |= FOLL_WRITE;
@@ -76,14 +76,14 @@
                                                     struct page **pages,
                                                     struct vm_area_struct **vmas)
         {
-            unsigned int flags = 0;
+            unsigned int flags = 0u;
 
             if (write)
                 flags |= FOLL_WRITE;
             if (force)
                 flags |= FOLL_FORCE;
 
-            return get_user_pages_remote(tsk, mm, start, nr_pages, flags, pages, vmas);
+            return get_user_pages_remote(tsk, mm, start, nr_pages, flags, pages, vmas, NULL);
         }
     #endif
 #else
diff -ur a/kernel/nvidia/ibmnpu_linux.c b/kernel/nvidia/ibmnpu_linux.c
--- a/kernel/nvidia/ibmnpu_linux.c	2016-11-16 02:44:18.000000000 +0300
+++ b/kernel/nvidia/ibmnpu_linux.c	2016-12-29 16:06:47.344272978 +0300
@@ -565,7 +565,7 @@
             (NV_PCI_RESOURCE_FLAGS(dev, i) & PCI_BASE_ADDRESS_SPACE)
                 == PCI_BASE_ADDRESS_SPACE_MEMORY)
         {
-            NvU32 bar = 0;
+            NvU32 bar = 0u;
 
             info->bars[j].offset = NVRM_PCICFG_BAR_OFFSET(i);
             pci_read_config_dword(dev, info->bars[j].offset, &bar);
@@ -708,7 +708,7 @@
     pci_set_drvdata(handle, NULL);
     pci_release_regions(handle);
 
-    for (bar = 0; bar < IBMNPU_MAX_BARS; bar++)
+    for (bar = 0u; bar < IBMNPU_MAX_BARS; bar++)
     {
         if (NULL != info->bars[bar].pBar)
         {
@@ -739,7 +739,7 @@
 
 NvU8 NVLINK_API_CALL ibmnpu_lib_pci_read_08 (void *handle, NvU32 offset)
 {
-    NvU8 buffer = 0xFF;
+    NvU8 buffer = 0xFFu;
     if (NULL == handle || offset > NV_PCIE_CFG_MAX_OFFSET)
     {
         return buffer;
@@ -751,7 +751,7 @@
 
 NvU16 NVLINK_API_CALL ibmnpu_lib_pci_read_16 (void *handle, NvU32 offset)
 {
-    NvU16 buffer = 0xFFFF;
+    NvU16 buffer = 0xFFFFu;
     if (NULL == handle || offset > NV_PCIE_CFG_MAX_OFFSET)
     {
         return buffer;
@@ -763,7 +763,7 @@
 
 NvU32 NVLINK_API_CALL ibmnpu_lib_pci_read_32 (void *handle, NvU32 offset)
 {
-    NvU32 buffer = 0xFFFFFFFF;
+    NvU32 buffer = 0xFFFFFFFFu;
     if (NULL == handle || offset > NV_PCIE_CFG_MAX_OFFSET)
     {
         return buffer;
@@ -866,17 +864,17 @@
 
 NvU8  NVLINK_API_CALL ibmnpu_lib_pci_read_08 (void *handle, NvU32 offset)
 {
-    return 0;
+    return 0u;
 }
 
 NvU16 NVLINK_API_CALL ibmnpu_lib_pci_read_16 (void *handle, NvU32 offset)
 {
-    return 0;
+    return 0u;
 }
 
 NvU32 NVLINK_API_CALL ibmnpu_lib_pci_read_32 (void *handle, NvU32 offset)
 {
-    return 0;
+    return 0u;
 }
 
 void  NVLINK_API_CALL ibmnpu_lib_pci_write_08(void *handle, NvU32 offset, NvU8  data)
diff -ur a/kernel/nvidia/nv.c b/kernel/nvidia/nv.c
--- a/kernel/nvidia/nv.c	2016-11-16 02:53:45.000000000 +0300
+++ b/kernel/nvidia/nv.c	2016-12-29 16:17:24.167030583 +0300
@@ -37,7 +37,7 @@
 
 #if (NV_BUILD_MODULE_INSTANCES != 0)
 #if defined(MODULE_LICENSE)
-MODULE_LICENSE("NVIDIA");
+MODULE_LICENSE("GPLv2");
 #endif
 #if defined(MODULE_INFO)
 MODULE_INFO(supported, "external");
@@ -56,11 +56,11 @@
  * our global state; one per device
  */
 
-static NvU32 num_nv_devices = 0;
-NvU32 num_probed_nv_devices = 0;
+static NvU32 num_nv_devices = 0u;
+NvU32 num_probed_nv_devices = 0u;
 
-NvU32 nv_assign_gpu_count = 0;
-nv_pci_info_t nv_assign_gpu_pci_info[NV_MAX_DEVICES];
+NvU32 nv_assign_gpu_count = 0u;
+nv_pci_info_t nv_assign_gpu_pci_info[NV_MAX_DEVICES] = {{ 0 }};
 
 nv_linux_state_t *nv_linux_devices;
 
@@ -189,7 +189,9 @@
     {
         nv_aperture_t *tmp = &nv->bars[bar];
 
-        bar_lo = bar_hi = 0;
+	bar_lo = 0u;
+	bar_hi = 0u;
+
         if (tmp->offset == 0)
             continue;
 
@@ -338,14 +340,14 @@
 
 #if defined(VM_CHECKER)
 /* kernel virtual memory usage/allocation information */
-NvU32 vm_usage = 0;
+NvU32 vm_usage = 0u;
 struct mem_track_t *vm_list = NULL;
 nv_spinlock_t vm_lock;
 #endif
 
 #if defined(KM_CHECKER)
 /* kernel logical memory usage/allocation information */
-NvU32 km_usage = 0;
+NvU32 km_usage = 0u;
 struct mem_track_t *km_list = NULL;
 nv_spinlock_t km_lock;
 #endif
@@ -404,10 +406,7 @@
     return at;
 }
 
-static
-int nvos_free_alloc(
-    nv_alloc_t *at
-)
+static int nvos_free_alloc(nv_alloc_t *at)
 {
     unsigned int i;
 
@@ -417,7 +416,7 @@
     if (NV_ATOMIC_READ(at->usage_count))
         return 1;
 
-    for (i = 0; i < at->num_pages; i++)
+    for (i = 0u; i < at->num_pages; i++)
     {
         if (at->page_table[i] != NULL)
             NV_KMEM_CACHE_FREE(at->page_table[i], nvidia_pte_t_cache);
@@ -431,13 +430,13 @@
 
 NvU8 nv_find_pci_capability(struct pci_dev *dev, NvU8 capability)
 {
-    u16 status = 0;
-    u8  cap_ptr = 0, cap_id = 0xff;
+    u16 status  = 0u;
+    u8  cap_ptr = 0u, cap_id = 0xff;
 
     pci_read_config_word(dev, PCI_STATUS, &status);
     status &= PCI_STATUS_CAP_LIST;
     if (!status)
-        return 0;
+        return 0u;
 
     switch (dev->hdr_type) {
         case PCI_HEADER_TYPE_NORMAL:
@@ -445,7 +444,7 @@
             pci_read_config_byte(dev, PCI_CAPABILITY_LIST, &cap_ptr);
             break;
         default:
-            return 0;
+            return 0u;
     }
 
     do {
@@ -456,7 +455,7 @@
         pci_read_config_byte(dev, cap_ptr + PCI_CAP_LIST_NEXT, &cap_ptr);
     } while (cap_ptr && cap_id != 0xff);
 
-    return 0;
+    return 0u;
 }
 
 static void nv_state_init_gpu_uuid_cache(nv_state_t* nv)
@@ -538,7 +537,7 @@
 int nv_verify_cpa_interface(void)
 {
     unsigned int i, size;
-    unsigned long large_page = 0;
+    unsigned long large_page = 0ul;
     unsigned long *vaddr_list;
     size = sizeof(unsigned long) * CPA_FIXED_MAX_ALLOCS;
 
@@ -554,7 +553,7 @@
     memset(vaddr_list, 0, size);
 
     /* try to track down an allocation from a 2M page. */
-    for (i = 0; i < CPA_FIXED_MAX_ALLOCS; i++)
+    for (i = 0u; i < CPA_FIXED_MAX_ALLOCS; i++)
     {
         vaddr_list[i] =  __get_free_page(GFP_KERNEL);
         if (!vaddr_list[i])
@@ -562,7 +561,7 @@
 
 #if defined(_PAGE_NX)
         if ((pgprot_val(PAGE_KERNEL) & _PAGE_NX) &&
-                virt_to_phys((void *)vaddr_list[i]) < 0x400000)
+                virt_to_phys((void *)vaddr_list[i]) < 0x400000u)
             continue;
 #endif
 
diff -ur a/kernel/nvidia/nv-dma.c b/kernel/nvidia/nv-dma.c
--- a/kernel/nvidia/nv-dma.c	2016-11-16 02:53:45.000000000 +0300
+++ b/kernel/nvidia/nv-dma.c	2016-12-29 16:07:54.900364484 +0300
@@ -29,6 +29,7 @@
 {
     *va = pci_map_page(dma_map->dev, dma_map->pages[0], 0,
             dma_map->page_count * PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+
     if (NV_PCI_DMA_MAPPING_ERROR(dma_map->dev, *va))
     {
         return NV_ERR_OPERATING_SYSTEM;
@@ -76,7 +77,7 @@
         sg_set_page(sg, pages[i], PAGE_SIZE, 0);
     }
 #else
-    for (i = 0; i < page_count; i++)
+    for (i = 0u; i < page_count; i++)
     {
         sg = &(sgl)[i];
         sg->page = pages[i];
@@ -111,7 +112,7 @@
     WARN_ON(NvU64_HI32(num_submaps) != 0);
 
     dma_map->mapping.discontig.submap_count = NvU64_LO32(num_submaps);
-    
+
     status = os_alloc_mem((void **)&dma_map->mapping.discontig.submaps,
         sizeof(nv_dma_submap_t) * dma_map->mapping.discontig.submap_count);
     if (status != NV_OK)
diff -ur a/kernel/nvidia/nv-frontend.c b/kernel/nvidia/nv-frontend.c
--- a/kernel/nvidia/nv-frontend.c	2016-11-16 02:53:45.000000000 +0300
+++ b/kernel/nvidia/nv-frontend.c	2016-12-29 15:56:21.712089819 +0300
@@ -15,7 +15,7 @@
 #include "nv-frontend.h"
 
 #if defined(MODULE_LICENSE)
-MODULE_LICENSE("NVIDIA");
+MODULE_LICENSE("GPLv2");
 #endif
 #if defined(MODULE_INFO)
 MODULE_INFO(supported, "external");
diff -ur a/kernel/nvidia/nv_gpu_ops.h b/kernel/nvidia/nv_gpu_ops.h
--- a/kernel/nvidia/nv_gpu_ops.h	2016-11-16 02:53:40.000000000 +0300
+++ b/kernel/nvidia/nv_gpu_ops.h	2016-12-29 16:08:57.182932013 +0300
@@ -29,10 +29,10 @@
 // should be OK when user is not sure which pagesize allocation it wants
 //
 #define PAGE_SIZE_DEFAULT    0x0
-#define PAGE_SIZE_4K         0x1000
-#define PAGE_SIZE_64K        0x10000
-#define PAGE_SIZE_128K       0x20000
-#define PAGE_SIZE_2M         0x200000
+#define PAGE_SIZE_4K         0x1000u
+#define PAGE_SIZE_64K        0x10000u
+#define PAGE_SIZE_128K       0x20000u
+#define PAGE_SIZE_2M         0x200000u
 
 #define GPU_PAGE_LEVEL_INFO_MAX_LEVELS         5
 #define GPU_PAGE_LEVEL_INFO_LEVEL_4K           0
diff -ur a/kernel/nvidia/nv-p2p.c b/kernel/nvidia/nv-p2p.c
--- a/kernel/nvidia/nv-p2p.c	2016-11-16 02:53:45.000000000 +0300
+++ b/kernel/nvidia/nv-p2p.c	2017-01-27 03:56:03.124569031 +0300
@@ -146,7 +146,7 @@
 int nvidia_p2p_get_pages(
     uint64_t p2p_token,
     uint32_t va_space,
-    uint64_t virtual_address,
+    uint64_t address,
     uint64_t length,
     struct nvidia_p2p_page_table **page_table,
     void (*free_callback)(void * data),
@@ -211,7 +211,7 @@
     }
 
     status = rm_p2p_get_pages(sp, p2p_token, va_space,
-            virtual_address, length, physical_addresses, wreqmb_h,
+            address, length, physical_addresses, wreqmb_h,
             rreqmb_h, &entries, &gpu_uuid, *page_table,
             free_callback, data);
     if (status != NV_OK)
@@ -286,7 +286,7 @@
 
     if (bGetPages)
     {
-        rm_p2p_put_pages(sp, p2p_token, va_space, virtual_address,
+        rm_p2p_put_pages(sp, p2p_token, va_space, address,
                 gpu_uuid, *page_table);
     }
 
@@ -329,7 +329,7 @@
 int nvidia_p2p_put_pages(
     uint64_t p2p_token,
     uint32_t va_space,
-    uint64_t virtual_address,
+    uint64_t address,
     struct nvidia_p2p_page_table *page_table
 )
 {
@@ -343,7 +343,7 @@
         return rc;
     }
 
-    status = rm_p2p_put_pages(sp, p2p_token, va_space, virtual_address,
+    status = rm_p2p_put_pages(sp, p2p_token, va_space, address,
             page_table->gpu_uuid, page_table);
     if (status == NV_OK)
         nvidia_p2p_free_page_table(page_table);
diff -ur a/kernel/nvidia-drm/nvidia-drm-fence.c b/kernel/nvidia-drm/nvidia-drm-fence.c
--- a/kernel/nvidia-drm/nvidia-drm-fence.c	2016-11-16 02:44:48.000000000 +0300
+++ b/kernel/nvidia-drm/nvidia-drm-fence.c	2017-01-27 04:04:30.766660580 +0300
@@ -31,7 +31,7 @@
 
 #if defined(NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ)
 struct nv_fence {
-    struct fence base;
+    struct dma_fence base;
     spinlock_t lock;
 
     struct nvidia_drm_device *nv_dev;
@@ -51,7 +51,7 @@
 
 static const char *nvidia_drm_gem_prime_fence_op_get_driver_name
 (
-    struct fence *fence
+    struct dma_fence *fence
 )
 {
     return "NVIDIA";
@@ -59,7 +59,7 @@
 
 static const char *nvidia_drm_gem_prime_fence_op_get_timeline_name
 (
-    struct fence *fence
+    struct dma_fence *fence
 )
 {
     return "nvidia.prime";
@@ -67,7 +67,7 @@
 
 static bool nvidia_drm_gem_prime_fence_op_signaled
 (
-    struct fence *fence
+    struct dma_fence *fence
 )
 {
     struct nv_fence *nv_fence = container_of(fence, struct nv_fence, base);
@@ -99,7 +99,7 @@
 
 static bool nvidia_drm_gem_prime_fence_op_enable_signaling
 (
-    struct fence *fence
+    struct dma_fence *fence
 )
 {
     bool ret = true;
@@ -107,7 +107,7 @@
     struct nvidia_drm_gem_object *nv_gem = nv_fence->nv_gem;
     struct nvidia_drm_device *nv_dev = nv_fence->nv_dev;
 
-    if (fence_is_signaled(fence))
+    if (dma_fence_is_signaled(fence))
     {
         return false;
     }
@@ -132,7 +132,7 @@
     }
 
     nv_gem->fenceContext.softFence = fence;
-    fence_get(fence);
+    dma_fence_get(fence);
 
 unlock_struct_mutex:
     mutex_unlock(&nv_dev->dev->struct_mutex);
@@ -142,7 +142,7 @@
 
 static void nvidia_drm_gem_prime_fence_op_release
 (
-    struct fence *fence
+    struct dma_fence *fence
 )
 {
     struct nv_fence *nv_fence = container_of(fence, struct nv_fence, base);
@@ -151,7 +151,7 @@
 
 static signed long nvidia_drm_gem_prime_fence_op_wait
 (
-    struct fence *fence,
+    struct dma_fence *fence,
     bool intr,
     signed long timeout
 )
@@ -166,12 +166,12 @@
      * that it should never get hit during normal operation, but not so long
      * that the system becomes unresponsive.
      */
-    return fence_default_wait(fence, intr,
-                              (timeout == MAX_SCHEDULE_TIMEOUT) ?
-                                  msecs_to_jiffies(96) : timeout);
+    return dma_fence_default_wait(fence, intr,
+                                 (timeout == MAX_SCHEDULE_TIMEOUT) ?
+                                   msecs_to_jiffies(96) : timeout);
 }
 
-static const struct fence_ops nvidia_drm_gem_prime_fence_ops = {
+static const struct dma_fence_ops nvidia_drm_gem_prime_fence_ops = {
     .get_driver_name = nvidia_drm_gem_prime_fence_op_get_driver_name,
     .get_timeline_name = nvidia_drm_gem_prime_fence_op_get_timeline_name,
     .signaled = nvidia_drm_gem_prime_fence_op_signaled,
@@ -281,7 +281,7 @@
     bool force
 )
 {
-    struct fence *fence = nv_gem->fenceContext.softFence;
+    struct dma_fence *fence = nv_gem->fenceContext.softFence;
 
     WARN_ON(!mutex_is_locked(&nv_dev->dev->struct_mutex));
 
@@ -297,10 +297,10 @@
 
         if (force || nv_fence_ready_to_signal(nv_fence))
         {
-            fence_signal(&nv_fence->base);
+            dma_fence_signal(&nv_fence->base);
 
             nv_gem->fenceContext.softFence = NULL;
-            fence_put(&nv_fence->base);
+            dma_fence_put(&nv_fence->base);
 
             nvKms->disableChannelEvent(nv_dev->pDevice,
                                        nv_gem->fenceContext.cb);
@@ -316,7 +316,7 @@
 
         nv_fence = container_of(fence, struct nv_fence, base);
 
-        fence_signal(&nv_fence->base);
+        dma_fence_signal(&nv_fence->base);
     }
 }
 
@@ -509,7 +509,7 @@
      * fence_context_alloc() cannot fail, so we do not need to check a return
      * value.
      */
-    nv_gem->fenceContext.context = fence_context_alloc(1);
+    nv_gem->fenceContext.context = dma_fence_context_alloc(1);
 
     ret = nvidia_drm_gem_prime_fence_import_semaphore(
               nv_dev, nv_gem, p->index,
@@ -666,13 +666,13 @@
     nv_fence->nv_gem = nv_gem;
 
     spin_lock_init(&nv_fence->lock);
-    fence_init(&nv_fence->base, &nvidia_drm_gem_prime_fence_ops,
+    dma_fence_init(&nv_fence->base, &nvidia_drm_gem_prime_fence_ops,
                &nv_fence->lock, nv_gem->fenceContext.context,
                p->sem_thresh);
 
     reservation_object_add_excl_fence(&nv_gem->fenceContext.resv,
                                       &nv_fence->base);
-    fence_put(&nv_fence->base); /* Reservation object has reference */
+    dma_fence_put(&nv_fence->base); /* Reservation object has reference */
 
     ret = 0;
 
diff -ur a/kernel/nvidia-drm/nvidia-drm-gem.h b/kernel/nvidia-drm/nvidia-drm-gem.h
--- a/kernel/nvidia-drm/nvidia-drm-gem.h	2016-11-16 02:44:48.000000000 +0300
+++ b/kernel/nvidia-drm/nvidia-drm-gem.h	2017-01-27 04:02:02.172251153 +0300
@@ -98,7 +98,7 @@
         /* Software signaling structures */
         struct NvKmsKapiChannelEvent *cb;
         struct nvidia_drm_gem_prime_soft_fence_event_args *cbArgs;
-        struct fence *softFence; /* Fence for software signaling */
+        struct dma_fence *softFence; /* Fence for software signaling */
     } fenceContext;
 #endif
 };
diff -ur a/kernel/nvidia-drm/nvidia-drm-linux.c b/kernel/nvidia-drm/nvidia-drm-linux.c
--- a/kernel/nvidia-drm/nvidia-drm-linux.c	2016-11-16 02:44:48.000000000 +0300
+++ b/kernel/nvidia-drm/nvidia-drm-linux.c	2016-12-29 15:56:21.681098493 +0300
@@ -196,7 +196,7 @@
 module_exit(nv_linux_drm_exit);
 
 #if defined(MODULE_LICENSE)
-  MODULE_LICENSE("MIT");
+  MODULE_LICENSE("GPLv2");
 #endif
 #if defined(MODULE_INFO)
   MODULE_INFO(supported, "external");
diff -ur a/kernel/nvidia-drm/nvidia-drm-modeset.c b/kernel/nvidia-drm/nvidia-drm-modeset.c
--- a/kernel/nvidia-drm/nvidia-drm-modeset.c	2016-11-16 02:44:48.000000000 +0300
+++ b/kernel/nvidia-drm/nvidia-drm-modeset.c	2017-01-27 04:18:26.120850576 +0300
@@ -51,7 +51,7 @@
 
 struct drm_atomic_state *nvidia_drm_atomic_state_alloc(struct drm_device *dev)
 {
-    struct nvidia_drm_atomic_state *nv_state =
+    struct nvidia_drm_atomic_state *nv_state = 
             nvidia_drm_calloc(1, sizeof(*nv_state));
 
     if (nv_state == NULL || drm_atomic_state_init(dev, &nv_state->base) < 0) {
@@ -69,8 +69,7 @@
 
 void nvidia_drm_atomic_state_free(struct drm_atomic_state *state)
 {
-    struct nvidia_drm_atomic_state *nv_state =
-                    to_nv_atomic_state(state);
+    struct nvidia_drm_atomic_state *nv_state = to_nv_atomic_state(state);
     drm_atomic_state_default_release(state);
     nvidia_drm_free(nv_state);
 }
@@ -983,7 +982,10 @@
      * drm_atomic_commit().
      */
     if (ret != 0) {
-        drm_atomic_state_free(state);
+	if (state) {
+                drm_atomic_state_put(state);
+                state = NULL;
+        }
     }
 
     drm_modeset_unlock_all(dev);
diff -ur a/kernel/nvidia-drm/nvidia-drm-priv.h b/kernel/nvidia-drm/nvidia-drm-priv.h
--- a/kernel/nvidia-drm/nvidia-drm-priv.h	2016-11-16 02:44:48.000000000 +0300
+++ b/kernel/nvidia-drm/nvidia-drm-priv.h	2017-01-27 03:53:22.685474852 +0300
@@ -34,7 +34,7 @@
 #endif
 
 #if defined(NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ)
-#include <linux/fence.h>
+#include <linux/dma-fence.h>
 #include <linux/reservation.h>
 #endif
 
diff -ur a/kernel/nvidia-modeset/nvidia-modeset-linux.c b/kernel/nvidia-modeset/nvidia-modeset-linux.c
--- a/kernel/nvidia-modeset/nvidia-modeset-linux.c	2016-11-16 02:44:49.000000000 +0300
+++ b/kernel/nvidia-modeset/nvidia-modeset-linux.c	2016-12-29 15:56:21.696094296 +0300
@@ -1285,7 +1285,7 @@
 module_exit(nvkms_exit);
 
 #if defined(MODULE_LICENSE)
-  MODULE_LICENSE("NVIDIA");
+  MODULE_LICENSE("GPLv2");
 #endif
 #if defined(MODULE_INFO)
   MODULE_INFO(supported, "external");
diff -ur a/kernel/nvidia-uvm/uvm8.c b/kernel/nvidia-uvm/uvm8.c
--- a/kernel/nvidia-uvm/uvm8.c	2016-11-16 02:53:31.000000000 +0300
+++ b/kernel/nvidia-uvm/uvm8.c	2017-01-27 03:49:12.313552264 +0300
@@ -101,7 +101,7 @@
 // so we force it to fail instead.
 static int uvm_vm_fault_sigbus(struct vm_area_struct *vma, struct vm_fault *vmf)
 {
-    UVM_DBG_PRINT_RL("Fault to address 0x%p in disabled vma\n", vmf->virtual_address);
+    UVM_DBG_PRINT_RL("Fault to address 0x%p in disabled vma\n", vmf->address);
     vmf->page = NULL;
     return VM_FAULT_SIGBUS;
 }
@@ -315,7 +315,7 @@
 {
     uvm_va_space_t *va_space = uvm_va_space_get(vma->vm_file);
     uvm_va_block_t *va_block;
-    NvU64 fault_addr = (NvU64)(uintptr_t)vmf->virtual_address;
+    NvU64 fault_addr = (NvU64)vmf->address;
     bool is_write = vmf->flags & FAULT_FLAG_WRITE;
     NV_STATUS status = uvm_global_get_status();
     bool tools_enabled;
diff -ur a/kernel/nvidia-uvm/uvm_common.c b/kernel/nvidia-uvm/uvm_common.c
--- a/kernel/nvidia-uvm/uvm_common.c	2016-11-16 02:53:31.000000000 +0300
+++ b/kernel/nvidia-uvm/uvm_common.c	2016-12-29 15:56:21.703092337 +0300
@@ -390,5 +390,5 @@
 MODULE_PARM_DESC(uvm_enable_builtin_tests,
                  "Enable the UVM built-in tests. (This is a security risk)");
 
-MODULE_LICENSE("MIT");
+MODULE_LICENSE("GPLv2");
 MODULE_INFO(supported, "external");
diff -ur a/kernel/nvidia-uvm/uvm_lite.c b/kernel/nvidia-uvm/uvm_lite.c
--- a/kernel/nvidia-uvm/uvm_lite.c	2016-11-16 02:53:32.000000000 +0300
+++ b/kernel/nvidia-uvm/uvm_lite.c	2017-01-27 03:47:31.737702766 +0300
@@ -1333,7 +1333,7 @@
 #if defined(NV_VM_OPERATIONS_STRUCT_HAS_FAULT)
 int _fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 {
-    unsigned long vaddr = (unsigned long)vmf->virtual_address;
+    unsigned long vaddr = (unsigned long)vmf->address;
     struct page *page = NULL;
     int retval;
 
diff -ur a/kernel/nvidia-uvm/uvm_unsupported.c b/kernel/nvidia-uvm/uvm_unsupported.c
--- a/kernel/nvidia-uvm/uvm_unsupported.c	2016-11-16 02:53:32.000000000 +0300
+++ b/kernel/nvidia-uvm/uvm_unsupported.c	2016-12-29 15:56:21.699093456 +0300
@@ -171,6 +171,6 @@
 module_init(uvm_unsupported_module_init);
 module_exit(uvm_unsupported_exit);
 
-MODULE_LICENSE("MIT");
+MODULE_LICENSE("GPLv2");
 MODULE_INFO(supported, "external");
 
