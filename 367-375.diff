diff -ur 367.57/kernel/common/inc/nvCpuUuid.h 375.20/kernel/common/inc/nvCpuUuid.h
--- 367.57/kernel/common/inc/nvCpuUuid.h	2016-11-13 19:53:24.594050447 +0300
+++ 375.20/kernel/common/inc/nvCpuUuid.h	2016-11-16 02:47:36.000000000 +0300
@@ -39,8 +39,8 @@
 {
     {
        // Produced via uuidgen(1): 73772a14-2c41-4750-a27b-d4d74e0f5ea6:
-       0xa7, 0x5e, 0x0f, 0x4e, 0xd7, 0xd5, 0x7b, 0xa1,
-       0x51, 0x47, 0x41, 0x2c, 0x15, 0x2a, 0x77, 0x73
+       0xa6, 0x5e, 0x0f, 0x4e, 0xd7, 0xd4, 0x7b, 0xa2,
+       0x50, 0x47, 0x41, 0x2c, 0x14, 0x2a, 0x77, 0x73
     }
 };
 
diff -ur 367.57/kernel/common/inc/nv.h 375.20/kernel/common/inc/nv.h
--- 367.57/kernel/common/inc/nv.h	2016-11-13 19:52:16.215333803 +0300
+++ 375.20/kernel/common/inc/nv.h	2016-11-16 02:53:47.000000000 +0300
@@ -1,6 +1,6 @@
 /* _NVRM_COPYRIGHT_BEGIN_
  *
- * Copyright 1999-2015 by NVIDIA Corporation.  All rights reserved.  All
+ * Copyright 1999-2016 by NVIDIA Corporation.  All rights reserved.  All
  * information contained herein is proprietary and confidential to NVIDIA
  * Corporation.  Any use, reproduction, or disclosure without the written
  * permission of NVIDIA Corporation is prohibited.
@@ -27,8 +27,8 @@
 #define NV_MAJOR_DEVICE_NUMBER 195
 
 /* most cards in a single system */
-#define NV_MAX_DEVICES 4
-#define GPU_UUID_LEN   (16)
+#define NV_MAX_DEVICES 32
+#define GPU_UUID_LEN    (16)
 
 typedef struct {
     NvU32    domain;        /* PCI domain number   */
@@ -267,7 +267,7 @@
 {
     NvU32 size;
     void *top;
-    NvU8  stack[NV_STACK_SIZE-16] __attribute__ ((aligned(8)));
+    NvU8  stack[NV_STACK_SIZE-16] __attribute__ ((aligned(16)));
 } nvidia_stack_t;
 
 /*
@@ -291,6 +291,16 @@
     struct nv_event_s  *next;
 } nv_event_t;
 
+/*
+ * this is to be able to associate a fd memdesc to a file descriptor
+ */
+typedef struct nv_fd_memdesc_s
+{
+    NvBool              bValid;
+    NvHandle            hParent;
+    NvHandle            hMemory;
+} nv_fd_memdesc_t;
+
 typedef struct nv_kern_mapping_s
 {
     void  *addr;
@@ -355,7 +365,7 @@
 
     void *mmap_mutex;
     nv_mmap_context_t *mmap_contexts;
- 
+
     /* DMA addressable range of the device */
     NvU64 dma_addressable_start;
     NvU64 dma_addressable_limit;
@@ -409,7 +419,6 @@
 #define NV_FLAG_IN_RECOVERY            0x1000
 #define NV_FLAG_SKIP_CFG_CHECK         0x2000
 #define NV_FLAG_UNBIND_LOCK            0x4000
-#define NV_FLAG_ACQ_GPU_LOCK           0x8000
 
 #define NV_PM_ACPI_HIBERNATE    0x0001
 #define NV_PM_ACPI_STANDBY      0x0002
@@ -420,25 +429,25 @@
 #define NV_IS_GVI_DEVICE(nv)    ((nv)->flags & NV_FLAG_GVI)
 #define NV_IS_CTL_DEVICE(nv)    ((nv)->flags & NV_FLAG_CONTROL)
 
-/*                                                                                    
- * The ACPI specification defines IDs for various ACPI video                          
- * extension events like display switch events, AC/battery                            
- * events, docking events, etc..                                                      
- * Whenever an ACPI event is received by the corresponding                            
- * event handler installed within the core NVIDIA driver, the                         
- * code can verify the event ID before processing it.                                 
+/*
+ * The ACPI specification defines IDs for various ACPI video
+ * extension events like display switch events, AC/battery
+ * events, docking events, etc..
+ * Whenever an ACPI event is received by the corresponding
+ * event handler installed within the core NVIDIA driver, the
+ * code can verify the event ID before processing it.
  */
-#define ACPI_DISPLAY_DEVICE_CHANGE_EVENT      0x80 
+#define ACPI_DISPLAY_DEVICE_CHANGE_EVENT      0x80
 #define NVIF_NOTIFY_DISPLAY_DETECT           0xCB
-#define NVIF_DISPLAY_DEVICE_CHANGE_EVENT     NVIF_NOTIFY_DISPLAY_DETECT 
-/*                                                                                    
- * NVIDIA ACPI event IDs to be passed into the core NVIDIA                            
- * driver for various events like display switch events,                              
- * AC/battery events, docking events, etc..                                           
- */                                                                                   
-#define NV_SYSTEM_ACPI_DISPLAY_SWITCH_EVENT  0x8001                                   
-#define NV_SYSTEM_ACPI_BATTERY_POWER_EVENT   0x8002                                   
-#define NV_SYSTEM_ACPI_DOCK_EVENT            0x8003                                   
+#define NVIF_DISPLAY_DEVICE_CHANGE_EVENT     NVIF_NOTIFY_DISPLAY_DETECT
+/*
+ * NVIDIA ACPI event IDs to be passed into the core NVIDIA
+ * driver for various events like display switch events,
+ * AC/battery events, docking events, etc..
+ */
+#define NV_SYSTEM_ACPI_DISPLAY_SWITCH_EVENT  0x8001
+#define NV_SYSTEM_ACPI_BATTERY_POWER_EVENT   0x8002
+#define NV_SYSTEM_ACPI_DOCK_EVENT            0x8003
 
 /*
  * Status bit definitions for display switch hotkey events.
@@ -448,15 +457,15 @@
 #define NV_HOTKEY_STATUS_DISPLAY_ENABLE_TV  0x04
 #define NV_HOTKEY_STATUS_DISPLAY_ENABLE_DFP 0x08
 
-/*                                                                                    
- * NVIDIA ACPI sub-event IDs (event types) to be passed into                          
- * to core NVIDIA driver for ACPI events.                                             
- */                                                                                   
-#define NV_SYSTEM_ACPI_EVENT_VALUE_DISPLAY_SWITCH_DEFAULT    0                        
-#define NV_SYSTEM_ACPI_EVENT_VALUE_POWER_EVENT_AC            0                        
-#define NV_SYSTEM_ACPI_EVENT_VALUE_POWER_EVENT_BATTERY       1                        
-#define NV_SYSTEM_ACPI_EVENT_VALUE_DOCK_EVENT_UNDOCKED       0                        
-#define NV_SYSTEM_ACPI_EVENT_VALUE_DOCK_EVENT_DOCKED         1                        
+/*
+ * NVIDIA ACPI sub-event IDs (event types) to be passed into
+ * to core NVIDIA driver for ACPI events.
+ */
+#define NV_SYSTEM_ACPI_EVENT_VALUE_DISPLAY_SWITCH_DEFAULT    0
+#define NV_SYSTEM_ACPI_EVENT_VALUE_POWER_EVENT_AC            0
+#define NV_SYSTEM_ACPI_EVENT_VALUE_POWER_EVENT_BATTERY       1
+#define NV_SYSTEM_ACPI_EVENT_VALUE_DOCK_EVENT_UNDOCKED       0
+#define NV_SYSTEM_ACPI_EVENT_VALUE_DOCK_EVENT_DOCKED         1
 
 #define NV_ACPI_NVIF_HANDLE_PRESENT 0x01
 #define NV_ACPI_DSM_HANDLE_PRESENT  0x02
@@ -604,6 +613,9 @@
 NV_STATUS  NV_API_CALL  nv_register_user_pages   (nv_state_t *, NvU64, NvU64 *, void **);
 NV_STATUS  NV_API_CALL  nv_unregister_user_pages (nv_state_t *, NvU64, void **);
 
+NV_STATUS NV_API_CALL   nv_register_user_physical_addresses (nv_state_t *, NvU64, NvU64 *, void **);
+NV_STATUS NV_API_CALL   nv_unregister_user_physical_addresses (nv_state_t *, NvU64, void *);
+
 NV_STATUS  NV_API_CALL  nv_dma_map_pages         (nv_state_t *, NvU64, NvU64 *, NvBool, void **);
 NV_STATUS  NV_API_CALL  nv_dma_unmap_pages       (nv_state_t *, NvU64, NvU64 *, void **);
 
@@ -643,6 +655,9 @@
 NvBool     NV_API_CALL  nv_is_virtualized_system  (nv_stack_t *);
 #endif
 
+nv_fd_memdesc_t* NV_API_CALL nv_get_fd_memdesc   (void *);
+NV_STATUS  NV_API_CALL  nv_add_fd_memdesc_to_fd  (NvU32, const nv_fd_memdesc_t*);
+
 /*
  * ---------------------------------------------------------------------------
  *
@@ -666,6 +681,7 @@
 NV_STATUS  NV_API_CALL  rm_ioctl                 (nvidia_stack_t *, nv_state_t *, void *, NvU32, void *, NvU32);
 BOOL       NV_API_CALL  rm_isr                   (nvidia_stack_t *, nv_state_t *, NvU32 *);
 void       NV_API_CALL  rm_isr_bh                (nvidia_stack_t *, nv_state_t *);
+void       NV_API_CALL  rm_isr_bh_unlocked       (nvidia_stack_t *, nv_state_t *);
 NV_STATUS  NV_API_CALL  rm_power_management      (nvidia_stack_t *, nv_state_t *, NvU32, NvU32);
 NV_STATUS  NV_API_CALL  rm_save_low_res_mode     (nvidia_stack_t *, nv_state_t *);
 NV_STATUS  NV_API_CALL  rm_get_vbios_version     (nvidia_stack_t *, nv_state_t *, NvU32 *, NvU32 *, NvU32 *, NvU32 *, NvU32 *);
@@ -679,6 +695,9 @@
 NV_STATUS  NV_API_CALL  rm_read_registry_binary  (nvidia_stack_t *, nv_state_t *, NvU8 *, NvU8 *, NvU8 *, NvU32 *);
 NV_STATUS  NV_API_CALL  rm_write_registry_binary (nvidia_stack_t *, nv_state_t *, NvU8 *, NvU8 *, NvU8 *, NvU32);
 NV_STATUS  NV_API_CALL  rm_write_registry_string (nvidia_stack_t *, nv_state_t *, NvU8 *, NvU8 *, const char *, NvU32);
+void       NV_API_CALL  rm_parse_option_string   (nvidia_stack_t *, const char *);
+char*      NV_API_CALL  rm_remove_spaces         (const char *);
+char*      NV_API_CALL  rm_string_token          (char **, const char);
 
 NV_STATUS  NV_API_CALL  rm_run_rc_callback       (nvidia_stack_t *, nv_state_t *);
 void       NV_API_CALL  rm_execute_work_item     (nvidia_stack_t *, void *);
@@ -693,7 +712,6 @@
 BOOL       NV_API_CALL  rm_is_legacy_arch        (NvU32, NvU32);
 NV_STATUS  NV_API_CALL  rm_is_supported_device   (nvidia_stack_t *, nv_state_t *, NvU32 *, NvU32 *);
 
-
 void       NV_API_CALL  rm_check_pci_config_space (nvidia_stack_t *, nv_state_t *nv, BOOL, BOOL, BOOL);
 
 NV_STATUS  NV_API_CALL  rm_i2c_remove_adapters    (nvidia_stack_t *, nv_state_t *);
@@ -764,6 +782,8 @@
 NV_STATUS NV_API_CALL rm_gpu_ops_own_page_fault_intr(nvidia_stack_t *, NvU8 *, unsigned, NvBool);
 NV_STATUS  NV_API_CALL rm_gpu_ops_init_fault_info(nvidia_stack_t *, nvgpuAddressSpaceHandle_t, nvgpuFaultInfo_t);
 NV_STATUS  NV_API_CALL rm_gpu_ops_destroy_fault_info(nvidia_stack_t *, nvgpuAddressSpaceHandle_t, nvgpuFaultInfo_t);
+NV_STATUS  NV_API_CALL rm_gpu_ops_get_non_replayable_faults(nvidia_stack_t *, nvgpuFaultInfo_t, void *, NvU32 *);
+NV_STATUS  NV_API_CALL rm_gpu_ops_has_pending_non_replayable_faults(nvidia_stack_t *, nvgpuFaultInfo_t, NvBool *);
 NV_STATUS  NV_API_CALL rm_gpu_ops_init_access_cntr_info(nvidia_stack_t *, nvgpuAddressSpaceHandle_t, nvgpuAccessCntrInfo_t);
 NV_STATUS  NV_API_CALL rm_gpu_ops_destroy_access_cntr_info(nvidia_stack_t *, nvgpuAddressSpaceHandle_t, nvgpuAccessCntrInfo_t);
 NV_STATUS  NV_API_CALL rm_gpu_ops_get_page_level_info(nvidia_stack_t *, nvgpuAddressSpaceHandle_t, NvU64, nvgpuPageLevelInfo_t);
@@ -789,8 +809,21 @@
 void       NV_API_CALL rm_gpu_ops_stop_channel(nvidia_stack_t *, nvgpuAddressSpaceHandle_t, NvP64, NvBool);
 NV_STATUS  NV_API_CALL rm_gpu_ops_get_channel_resource_ptes(nvidia_stack_t *, nvgpuAddressSpaceHandle_t, NvP64, NvU64, NvU64, nvgpuExternalMappingInfo_t);
 void       NV_API_CALL rm_kernel_rmapi_op(nvidia_stack_t *sp, void *ops_cmd);
+NvBool     NV_API_CALL rm_get_device_remove_flag(nvidia_stack_t *sp, NvU32 gpu_id);
 NV_STATUS  NV_API_CALL rm_gpu_copy_mmu_faults(nvidia_stack_t *, nv_state_t *, NvU32 *);
 NV_STATUS  NV_API_CALL rm_gpu_copy_mmu_faults_unlocked(nvidia_stack_t *, nv_state_t *, NvU32 *);
+NV_STATUS  NV_API_CALL rm_gpu_need_4k_page_isolation(nv_state_t *, NvBool *);
+NvBool     NV_API_CALL rm_is_chipset_io_coherent(nv_stack_t *); 
+
+/* vGPU VFIO specific functions */
+NV_STATUS  NV_API_CALL  nv_vgpu_supported_config(nvidia_stack_t *, nv_state_t *, NvU8 *);
+NV_STATUS  NV_API_CALL  nv_vgpu_create_request(nvidia_stack_t *, nv_state_t *, NvU8 *, NvU32, NvU8 *, NvU32);
+NV_STATUS  NV_API_CALL  nv_vgpu_delete(nvidia_stack_t *, NvU8 *, NvU32);
+NV_STATUS  NV_API_CALL  nv_vgpu_get_bar_info(nvidia_stack_t *, nv_state_t *, NvU8 *, NvU32, NvU64 *, NvU32, void *);
+NV_STATUS  NV_API_CALL  nv_vgpu_reg_access(nvidia_stack_t *, nv_state_t *, NvU8 *, NvU32, NvU8 *, NvU32, NvU64 , NvS32 *, void *, NvBool, NvU32);
+NV_STATUS  NV_API_CALL  nv_vgpu_start(nvidia_stack_t *, NvU8 *, NvU32,  void *, NvS32 *);
+NV_STATUS  NV_API_CALL  nv_vgpu_power_op(nvidia_stack_t *, nv_state_t *, NvU8 *, NvU32,  void *, NvS32 *, NvBool);
+NV_STATUS  NV_API_CALL  nv_vgpu_validate_map_request(nvidia_stack_t *, nv_state_t *, NvU8 *, NvU32, NvU64 *, NvU64 *, NvU64 *);
 
 #endif /* NVWATCH */
 
diff -ur 367.57/kernel/common/inc/nv-hypervisor.h 375.20/kernel/common/inc/nv-hypervisor.h
--- 367.57/kernel/common/inc/nv-hypervisor.h	2016-10-04 05:37:33.000000000 +0300
+++ 375.20/kernel/common/inc/nv-hypervisor.h	2016-11-16 02:44:00.000000000 +0300
@@ -24,6 +24,28 @@
     OS_HYPERVISOR_UNKNOWN
 } HYPERVISOR_TYPE;
 
+#define CMD_VGPU_VFIO_WAKE_WAIT_QUEUE 0
+#define CMD_VGPU_VFIO_UNMAP_GUEST_MMIO 1
+#define CMD_VGPU_VFIO_INJECT_INTERRUPT 2
+#define CMD_VGPU_VFIO_WAKE_ON_STOP_COMPLETION 3
+#define CMD_VGPU_VFIO_UNPIN_PAGES                4
+#define CMD_VGPU_VFIO_TRANSLATE_GFN_TO_PFN       5
+#define CMD_VGPU_VFIO_TRANSLATE_GFN_TO_PAGES     6
+
+typedef struct
+{
+    void  *vgpuVfioRef;
+    void  *waitQueue;
+    NvU64  physMmioAddr;
+    NvU64  virtMmioAddr;
+    NvU64  mmioSize;
+    NvU8  *pVmUuid;
+    NvU32  vgpuDeviceInstanceId;
+    NvU32  returnStatus;
+    NvU64 *pfnBuffer;
+    NvU32  pfnCount;
+    NvP64 *pageArray;
+} vgpu_vfio_info;
 
 /*
  * Function prototypes
diff -ur 367.57/kernel/common/inc/nvkms-api-types.h 375.20/kernel/common/inc/nvkms-api-types.h
--- 367.57/kernel/common/inc/nvkms-api-types.h	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/common/inc/nvkms-api-types.h	2016-11-16 02:44:49.000000000 +0300
@@ -102,6 +102,8 @@
     NvKmsSurfaceMemoryFormatX8R8G8B8    = 5,
     NvKmsSurfaceMemoryFormatA2B10G10R10 = 6,
     NvKmsSurfaceMemoryFormatX2B10G10R10 = 7,
+    NvKmsSurfaceMemoryFormatA8B8G8R8    = 8,
+    NvKmsSurfaceMemoryFormatX8B8G8R8    = 9,
 };
 
 static inline NvU8 nvKmsSurfaceMemoryFormatToBpp
@@ -119,6 +121,8 @@
             return  16;
         case NvKmsSurfaceMemoryFormatA8R8G8B8:
         case NvKmsSurfaceMemoryFormatX8R8G8B8:
+        case NvKmsSurfaceMemoryFormatA8B8G8R8:
+        case NvKmsSurfaceMemoryFormatX8B8G8R8:
         case NvKmsSurfaceMemoryFormatA2B10G10R10:
         case NvKmsSurfaceMemoryFormatX2B10G10R10:
             return  32;
@@ -142,10 +146,12 @@
         case NvKmsSurfaceMemoryFormatR5G6B5:
             return 16;
         case NvKmsSurfaceMemoryFormatX8R8G8B8:
+        case NvKmsSurfaceMemoryFormatX8B8G8R8:
             return 24;
         case NvKmsSurfaceMemoryFormatX2B10G10R10:
             return 30;
         case NvKmsSurfaceMemoryFormatA8R8G8B8:
+        case NvKmsSurfaceMemoryFormatA8B8G8R8:
         case NvKmsSurfaceMemoryFormatA2B10G10R10:
             return 32;
     }
@@ -179,6 +185,10 @@
             return "A2B10G10R10";
         case NvKmsSurfaceMemoryFormatX2B10G10R10:
             return "X2B10G10R10";
+        case NvKmsSurfaceMemoryFormatA8B8G8R8:
+            return "A8B8G8R8";
+        case NvKmsSurfaceMemoryFormatX8B8G8R8:
+            return "X8B8G8R8";
     }
 
     return "Unknown";
diff -ur 367.57/kernel/common/inc/nvkms-kapi.h 375.20/kernel/common/inc/nvkms-kapi.h
--- 367.57/kernel/common/inc/nvkms-kapi.h	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/common/inc/nvkms-kapi.h	2016-11-16 02:44:49.000000000 +0300
@@ -45,6 +45,7 @@
 struct NvKmsKapiDevice;
 struct NvKmsKapiMemory;
 struct NvKmsKapiSurface;
+struct NvKmsKapiChannelEvent;
 
 typedef NvU32 NvKmsKapiConnector;
 typedef NvU32 NvKmsKapiDisplay;
@@ -52,6 +53,15 @@
 /** @} */
 
 /**
+ * \defgroup FuncPtrs
+ * @{
+ */
+
+typedef void NVKMS_KAPI_CALL NvKmsChannelEventProc(void *dataPtr, NvU32 dataU32);
+
+/** @} */
+
+/**
  * \defgroup Structs
  * @{
  */
@@ -123,6 +133,11 @@
 
 #define NVKMS_KAPI_PLANE_MASK(planeType) (1 << (planeType))
 
+typedef enum NvKmsKapiMappingTypeRec {
+    NVKMS_KAPI_MAPPING_TYPE_USER   = 1,
+    NVKMS_KAPI_MAPPING_TYPE_KERNEL = 2,
+} NvKmsKapiMappingType;
+
 struct NvKmsKapiConnectorInfo {
 
     NvKmsKapiConnector handle;
@@ -499,6 +514,8 @@
      *
      * \param [in]  memory           Memory allocated using allocateMemory()
      *
+     * \param [in]  type             Userspace or kernelspace mapping
+     *
      * \param [out] ppLinearAddress  The MMIO address where memory object is
      *                               mapped.
      *
@@ -507,7 +524,8 @@
     NvBool NVKMS_KAPI_CALL (*mapMemory)
     (
         const struct NvKmsKapiDevice *device,
-        const struct NvKmsKapiMemory *memory, void **ppLinearAddress
+        const struct NvKmsKapiMemory *memory, NvKmsKapiMappingType type,
+        void **ppLinearAddress
     );
 
     /*!
@@ -518,12 +536,15 @@
      *
      * \param [in]  memory           Memory allocated using allocateMemory()
      *
+     * \param [in]  type             Userspace or kernelspace mapping
+     *
      * \param [in]  pLinearAddress   The MMIO address return by mapMemory()
      */
     void NVKMS_KAPI_CALL (*unmapMemory)
     (
         const struct NvKmsKapiDevice *device,
-        const struct NvKmsKapiMemory *memory, const void *pLinearAddress
+        const struct NvKmsKapiMemory *memory, NvKmsKapiMappingType type,
+        const void *pLinearAddress
     );
 
     /*!
@@ -658,6 +679,76 @@
         const NvKmsKapiPlaneType plane,
         NvBool *pending
     );
+
+    /*!
+     * Allocate an event callback. Disabled by default.
+     *
+     * \param [in]  device          A device allocated using allocateDevice().
+     *
+     * \param [in]  proc            Function pointer to call when triggered.
+     *
+     * \param [in]  data            Argument to pass into function.
+     *
+     * \param [in] nvKmsParamsUser  Userspace pointer to driver-specific
+     *                              parameters describing the event callback
+     *                              being created.
+     *
+     * \param [in] nvKmsParamsSize  Size of the driver-specific parameter struct.
+     *
+     * \return struct NvKmsKapiChannelEvent* on success, NULL on failure.
+     */
+    struct NvKmsKapiChannelEvent* NVKMS_KAPI_CALL (*allocateChannelEvent)
+    (
+        struct NvKmsKapiDevice *device,
+        NvKmsChannelEventProc *proc,
+        void *data,
+        NvU64 nvKmsParamsUser,
+        NvU64 nvKmsParamsSize
+    );
+
+    /*!
+     * Free an event callback.
+     *
+     * \param [in]  device  A device allocated using allocateDevice().
+     *
+     * \param [in]  cb      struct NvKmsKapiChannelEvent* returned from
+     *                      allocateChannelEvent()
+     */
+    void NVKMS_KAPI_CALL (*freeChannelEvent)
+    (
+        struct NvKmsKapiDevice *device,
+        struct NvKmsKapiChannelEvent *cb
+    );
+
+    /*!
+     * Enable an event callback.
+     *
+     * \param [in]  device  A device allocated using allocateDevice().
+     *
+     * \param [in]  cb      struct NvKmsKapiChannelEvent* returned from
+     *                      allocateChannelEvent()
+     *
+     * \return NV_TRUE on success, NV_FALSE on failure.
+     */
+    NvBool NVKMS_KAPI_CALL (*enableChannelEvent)
+    (
+        struct NvKmsKapiDevice *device,
+        struct NvKmsKapiChannelEvent *cb
+    );
+
+    /*!
+     * Disable an event callback.
+     *
+     * \param [in]  device  A device allocated using allocateDevice().
+     *
+     * \param [in]  cb      struct NvKmsKapiChannelEvent* returned from
+     *                      allocateChannelEvent()
+     */
+    void NVKMS_KAPI_CALL (*disableChannelEvent)
+    (
+        struct NvKmsKapiDevice *device,
+        struct NvKmsKapiChannelEvent *cb
+    );
 };
 
 /** @} */
diff -ur 367.57/kernel/common/inc/nvkms-kapi-user.h 375.20/kernel/common/inc/nvkms-kapi-user.h
--- 367.57/kernel/common/inc/nvkms-kapi-user.h	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/common/inc/nvkms-kapi-user.h	2016-11-16 02:44:49.000000000 +0300
@@ -18,6 +18,11 @@
     NvU32 hMemory;
 };
 
+struct NvKmsAllocateChannelEventParams {
+    NvU32 hClient;
+    NvU32 hChannel;
+};
+
 struct NvKmsSurfaceParams {
     struct {
         NvU32 x;
Только в 375.20/kernel/common/inc: nv-kthread-q.h
diff -ur 367.57/kernel/common/inc/nv-linux.h 375.20/kernel/common/inc/nv-linux.h
--- 367.57/kernel/common/inc/nv-linux.h	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/common/inc/nv-linux.h	2016-11-16 02:53:45.000000000 +0300
@@ -1,6 +1,6 @@
 /* _NVRM_COPYRIGHT_BEGIN_
  *
- * Copyright 2001-2014 by NVIDIA Corporation.  All rights reserved.  All
+ * Copyright 2001-2016 by NVIDIA Corporation.  All rights reserved.  All
  * information contained herein is proprietary and confidential to NVIDIA
  * Corporation.  Any use, reproduction, or disclosure without the written
  * permission of NVIDIA Corporation is prohibited.
@@ -100,6 +100,7 @@
 #include <linux/pci.h>              /* pci_find_class, etc              */
 #include <linux/interrupt.h>        /* tasklets, interrupt helpers      */
 #include <linux/timer.h>
+#include <linux/file.h>             /* fget(), fput()                   */
 
 #include <asm/div64.h>              /* do_div()                         */
 #if defined(NV_ASM_SYSTEM_H_PRESENT)
@@ -173,6 +174,7 @@
 #include <linux/highmem.h>
 
 #include <linux/workqueue.h>        /* workqueue                        */
+#include <nv-kthread-q.h>           /* kthread based queue              */
 
 #if defined(NV_LINUX_EFI_H_PRESENT)
 #include <linux/efi.h>              /* efi_enabled                      */
@@ -724,6 +726,7 @@
         free_pages(ptr, order);                      \
     }
 
+extern NvBool nvos_is_chipset_io_coherent(void);
 
 static inline NvUPtr nv_vmap(struct page **pages, NvU32 page_count,
     NvBool cached)
@@ -732,11 +735,11 @@
     pgprot_t prot = PAGE_KERNEL;
 #if defined(NVCPU_X86) || defined(NVCPU_X86_64)
     prot = cached ? PAGE_KERNEL : PAGE_KERNEL_NOCACHE;
-#elif defined(NVCPU_ARM)
-    prot = cached ? PAGE_KERNEL : NV_PGPROT_UNCACHED(PAGE_KERNEL);
+#elif defined(NVCPU_FAMILY_ARM)
+    if (!nvos_is_chipset_io_coherent()) 
+        prot = cached ? PAGE_KERNEL : NV_PGPROT_UNCACHED(PAGE_KERNEL);
 #endif
-    /* All memory cached in PPC64LE; can't honor 'cached' input.
-     * For now, treat all memory as cached on AArch64 as well. */   \
+    /* All memory cached in PPC64LE; can't honor 'cached' input. */
     ptr = vmap(pages, page_count, VM_MAP, prot);
     VM_ALLOC_RECORD(ptr, page_count * PAGE_SIZE, "vm_vmap");
     return (NvUPtr)ptr;
@@ -802,18 +805,19 @@
 }
 
 #if defined(NV_CONFIG_PREEMPT_RT)
-#define NV_INIT_MUTEX(mutex) sema_init(mutex,1)
+#define NV_INIT_SEMA(sema, val) sema_init(sema,val)
 #else
 #if !defined(__SEMAPHORE_INITIALIZER) && defined(__COMPAT_SEMAPHORE_INITIALIZER)
 #define __SEMAPHORE_INITIALIZER __COMPAT_SEMAPHORE_INITIALIZER
 #endif
-#define NV_INIT_MUTEX(mutex)                       \
+#define NV_INIT_SEMA(sema, val)                    \
     {                                              \
-        struct semaphore __mutex =                 \
-            __SEMAPHORE_INITIALIZER(*(mutex), 1);  \
-        *(mutex) = __mutex;                        \
+        struct semaphore __sema =                  \
+            __SEMAPHORE_INITIALIZER(*(sema), val); \
+        *(sema) = __sema;                          \
     }
 #endif
+#define NV_INIT_MUTEX(mutex) NV_INIT_SEMA(mutex, 1)
 
 #if defined(NV_GET_NUM_PHYSPAGES_PRESENT)
 #define NV_NUM_PHYSPAGES                get_num_physpages()
@@ -988,6 +992,12 @@
 #define NV_GET_DOMAIN_BUS_AND_SLOT(domain,bus,devfn) pci_find_slot(bus,devfn)
 #endif
 
+#if defined(NV_PCI_STOP_AND_REMOVE_BUS_DEVICE_PRESENT)  // introduced in 3.4.9
+#define NV_PCI_STOP_AND_REMOVE_BUS_DEVICE(dev) pci_stop_and_remove_bus_device(dev)
+#elif defined(NV_PCI_REMOVE_BUS_DEVICE_PRESENT) // introduced in 2.6
+#define NV_PCI_STOP_AND_REMOVE_BUS_DEVICE(dev) pci_remove_bus_device(dev)
+#endif
+
 #define NV_PRINT_AT(nv_debug_level,at)                                           \
     {                                                                            \
         nv_printf(nv_debug_level,                                                \
@@ -1661,6 +1671,10 @@
     /* DRM private information */
     struct drm_device *drm;
 
+    /* kthread based bottom half servicing queue and elements */
+    nv_kthread_q_t bottom_half_q;
+    nv_kthread_q_item_t bottom_half_q_item;
+
     NvBool tce_bypass_enabled;
 } nv_linux_state_t;
 
@@ -1741,6 +1755,9 @@
     void *proc_data;
     void *data;
     nvidia_event_t *event_head, *event_tail;
+    // This field is only applicable for the control device
+    // it should not be used for the per-GPU special files.
+    nv_fd_memdesc_t fd_memdesc;
     int event_pending;
     nv_spinlock_t fp_lock;
     wait_queue_head_t waitqueue;
@@ -1903,7 +1920,7 @@
 #undef NV_UVM_ENABLE
 #endif
 
-#if defined(NV_DOM0_KERNEL_PRESENT)
+#if defined(NV_DOM0_KERNEL_PRESENT) || defined(NV_VGPU_KVM_BUILD)
 #define NV_VGX_HYPER
 #if defined(NV_XEN_IOEMU_INJECT_MSI)
 #include <xen/ioemu.h>
diff -ur 367.57/kernel/common/inc/nv-misc.h 375.20/kernel/common/inc/nv-misc.h
--- 367.57/kernel/common/inc/nv-misc.h	2016-11-13 19:46:33.209048103 +0300
+++ 375.20/kernel/common/inc/nv-misc.h	2016-11-16 02:53:47.000000000 +0300
@@ -24,7 +24,7 @@
 #define FALSE           0L
 #endif
 #ifndef NULL
-#define NULL            (void *)0L
+#define NULL            0L
 #endif
 
 /*
diff -ur 367.57/kernel/common/inc/nvmisc.h 375.20/kernel/common/inc/nvmisc.h
--- 367.57/kernel/common/inc/nvmisc.h	2016-10-04 05:37:36.000000000 +0300
+++ 375.20/kernel/common/inc/nvmisc.h	2016-11-16 02:44:01.000000000 +0300
@@ -140,10 +140,27 @@
 #define DRF_DEF64(d,r,f,c)              (((NvU64)(NV ## d ## r ## f ## c))<<DRF_SHIFT64(NV ## d ## r ## f))
 #define DRF_NUM64(d,r,f,n)              ((((NvU64)(n))&DRF_MASK64(NV ## d ## r ## f))<<DRF_SHIFT64(NV ## d ## r ## f))
 #define DRF_VAL64(d,r,f,v)              ((((NvU64)(v))>>DRF_SHIFT64(NV ## d ## r ## f))&DRF_MASK64(NV ## d ## r ## f))
+
+#define DRF_VAL_SIGNED64(d,r,f,v)       (((DRF_VAL64(d,r,f,v) ^ (NVBIT64(DRF_SIZE(NV ## d ## r ## f)-1)))) - (NVBIT64(DRF_SIZE(NV ## d ## r ## f)-1)))
 #define DRF_IDX_DEF64(d,r,f,i,c)        (((NvU64)(NV ## d ## r ## f ## c))<<DRF_SHIFT64(NV##d##r##f(i)))
+#define DRF_IDX_OFFSET_DEF64(d,r,f,i,o,c) ((NvU64)(NV ## d ## r ## f ## c)<<DRF_SHIFT64(NV##d##r##f(i,o)))
 #define DRF_IDX_NUM64(d,r,f,i,n)        ((((NvU64)(n))&DRF_MASK64(NV##d##r##f(i)))<<DRF_SHIFT64(NV##d##r##f(i)))
 #define DRF_IDX_VAL64(d,r,f,i,v)        ((((NvU64)(v))>>DRF_SHIFT64(NV##d##r##f(i)))&DRF_MASK64(NV##d##r##f(i)))
+#define DRF_IDX_OFFSET_VAL64(d,r,f,i,o,v) (((NvU64)(v)>>DRF_SHIFT64(NV##d##r##f(i,o)))&DRF_MASK64(NV##d##r##f(i,o)))
+
+#define FLD_SET_DRF64(d,r,f,c,v)        (((NvU64)(v) & ~DRF_SHIFTMASK64(NV##d##r##f)) | DRF_DEF64(d,r,f,c))
 #define FLD_SET_DRF_NUM64(d,r,f,n,v)    ((((NvU64)(v)) & ~DRF_SHIFTMASK64(NV##d##r##f)) | DRF_NUM64(d,r,f,n))
+#define FLD_IDX_SET_DRF64(d,r,f,i,c,v)  (((NvU64)(v) & ~DRF_SHIFTMASK64(NV##d##r##f(i))) | DRF_IDX_DEF64(d,r,f,i,c))
+#define FLD_IDX_OFFSET_SET_DRF64(d,r,f,i,o,c,v) (((NvU64)(v) & ~DRF_SHIFTMASK64(NV##d##r##f(i,o))) | DRF_IDX_OFFSET_DEF64(d,r,f,i,o,c))
+#define FLD_IDX_SET_DRF_DEF64(d,r,f,i,c,v) (((NvU64)(v) & ~DRF_SHIFTMASK64(NV##d##r##f(i))) | DRF_IDX_DEF64(d,r,f,i,c))
+#define FLD_IDX_SET_DRF_NUM64(d,r,f,i,n,v) (((NvU64)(v) & ~DRF_SHIFTMASK64(NV##d##r##f(i))) | DRF_IDX_NUM64(d,r,f,i,n))
+#define FLD_SET_DRF_IDX64(d,r,f,c,i,v)  (((NvU64)(v) & ~DRF_SHIFTMASK64(NV##d##r##f)) | DRF_DEF64(d,r,f,c(i)))
+
+#define FLD_TEST_DRF64(d,r,f,c,v)       (DRF_VAL64(d, r, f, v) == NV##d##r##f##c)
+#define FLD_TEST_DRF_AND64(d,r,f,c,v)   (DRF_VAL64(d, r, f, v) & NV##d##r##f##c)
+#define FLD_TEST_DRF_NUM64(d,r,f,n,v)   (DRF_VAL64(d, r, f, v) == n)
+#define FLD_IDX_TEST_DRF64(d,r,f,i,c,v) (DRF_IDX_VAL64(d, r, f, i, v) == NV##d##r##f##c)
+#define FLD_IDX_OFFSET_TEST_DRF64(d,r,f,i,o,c,v) (DRF_IDX_OFFSET_VAL64(d, r, f, i, o, v) == NV##d##r##f##c)
 
 //
 // 32 Bit Versions
@@ -355,6 +372,20 @@
 }
 
 /*!
+ * Calculate number of bits set in a 64-bit unsigned integer.
+ */
+static NV_FORCEINLINE NvU32
+nvPopCount64(const NvU64 x)
+{
+    NvU64 temp = x;
+    temp = temp - ((temp >> 1) & 0x5555555555555555ull);
+    temp = (temp & 0x3333333333333333ull) + ((temp >> 2) & 0x3333333333333333ull);
+    temp = (temp + (temp >> 4)) & 0x0F0F0F0F0F0F0F0Full;
+    temp = (temp * 0x0101010101010101ull) >> 56;
+    return (NvU32)temp;
+}
+
+/*!
  * Determine how many bits are set below a bit index within a mask.
  * This assigns a dense ordering to the set bits in the mask.
  *
@@ -509,7 +540,7 @@
     #if defined(__GNUC__) && __GNUC__ > 3
         #define NV_OFFSETOF(type, member)   ((NvU32)__builtin_offsetof(type, member))
     #else
-        #define NV_OFFSETOF(type, member)    ((NvU32)&(((type *)0)->member))
+        #define NV_OFFSETOF(type, member)    ((NvU32)(NvU64)&(((type *)0)->member)) // shouldn't we use PtrToUlong? But will need to include windows header.
     #endif
 #endif
 
diff -ur 367.57/kernel/common/inc/nv-mm.h 375.20/kernel/common/inc/nv-mm.h
--- 367.57/kernel/common/inc/nv-mm.h	2016-11-13 19:49:10.872891356 +0300
+++ 375.20/kernel/common/inc/nv-mm.h	2016-11-16 02:53:45.000000000 +0300
@@ -23,6 +23,8 @@
 #ifndef __NV_MM_H__
 #define __NV_MM_H__
 
+#include "conftest.h"
+
 /*  get_user_pages_remote() was added by:
  *    2016 Feb 12: 1e9877902dc7e11d2be038371c6fbf2dfcd469d7
  *
@@ -34,17 +36,62 @@
  *  version that uses something other than current and current->mm. Use
  *  NV_GET_USER_PAGES if you are refering to current and current->mm.
  *
-*  Note that get_user_pages_remote() requires the caller to hold a reference on
-*  the task_struct (if non-NULL) and the mm_struct. This will always be true
-*  when using current and current->mm. If the kernel passes the driver a vma
-*  via driver callback, the kernel holds a reference on vma->vm_mm over that
-*  callback.
+ *  Note that get_user_pages_remote() requires the caller to hold a reference on
+ *  the task_struct (if non-NULL) and the mm_struct. This will always be true
+ *  when using current and current->mm. If the kernel passes the driver a vma
+ *  via driver callback, the kernel holds a reference on vma->vm_mm over that
+ *  callback.
  */
 
-#define NV_GET_USER_PAGES(start, nr_pages, flags, pages, vmas) \
-	   get_user_pages(start, nr_pages, flags, pages, vmas)
+#if defined(NV_GET_USER_PAGES_REMOTE_PRESENT)
+    #if defined(NV_GET_USER_PAGES_HAS_WRITE_AND_FORCE_ARGS)
+        #define NV_GET_USER_PAGES           get_user_pages
+        #define NV_GET_USER_PAGES_REMOTE    get_user_pages_remote
+    #else
+        #include <linux/mm.h>
+
+        static inline long NV_GET_USER_PAGES(unsigned long start,
+                                             unsigned long nr_pages,
+                                             int write,
+                                             int force,
+                                             struct page **pages,
+                                             struct vm_area_struct **vmas)
+        {
+            unsigned int flags = 0;
+
+            if (write)
+                flags |= FOLL_WRITE;
+            if (force)
+                flags |= FOLL_FORCE;
+
+            return get_user_pages(start, nr_pages, flags, pages, vmas);
+        }
+
+        static inline long NV_GET_USER_PAGES_REMOTE(struct task_struct *tsk,
+                                                    struct mm_struct *mm,
+                                                    unsigned long start,
+                                                    unsigned long nr_pages,
+                                                    int write,
+                                                    int force,
+                                                    struct page **pages,
+                                                    struct vm_area_struct **vmas)
+        {
+            unsigned int flags = 0;
+
+            if (write)
+                flags |= FOLL_WRITE;
+            if (force)
+                flags |= FOLL_FORCE;
+
+            return get_user_pages_remote(tsk, mm, start, nr_pages, flags, pages, vmas);
+        }
+    #endif
+#else
+    #define NV_GET_USER_PAGES(start, nr_pages, write, force, pages, vmas) \
+        get_user_pages(current, current->mm, start, nr_pages, write, force, pages, vmas)
 
-#define NV_GET_USER_PAGES_REMOTE get_user_pages_remote
+    #define NV_GET_USER_PAGES_REMOTE    get_user_pages
+#endif
 
 
 #endif // __NV_MM_H__
diff -ur 367.57/kernel/common/inc/nv-pgprot.h 375.20/kernel/common/inc/nv-pgprot.h
--- 367.57/kernel/common/inc/nv-pgprot.h	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/common/inc/nv-pgprot.h	2016-11-16 02:53:45.000000000 +0300
@@ -97,8 +97,16 @@
     __pgprot(pgprot_val((old_prot)) & ~_PAGE_RW)
 #elif defined(NVCPU_PPC64LE)
 /*
- * Linux kernel-4.6+ use H_PAGE instead of _PAGE
+ * Some kernels use H_PAGE instead of _PAGE
  */
+#if defined(_PAGE_RW)
+#define NV_PAGE_RW _PAGE_RW
+#elif defined(H_PAGE_RW)
+#define NV_PAGE_RW H_PAGE_RW
+#else
+#warning "The kernel does not provide page protection defines!" 
+#endif
+
 #if defined(_PAGE_4K_PFN)
 #define NV_PAGE_4K_PFN _PAGE_4K_PFN
 #elif defined(H_PAGE_4K_PFN)
@@ -112,7 +120,7 @@
 /* Don't attempt to mark sysmem pages as write combined on ppc64le */
 #define NV_PGPROT_WRITE_COMBINED(old_prot)    old_prot
 #define NV_PGPROT_READ_ONLY(old_prot)                                         \
-    __pgprot(pgprot_val((old_prot)) & ~_PAGE_RW)
+    __pgprot(pgprot_val((old_prot)) & ~NV_PAGE_RW)
 #else
 /* Writecombine is not supported */
 #undef NV_PGPROT_WRITE_COMBINED_DEVICE(old_prot)
diff -ur 367.57/kernel/common/inc/nv-proto.h 375.20/kernel/common/inc/nv-proto.h
--- 367.57/kernel/common/inc/nv-proto.h	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/common/inc/nv-proto.h	2016-11-16 02:53:45.000000000 +0300
@@ -82,6 +82,8 @@
 void        nv_uvm_notify_stop_device   (NvU8 *uuid);
 NV_STATUS   nv_uvm_event_interrupt      (NvU8 *uuid);
 
+void        nvidia_vgpu_vfio_init           (void);
+
 /* Move these to nv.h once implemented by other UNIX platforms */
 NvBool      nvidia_get_gpuid_list       (NvU32 *gpu_ids, NvU32 *gpu_count);
 int         nvidia_dev_get              (NvU32, nvidia_stack_t *);
diff -ur 367.57/kernel/common/inc/nv_uvm_interface.h 375.20/kernel/common/inc/nv_uvm_interface.h
--- 367.57/kernel/common/inc/nv_uvm_interface.h	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/common/inc/nv_uvm_interface.h	2016-11-16 02:53:45.000000000 +0300
@@ -949,6 +949,8 @@
     nvUvmInterfaceInitFaultInfo
 
     This function obtains fault buffer address, size and a few register mappings
+    for replayable faults, and creates a shadow buffer to store non-replayable
+    faults if the GPU supports it.
 
     Arguments:
         vaspace[IN]       - Pointer to vaSpace object associated with the gpu
@@ -956,15 +958,17 @@
 
     Error codes:
       NV_ERR_GENERIC
+      NV_ERR_NO_MEMORY
       NV_ERR_INVALID_ARGUMENT
 */
 NV_STATUS nvUvmInterfaceInitFaultInfo(uvmGpuAddressSpaceHandle vaSpace,
-    UvmGpuFaultInfo *pFaultInfo);
+                                      UvmGpuFaultInfo *pFaultInfo);
 
 /*******************************************************************************
     nvUvmInterfaceDestroyFaultInfo
 
     This function obtains destroys unmaps the fault buffer and clears faultInfo
+    for replayable faults, and frees the shadow buffer for non-replayable faults.
 
     Arguments:
         vaspace[IN]       - Pointer to vaSpace object associated with the gpu
@@ -975,7 +979,54 @@
       NV_ERR_INVALID_ARGUMENT
 */
 NV_STATUS nvUvmInterfaceDestroyFaultInfo(uvmGpuAddressSpaceHandle vaSpace,
-    UvmGpuFaultInfo *pFaultInfo);
+                                         UvmGpuFaultInfo *pFaultInfo);
+
+/*******************************************************************************
+    nvUvmInterfaceHasPendingNonReplayableFaults
+
+    This function tells whether there are pending non-replayable faults in the
+    client shadow fault buffer ready to be consumed.
+
+    NOTE: this function DOES NOT acquire the RM API or GPU locks. That is
+    because it is called during fault servicing, which could produce deadlocks.
+
+    Arguments:
+        pFaultInfo[IN]        - information provided by RM for fault handling.
+                                Contains a pointer to the shadow fault buffer
+        hasPendingFaults[OUT] - return value that tells if there are
+                                non-replayable faults ready to be consumed by
+                                the client
+
+    Error codes:
+      NV_ERR_INVALID_ARGUMENT
+*/
+NV_STATUS nvUvmInterfaceHasPendingNonReplayableFaults(UvmGpuFaultInfo *pFaultInfo,
+                                                      NvBool *hasPendingFaults);
+
+/*******************************************************************************
+    nvUvmInterfaceGetNonReplayableFaults
+
+    This function consumes all the non-replayable fault packets in the client
+    shadow fault buffer and copies them to the given buffer. It also returns the
+    number of faults that have been copied
+
+    NOTE: this function DOES NOT acquire the RM API or GPU locks. That is
+    because it is called during fault servicing, which could produce deadlocks.
+
+    Arguments:
+        pFaultInfo[IN]    - information provided by RM for fault handling.
+                            Contains a pointer to the shadow fault buffer
+        pFaultBuffer[OUT] - buffer provided by the client where fault buffers
+                            are copied when they are popped out of the shadow
+                            fault buffer (which is a circular queue).
+        numFaults[OUT]    - return value that tells the number of faults copied
+                            to the client's buffer
+                            
+    Error codes:
+      NV_ERR_INVALID_ARGUMENT
+*/
+NV_STATUS nvUvmInterfaceGetNonReplayableFaults(UvmGpuFaultInfo *pFaultInfo,
+                                               void *pFaultBuffer, NvU32 *numFaults);
 
 /*******************************************************************************
     nvUvmInterfaceInitAccessCntrInfo
diff -ur 367.57/kernel/common/inc/nv_uvm_types.h 375.20/kernel/common/inc/nv_uvm_types.h
--- 367.57/kernel/common/inc/nv_uvm_types.h	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/common/inc/nv_uvm_types.h	2016-11-16 02:53:40.000000000 +0300
@@ -474,6 +474,9 @@
         // here the non-replayable faults that need to be handled by UVM
         void* shadowBufferAddress;
         NvU32  bufferSize;
+
+        // Preallocated stack for functions called from the UVM isr top half
+        void *isr_sp;
     } nonReplayable;
     NvHandle faultBufferHandle;
 } UvmGpuFaultInfo;
diff -ur 367.57/kernel/common/inc/os-interface.h 375.20/kernel/common/inc/os-interface.h
--- 367.57/kernel/common/inc/os-interface.h	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/common/inc/os-interface.h	2016-11-16 02:53:47.000000000 +0300
@@ -1,6 +1,6 @@
 /* _NVRM_COPYRIGHT_BEGIN_
  *
- * Copyright 1999-2014 by NVIDIA Corporation.  All rights reserved.  All
+ * Copyright 1999-2016 by NVIDIA Corporation.  All rights reserved.  All
  * information contained herein is proprietary and confidential to NVIDIA
  * Corporation.  Any use, reproduction, or disclosure without the written
  * permission of NVIDIA Corporation is prohibited.
@@ -84,6 +84,8 @@
 NV_STATUS   NV_API_CALL  os_get_current_thread       (NvU64 *);
 char*       NV_API_CALL  os_string_copy              (char *, const char *);
 NvU32       NV_API_CALL  os_string_length            (const char *);
+NvU32       NV_API_CALL  os_strtoul                  (const char *, char **, NvU32);
+NvS32       NV_API_CALL  os_string_compare           (const char *, const char *);
 NvS32       NV_API_CALL  os_snprintf                 (char *, NvU32, const char *, ...);
 void        NV_API_CALL  os_log_error                (NvU32, NvU32, const char *, va_list);
 NvU8*       NV_API_CALL  os_mem_copy                 (NvU8 *, const NvU8 *, NvU32);
@@ -98,6 +100,8 @@
 NV_STATUS   NV_API_CALL  os_pci_write_byte           (void *, NvU32, NvU8);
 NV_STATUS   NV_API_CALL  os_pci_write_word           (void *, NvU32, NvU16);
 NV_STATUS   NV_API_CALL  os_pci_write_dword          (void *, NvU32, NvU32);
+NvBool      NV_API_CALL  os_remove_gpu_supported     (void);
+void        NV_API_CALL  os_remove_gpu               (NvU32, NvU8, NvU8);
 void*       NV_API_CALL  os_map_kernel_space         (NvU64, NvU64, NvU32, NvU32);
 void        NV_API_CALL  os_unmap_kernel_space       (void *, NvU64);
 void*       NV_API_CALL  os_map_user_space           (NvU64, NvU64, NvU32, NvU32, void **);
@@ -136,7 +140,7 @@
 NV_STATUS   NV_API_CALL  os_acquire_mutex            (void *);
 NV_STATUS   NV_API_CALL  os_cond_acquire_mutex       (void *);
 void        NV_API_CALL  os_release_mutex            (void *);
-void*       NV_API_CALL  os_alloc_semaphore          (NvU32, NvU32);
+void*       NV_API_CALL  os_alloc_semaphore          (NvU32);
 void        NV_API_CALL  os_free_semaphore           (void *);
 NV_STATUS   NV_API_CALL  os_acquire_semaphore        (void *);
 NV_STATUS   NV_API_CALL  os_release_semaphore        (void *);
@@ -151,7 +155,7 @@
 NvBool      NV_API_CALL  os_is_vgx_hyper             (void);
 NV_STATUS   NV_API_CALL  os_inject_vgx_msi           (NvU16, NvU64, NvU32);
 NvBool      NV_API_CALL  os_is_grid_supported        (void);
-void        NV_API_CALL  os_get_screen_info          (NvU64 *, NvU64 *);
+void        NV_API_CALL  os_get_screen_info          (NvU64 *, NvU16 *, NvU16 *, NvU16 *, NvU16 *);
 void        NV_API_CALL  os_bug_check                (NvU32, const char *);
 NV_STATUS   NV_API_CALL  os_lock_user_pages          (void *, NvU64, void **);
 NV_STATUS   NV_API_CALL  os_lookup_user_io_memory    (void *, NvU64, NvU64 **);
@@ -162,6 +166,8 @@
 NV_STATUS   NV_API_CALL  os_get_acpi_rsdp_from_uefi  (NvU32 *);
 void        NV_API_CALL  os_add_record_for_crashLog  (void *, NvU32);
 void        NV_API_CALL  os_delete_record_for_crashLog (void *);
+NV_STATUS   NV_API_CALL  os_call_vgpu_vfio           (void *, NvU32);
+
 
 /*
  * ---------------------------------------------------------------------------
Только в 375.20/kernel: conftest3353.c
Только в 375.20/kernel: conftest3563.c
Только в 375.20/kernel: conftest3672.c
Только в 375.20/kernel: conftest3674.c
Только в 375.20/kernel: conftest3722.c
Только в 375.20/kernel: conftest3768.c
Только в 375.20/kernel: conftest3784.c
Только в 375.20/kernel: conftest3800.c
Только в 375.20/kernel: conftest3824.c
Только в 375.20/kernel: conftest3831.c
Только в 375.20/kernel: conftest3839.c
Только в 375.20/kernel: conftest3841.c
Только в 375.20/kernel: conftest3858.c
Только в 375.20/kernel: conftest3867.c
diff -ur 367.57/kernel/conftest.sh 375.20/kernel/conftest.sh
--- 367.57/kernel/conftest.sh	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/conftest.sh	2016-11-16 02:53:45.000000000 +0300
@@ -27,6 +27,7 @@
 COMPILE_TEST_HEADERS="macros functions symbols types generic headers"
 
 # VGX_BUILD parameter defined only for VGX builds (vGPU Host driver)
+# VGX_KVM_BUILD parameter defined only vGPU builds on KVM hypervisor
 # GRID_BUILD parameter defined only for GRID builds (GRID Guest driver)
 
 test_xen() {
@@ -1202,7 +1203,7 @@
             #
             echo "$CONFTEST_PREAMBLE
             #include <linux/scatterlist.h>
-            int conftest_sg_alloc_table(void) {
+            void conftest_sg_alloc_table(void) {
             }" > conftest$$.c
 
             $CC $CFLAGS -c conftest$$.c > /dev/null 2>&1
@@ -1218,7 +1219,7 @@
 
             CODE="
             #include <linux/scatterlist.h>
-            int conftest_sg_alloc_table(void) {
+            void conftest_sg_alloc_table(void) {
                 sg_alloc_table();
             }"
 
@@ -1291,6 +1292,18 @@
             return
         ;;
 
+        nvidia_vgpu_kvm_build)
+           #
+           # Add config parameter if running on KVM host.
+           #
+           if [ -n "$VGX_KVM_BUILD" ]; then
+                echo "#define NV_VGPU_KVM_BUILD" | append_conftest "generic"
+            else
+                echo "#undef NV_VGPU_KVM_BUILD" | append_conftest "generic"
+            fi
+            return
+        ;;
+
         nvidia_grid_build)
             if [ -n "$GRID_BUILD" ]; then
                 echo "#define NV_GRID_BUILD" | append_conftest "generic"
@@ -1798,7 +1811,7 @@
             #
             # Determine if the DRM atomic modesetting subsystem is usable
             #
-            echo "$CONFTEST_PREAMBLE
+            CODE="
             #if defined(NV_DRM_DRMP_H_PRESENT)
             #include <drm/drmP.h>
             #endif
@@ -1828,36 +1841,9 @@
 
                 /* 2015-04-10 df63b9994eaf942afcdb946d27a28661d7dfbf2a */
                 for_each_crtc_in_state(s, c, cs, i) { }
-            }" > conftest$$.c
-
-            $CC $CFLAGS -c conftest$$.c > /dev/null 2>&1
-            rm -f conftest$$.c
-
-            if [ -f conftest$$.o ]; then
-                rm -f conftest$$.o
-
-                #
-                # Nonblocking commit support in the DRM atomic modesetting subsystem,
-                # has been added by commit -
-                #   2016-05-08:  9f2a7950e77abf00a2a87f3b4cbefa36e9b6009b
-                # It's risky to get it supported in release branches, therefore force
-                # disable modeset support on those kernels.
-                #
-                echo "$CONFTEST_PREAMBLE
-                #include <drm/drm_crtc.h>
-                int conftest_drm_atomic_modeset_nonblocking_commit_available(void) {
-                    return offsetof(struct drm_mode_config, helper_private);
-                }" > conftest$$.c
-
-                $CC $CFLAGS -c conftest$$.c > /dev/null 2>&1
-                rm -f conftest$$.c
+            }"
 
-                if [ -f conftest$$.o ]; then
-                    rm -f conftest$$.o
-                else
-                    echo "#define NV_DRM_ATOMIC_MODESET_AVAILABLE" | append_conftest "generic"
-                fi
-            fi
+            compile_check_conftest "$CODE" "NV_DRM_ATOMIC_MODESET_AVAILABLE" "" "generic"
         ;;
 
         drm_bus_present)
@@ -2035,6 +2021,23 @@
             compile_check_conftest "$CODE" "NV_DRM_DRIVER_HAS_SET_BUSID" "" "types"
         ;;
 
+        drm_driver_has_gem_prime_res_obj)
+            #
+            # Determine if the drm_driver structure has a 'gem_prime_res_obj'
+            # callback field.
+            #
+            # drm_driver::gem_prime_res_obj field was added by:
+            #   2014-07-01  3aac4502fd3f80dcf7e65dbf6edd8676893c1f46
+            #
+            CODE="
+            #include <drm/drmP.h>
+            int conftest_drm_driver_has_gem_prime_res_obj(void) {
+                return offsetof(struct drm_driver, gem_prime_res_obj);
+            }"
+
+            compile_check_conftest "$CODE" "NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ" "" "types"
+        ;;
+
         drm_crtc_state_has_connectors_changed)
             #
             # Determine if the crtc_state has a 'connectors_changed' field.
@@ -2126,6 +2129,36 @@
             compile_check_conftest "$CODE" "NV_BITMAP_CLEAR_PRESENT" "" "functions"
         ;;
 
+        pci_stop_and_remove_bus_device)
+            #
+            # Determine if the pci_stop_and_remove_bus_device() function is present.
+            # Added in 3.4-rc1 2012-02-25 210647af897af8ef2d00828aa2a6b1b42206aae6
+            #
+            CODE="
+            #include <linux/types.h>
+            #include <linux/pci.h>
+            void conftest_pci_stop_and_remove_bus_device() {
+                pci_stop_and_remove_bus_device();
+            }"
+
+            compile_check_conftest "$CODE" "NV_PCI_STOP_AND_REMOVE_BUS_DEVICE_PRESENT" "" "functions"
+        ;;
+
+        pci_remove_bus_device)
+            #
+            # Determine if the pci_remove_bus_device() function is present.
+            # Added before Linux-2.6.12-rc2 2005-04-16
+            #
+            CODE="
+            #include <linux/types.h>
+            #include <linux/pci.h>
+            void conftest_pci_remove_bus_device() {
+                pci_remove_bus_device();
+            }"
+
+            compile_check_conftest "$CODE" "NV_PCI_REMOVE_BUS_DEVICE_PRESENT" "" "functions"
+        ;;
+
         drm_atomic_set_mode_for_crtc)
             #
             # Determine if the function drm_atomic_set_mode_for_crtc() is
@@ -2170,7 +2203,7 @@
             # drm_mode_config_funcs::fb_create and drm_helper_mode_fill_fb_struct().
             #
             # The drm_mode_fb_cmd2 pointer through this call chain was made const by:
-            #   2015-11-11  1eb83451ba55d7a8c82b76b1591894ff2d4a95f2  drm: Pass the user drm_mode_fb_cmd2 as const to .fb_create()
+            #   2015-11-11  1eb83451ba55d7a8c82b76b1591894ff2d4a95f2
             #
             CODE="
             #include <drm/drm_crtc_helper.h>
@@ -2210,18 +2243,52 @@
         get_user_pages_remote)
             #
             # Determine if the function get_user_pages_remote() is
-            # present.
+            # present and has write/force parameters.
             #
             # get_user_pages_remote() was added by:
             #   2016 Feb 12: 1e9877902dc7e11d2be038371c6fbf2dfcd469d7
             #
-            CODE="
+            # get_user_pages[_remote]() write/force parameters
+            # replaced with gup_flags:
+            #   2016 Oct 13: 768ae309a96103ed02eb1e111e838c87854d8b51
+            #   2016 Oct 13: 9beae1ea89305a9667ceaab6d0bf46a045ad71e7
+            #
+            echo "$CONFTEST_PREAMBLE
             #include <linux/mm.h>
-            int conftest_get_user_pages_remote(void) {
+            void conftest_get_user_pages_remote(void) {
                 get_user_pages_remote();
-            }"
+            }" > conftest$$.c
+
+            $CC $CFLAGS -c conftest$$.c > /dev/null 2>&1
+            rm -f conftest$$.c
+
+            if [ -f conftest$$.o ]; then
+                echo "#undef NV_GET_USER_PAGES_REMOTE_PRESENT" | append_conftest "functions"
+                echo "#undef NV_GET_USER_PAGES_HAS_WRITE_AND_FORCE_ARGS" | append_conftest "functions"
+            else
+                echo "#define NV_GET_USER_PAGES_REMOTE_PRESENT" | append_conftest "functions"
+                rm -f conftest$$.o
 
-            compile_check_conftest "$CODE" "NV_GET_USER_PAGES_REMOTE_PRESENT" "" "functions"
+                echo "$CONFTEST_PREAMBLE
+                #include <linux/mm.h>
+                long get_user_pages(unsigned long start,
+                                    unsigned long nr_pages,
+                                    int write,
+                                    int force,
+                                    struct page **pages,
+                                    struct vm_area_struct **vmas) {
+                    return 0;
+                }" > conftest$$.c
+
+                $CC $CFLAGS -c conftest$$.c > /dev/null 2>&1
+                rm -f conftest$$.c
+
+                if [ -f conftest$$.o ]; then
+                    echo "#define NV_GET_USER_PAGES_HAS_WRITE_AND_FORCE_ARGS" | append_conftest "functions"
+                else
+                    echo "#undef NV_GET_USER_PAGES_HAS_WRITE_AND_FORCE_ARGS" | append_conftest "functions"
+                fi
+            fi
         ;;
 
         usleep_range)
@@ -2233,7 +2300,7 @@
             #
             CODE="
             #include <linux/delay.h>
-            int conftest_usleep_range(void) {
+            void conftest_usleep_range(void) {
                 usleep_range();
             }"
 
@@ -2311,6 +2378,54 @@
             fi
         ;;
 
+        drm_master_drop_has_from_release_arg)
+            #
+            # Determine if drm_driver::master_drop() has 'from_release' argument.
+            #
+            # Last argument 'bool from_release' has been removed by:
+            #  2016-06-21 : d6ed682eba54915ea56315bc2e5a33fca5922997
+            #
+            CODE="
+            #include <drm/drmP.h>
+            void conftest_drm_master_drop_has_from_release_arg(struct drm_driver *drv) {
+                drv->master_drop(NULL, NULL, false);
+            }"
+
+            compile_check_conftest "$CODE" "NV_DRM_MASTER_DROP_HAS_FROM_RELEASE_ARG" "" "types"
+        ;;
+
+        drm_mode_config_funcs_has_atomic_state_alloc)
+            #
+            # Determine if the 'drm_mode_config_funcs' structure has
+            # an 'atomic_state_alloc' field.
+            #
+            # added:   2015-05-18  036ef5733ba433760a3512bb5f7a155946e2df05
+            #
+            CODE="
+            #include <drm/drm_crtc.h>
+            int conftest_drm_mode_config_funcs_has_atomic_state_alloc(void) {
+                return offsetof(struct drm_mode_config_funcs, atomic_state_alloc);
+            }"
+
+            compile_check_conftest "$CODE" "NV_DRM_MODE_CONFIG_FUNCS_HAS_ATOMIC_STATE_ALLOC" "" "types"
+        ;;
+
+        drm_atomic_modeset_nonblocking_commit_available)
+            #
+            # Determine if nonblocking commit support avaiable in the DRM atomic
+            # modesetting subsystem.
+            #
+            # added:   2016-05-08  9f2a7950e77abf00a2a87f3b4cbefa36e9b6009b
+            #
+            CODE="
+            #include <drm/drm_crtc.h>
+            int conftest_drm_atomic_modeset_nonblocking_commit_available(void) {
+                return offsetof(struct drm_mode_config, helper_private);
+            }"
+
+            compile_check_conftest "$CODE" "NV_DRM_ATOMIC_MODESET_NONBLOCKING_COMMIT_AVAILABLE" "" "generic"
+        ;;
+
     esac
 }
 
diff -ur 367.57/kernel/Kbuild 375.20/kernel/Kbuild
--- 367.57/kernel/Kbuild	2016-10-04 05:58:49.000000000 +0300
+++ 375.20/kernel/Kbuild	2016-11-16 03:10:51.000000000 +0300
@@ -59,7 +59,7 @@
 EXTRA_CFLAGS += -I$(src)/common/inc
 EXTRA_CFLAGS += -I$(src)
 EXTRA_CFLAGS += -Wall -MD $(DEFINES) $(INCLUDES) -Wsign-compare -Wno-cast-qual -Wno-error
-EXTRA_CFLAGS += -D__KERNEL__ -DMODULE -DNVRM -DNV_VERSION_STRING=\"367.57\" -Wno-unused-function -Wuninitialized -fno-strict-aliasing -mno-red-zone -mcmodel=kernel -DNV_UVM_ENABLE -Wno-sign-compare -Wno-format-extra-args
+EXTRA_CFLAGS += -D__KERNEL__ -DMODULE -DNVRM -DNV_VERSION_STRING=\"375.20\" -Wno-unused-function -Wuninitialized -fno-strict-aliasing -mno-red-zone -mcmodel=kernel -DNV_UVM_ENABLE -Wno-sign-compare -Wno-format-extra-args
 
 #
 # Detect SGI UV systems and apply system-specific optimizations.
diff -ur 367.57/kernel/Makefile 375.20/kernel/Makefile
--- 367.57/kernel/Makefile	2016-10-04 05:58:49.000000000 +0300
+++ 375.20/kernel/Makefile	2016-11-16 03:10:51.000000000 +0300
@@ -59,13 +59,13 @@
     )
   endif
 
-  NV_KERNEL_MODULES ?= $(wildcard nvidia nvidia-uvm nvidia-modeset nvidia-drm)
+  NV_KERNEL_MODULES ?= $(wildcard nvidia nvidia-uvm nvidia-modeset nvidia-drm nvidia-vgpu-vfio)
   NV_KERNEL_MODULES := $(filter-out $(NV_EXCLUDE_KERNEL_MODULES), \
                                     $(NV_KERNEL_MODULES))
   NV_VERBOSE ?=
 
   KBUILD_PARAMS += KBUILD_VERBOSE=$(NV_VERBOSE)
-  KBUILD_PARAMS += -C $(KERNEL_SOURCES) M=$(PWD)
+  KBUILD_PARAMS += -C $(KERNEL_SOURCES) M=$(CURDIR)
   KBUILD_PARAMS += ARCH=$(ARCH)
   KBUILD_PARAMS += NV_KERNEL_SOURCES=$(KERNEL_SOURCES)
   KBUILD_PARAMS += NV_KERNEL_OUTPUT=$(KERNEL_OUTPUT)
diff -ur 367.57/kernel/nvidia/ebridge_linux.c 375.20/kernel/nvidia/ebridge_linux.c
--- 367.57/kernel/nvidia/ebridge_linux.c	2016-10-04 05:40:14.000000000 +0300
+++ 375.20/kernel/nvidia/ebridge_linux.c	2016-11-16 02:44:18.000000000 +0300
@@ -70,6 +70,8 @@
     .remove   = ebridge_remove,
 };
 
+static int init_succeeded = 0;
+
 static int
 ebridge_probe
 (
@@ -251,13 +253,19 @@
         goto out;
     }
 
+    init_succeeded = 1;
+
 out:
     return rc;
 }
 
 void ebridge_exit(void)
 {
-    pci_unregister_driver(&ebridge_pci_driver);
+    if (init_succeeded)
+    {
+        pci_unregister_driver(&ebridge_pci_driver);
+        init_succeeded = 0;
+    }
 }
 
 
diff -ur 367.57/kernel/nvidia/ibmnpu_export.h 375.20/kernel/nvidia/ibmnpu_export.h
--- 367.57/kernel/nvidia/ibmnpu_export.h	2016-10-04 05:40:14.000000000 +0300
+++ 375.20/kernel/nvidia/ibmnpu_export.h	2016-11-16 02:44:18.000000000 +0300
@@ -35,21 +35,14 @@
 #define IBMNPU_LINK_NAME                                "NPU Link"
 #define IBMNPU_MAX_BARS                                 2
 
-#ifndef PCI_CLASS_BRIDGE_NPU
 #define PCI_CLASS_BRIDGE_NPU                            0x0680
-#endif
 
-#ifndef PCI_DEVICE_ID_IBM_NPU
 #define PCI_DEVICE_ID_IBM_NPU                           0x04ea
-#endif
 
-#ifndef PCI_VENDOR_ID_IBM
 #define PCI_VENDOR_ID_IBM                               0x1014
-#endif
 
-#ifndef PCI_REVISION_ID_IBM_NPU
-#define PCI_REVISION_ID_IBM_NPU                         0x0
-#endif
+#define PCI_REVISION_ID_IBM_NPU_P8                      0x0
+#define PCI_REVISION_ID_IBM_NPU_P9                      0x1
 
 
 /*
diff -ur 367.57/kernel/nvidia/nv.c 375.20/kernel/nvidia/nv.c
--- 367.57/kernel/nvidia/nv.c	2016-11-09 18:48:50.727829830 +0300
+++ 375.20/kernel/nvidia/nv.c	2016-11-25 17:45:06.357236113 +0300
@@ -1,6 +1,6 @@
 /* _NVRM_COPYRIGHT_BEGIN_
  *
- * Copyright 1999-2014 by NVIDIA Corporation.  All rights reserved.  All
+ * Copyright 1999-2016 by NVIDIA Corporation.  All rights reserved.  All
  * information contained herein is proprietary and confidential to NVIDIA
  * Corporation.  Any use, reproduction, or disclosure without the written
  * permission of NVIDIA Corporation is prohibited.
@@ -22,10 +22,15 @@
 #include "nv_uvm_interface.h"
 #endif
 
+#if defined(NV_VGPU_KVM_BUILD) 
+#include "nv-vgpu-vfio-interface.h"
+#endif
+
 #include "nv-frontend.h"
 #include "nv-hypervisor.h"
+#include "nv-kthread-q.h"
 
-/* 
+/*
  * The module information macros for Linux single-module builds
  * are present in nv-frontend.c.
  */
@@ -125,6 +130,8 @@
 
 struct semaphore nv_linux_devices_lock;
 
+static NvTristate nv_chipset_is_io_coherent = NV_TRISTATE_INDETERMINATE;
+
 static int nv_use_threaded_interrupts = 0;
 
 // allow an easy way to convert all debug printfs related to events
@@ -300,19 +307,20 @@
  *** EXPORTS to Linux Kernel
  ***/
 
-static long          nvidia_unlocked_ioctl (struct file *, unsigned int, unsigned long);
-static void          nvidia_isr_tasklet_bh (unsigned long);
-static irqreturn_t   nvidia_isr_kthread_bh (int, void *);
-static irqreturn_t   nvidia_isr_common_bh  (void *);
+static long          nvidia_unlocked_ioctl  (struct file *, unsigned int, unsigned long);
+static void          nvidia_isr_tasklet_bh  (unsigned long);
+static irqreturn_t   nvidia_isr_kthread_bh  (int, void *);
+static irqreturn_t   nvidia_isr_common_bh   (void *);
+static void          nvidia_isr_bh_unlocked (void *);
 #if !defined(NV_IRQ_HANDLER_T_PRESENT) || (NV_IRQ_HANDLER_T_ARGUMENT_COUNT == 3)
-static irqreturn_t   nvidia_isr            (int, void *, struct pt_regs *);
+static irqreturn_t   nvidia_isr             (int, void *, struct pt_regs *);
 #else
-static irqreturn_t   nvidia_isr            (int, void *);
+static irqreturn_t   nvidia_isr             (int, void *);
 #endif
-static void          nvidia_rc_timer       (unsigned long);
+static void          nvidia_rc_timer        (unsigned long);
 
-static int           nvidia_ctl_open       (struct inode *, struct file *);
-static int           nvidia_ctl_close      (struct inode *, struct file *);
+static int           nvidia_ctl_open        (struct inode *, struct file *);
+static int           nvidia_ctl_close       (struct inode *, struct file *);
 
 #if defined(NV_PCI_ERROR_RECOVERY)
 static pci_ers_result_t nvidia_pci_error_detected   (struct pci_dev *, enum pci_channel_state);
@@ -633,7 +641,7 @@
 
     if (nv_multiple_kernel_modules)
     {
-        nv_printf(NV_DBG_INFO, "NVRM: nvidia module instance %d\n", 
+        nv_printf(NV_DBG_INFO, "NVRM: nvidia module instance %d\n",
                   nv_module_instance);
     }
 
@@ -687,7 +695,7 @@
     nv->os_state = (void *) &nv_ctl_device;
     nv_lock_init_locks(nv);
     nv_state_init_gpu_uuid_cache(nv);
-    
+
     count = nvos_count_devices(sp);
     if (count == 0)
     {
@@ -709,7 +717,11 @@
     nv_linux_devices = NULL;
     NV_INIT_MUTEX(&nv_linux_devices_lock);
 
-    /* 
+#if defined(NV_VGPU_KVM_BUILD)
+    nvidia_vgpu_vfio_init();
+#endif
+
+    /*
      * Determine the TCE bypass mode here so it can be used during device
      * probe. This must happen after the call to rm_init_rm(), because the
      * registry must be initialized.
@@ -838,6 +850,10 @@
     {
         nv_printf(NV_DBG_ERRORS," (using threaded interrupts)");
     }
+    else
+    {
+        nv_printf(NV_DBG_ERRORS," (using tasklets)");
+    }
 
     if (__nv_patches[0].short_description != NULL)
     {
@@ -868,7 +884,7 @@
         if (nvidia_p2p_page_t_cache == NULL)
         {
             rc = -ENOMEM;
-            nv_printf(NV_DBG_ERRORS, 
+            nv_printf(NV_DBG_ERRORS,
                       "NVRM: p2p page cache allocation failed\n");
             goto failed;
         }
@@ -985,7 +1001,7 @@
     return rc;
 }
 
-void __exit nvidia_exit_module(void)
+void nvidia_exit_module(void)
 {
     nvidia_stack_t *sp = __nv_init_sp;
 
@@ -1051,7 +1067,7 @@
 }
 
 
-/* 
+/*
  * Module entry and exit functions for Linux single-module builds
  * are present in nv-frontend.c.
  */
@@ -1165,7 +1181,7 @@
     nv_linux_state_t *nvl = NULL;
     nv_state_t *nv;
     NvU8 *dev_uuid;
- 
+
     LOCK_NV_LINUX_DEVICES();
 
     for (nvl = nv_linux_devices; nvl; nvl = nvl->next)
@@ -1209,7 +1225,7 @@
 
     LOCK_NV_LINUX_DEVICES();
 
-    /* 
+    /*
      * Take two passes through the list. The first pass just looks for the UUID.
      * The second looks for the target or missing UUIDs. It would be nice if
      * this could be done in a single pass by remembering which nvls are missing
@@ -1292,6 +1308,7 @@
 #endif
     int rc = 0;
     NvU8 *uuid;
+    NvBool kthread_init = NV_FALSE;
 
     /*
      * map the memory and allocate isr on first open
@@ -1416,6 +1433,15 @@
         tasklet_init(&nvl->tasklet, nvidia_isr_tasklet_bh, (NvUPtr)NV_STATE_PTR(nvl));
     }
 
+    if (!(nv->flags & NV_FLAG_PERSISTENT_SW_STATE))
+    {
+        nv_kthread_q_item_init(&nvl->bottom_half_q_item, nvidia_isr_bh_unlocked, (void *)nv);
+        rc = nv_kthread_q_init(&nvl->bottom_half_q, nv_device_name);
+        if (rc != 0)
+            goto failed;
+        kthread_init = NV_TRUE;
+    }
+
     if (!rm_init_adapter(sp, nv))
     {
         if (!(nv->flags & NV_FLAG_PERSISTENT_SW_STATE) && !nv_use_threaded_interrupts)
@@ -1466,6 +1492,9 @@
         NV_PCI_DISABLE_MSI(nvl->dev);
     }
 #endif
+    if (kthread_init && !(nv->flags & NV_FLAG_PERSISTENT_SW_STATE))
+        nv_kthread_q_stop(&nvl->bottom_half_q);
+
     nv_dev_free_stacks(nvl);
     return rc;
 }
@@ -1670,6 +1699,7 @@
         }
         else
         {
+            nv_kthread_q_stop(&nvl->bottom_half_q);
             nv_shutdown_adapter(sp, nv, nvl);
         }
     }
@@ -1730,6 +1760,7 @@
     nv_file_private_t *nvfp = NV_GET_FILE_PRIVATE(file);
     nvidia_stack_t *sp = nvfp->sp;
     unsigned int i;
+    NvBool bRemove = NV_FALSE;
 
     NV_CHECK_PCI_CONFIG_SPACE(sp, nv, TRUE, TRUE, NV_MAY_SLEEP());
 
@@ -1745,6 +1776,8 @@
 
     down(&nvl->ldata_lock);
     nv_close_device(nv, sp);
+    bRemove = (NV_ATOMIC_READ(nvl->usage_count) == 0) &&
+                rm_get_device_remove_flag(sp, nv->gpu_id);
     up(&nvl->ldata_lock);
 
     for (i = 0; i < NV_FOPS_STACK_INDEX_COUNT; ++i)
@@ -1755,6 +1788,13 @@
     nv_free_file_private(nvfp);
     NV_SET_FILE_PRIVATE(file, NULL);
 
+#if defined(NV_PCI_STOP_AND_REMOVE_BUS_DEVICE)
+    if (bRemove)
+    {
+        NV_PCI_STOP_AND_REMOVE_BUS_DEVICE(nvl->dev);
+    }
+#endif
+
     nv_kmem_cache_free_stack(sp);
 
     return 0;
@@ -1770,6 +1810,15 @@
     nv_file_private_t *nvfp = NV_GET_FILE_PRIVATE(file);
     unsigned long eflags;
 
+    //
+    // File descriptors with an attached memory descriptor are intended
+    // for use only via mmap(). poll() operations are forbidden
+    //
+    if (nvfp->fd_memdesc.bValid)
+    {
+        return POLLERR;
+    }
+
     if ((file->f_flags & O_NONBLOCK) == 0)
         poll_wait(file, &nvfp->waitqueue, wait);
 
@@ -1874,6 +1923,16 @@
     arg_size = _IOC_SIZE(cmd);
     arg_cmd  = _IOC_NR(cmd);
 
+    //
+    // File descriptors with an attached memory descriptor are intended
+    // for use only via mmap(). ioctl() operations are forbidden
+    //
+    if (nvfp->fd_memdesc.bValid)
+    { 
+        status = -EINVAL;
+        goto done;
+    }
+
     if (arg_cmd == NV_ESC_IOCTL_XFER_CMD)
     {
         if (arg_size != sizeof(nv_ioctl_xfer_t))
@@ -2050,11 +2109,11 @@
     nv_linux_state_t *nvl = (void *) arg;
     nv_state_t *nv = NV_STATE_PTR(nvl);
     NvU32 need_to_run_bottom_half_gpu_lock_held = 0;
-    BOOL rm_handled = FALSE,  uvm_handled = FALSE, mmu_faults_copied = FALSE;
-    NvU32 faultsCopied = 0;
+    BOOL rm_handled = FALSE,  uvm_handled = FALSE, rm_fault_handling_needed = FALSE;
+    NvU32 rm_serviceable_fault_cnt = 0;
 
-    rm_gpu_copy_mmu_faults_unlocked(nvl->sp[NV_DEV_STACK_ISR], nv, &faultsCopied);
-    mmu_faults_copied = (faultsCopied != 0);
+    rm_gpu_copy_mmu_faults_unlocked(nvl->sp[NV_DEV_STACK_ISR], nv, &rm_serviceable_fault_cnt);
+    rm_fault_handling_needed = (rm_serviceable_fault_cnt != 0);
 
 #if defined (NV_UVM_ENABLE)
     if (!NV_IS_GVI_DEVICE(nv))
@@ -2080,23 +2139,26 @@
     rm_handled = rm_isr(nvl->sp[NV_DEV_STACK_ISR], nv,
                         &need_to_run_bottom_half_gpu_lock_held);
 
-    //
-    // If the rm_isr does not need to run a bottom half, but mmu_faults_copied
-    // indicates that it needs a bottom half, then we need to indicate to the
-    // bottom half that it must acquire the GPU lock:
-    //
-    if (!need_to_run_bottom_half_gpu_lock_held && mmu_faults_copied)
-        nv->flags |=  NV_FLAG_ACQ_GPU_LOCK;
-
-    if (need_to_run_bottom_half_gpu_lock_held || mmu_faults_copied)
+    if (need_to_run_bottom_half_gpu_lock_held)
     {
         if (nv_use_threaded_interrupts)
             return IRQ_WAKE_THREAD;
 
         tasklet_schedule(&nvl->tasklet);
     }
+    else
+    {
+        //
+        // If rm_isr does not need to run a bottom half and mmu_faults_copied
+        // indicates that bottom half is needed, then we enqueue a kthread based
+        // bottom half rather than tasklet as this specific bottom_half will acquire
+        // GPU lock
+        //
+        if (rm_fault_handling_needed)
+            nv_kthread_q_schedule_q_item(&nvl->bottom_half_q, &nvl->bottom_half_q_item);
+    }
 
-    return IRQ_RETVAL(rm_handled || uvm_handled || mmu_faults_copied);
+    return IRQ_RETVAL(rm_handled || uvm_handled || rm_fault_handling_needed);
 }
 
 static irqreturn_t
@@ -2131,6 +2193,19 @@
 }
 
 static void
+nvidia_isr_bh_unlocked(
+    void * args
+)
+{
+    nv_state_t *nv = (nv_state_t *) args;
+    nv_linux_state_t *nvl = NV_GET_NVL_FROM_NV_STATE(nv);
+    nvidia_stack_t *sp = nvl->sp[NV_DEV_STACK_ISR_BH];
+
+    NV_CHECK_PCI_CONFIG_SPACE(sp, nv, TRUE, FALSE, FALSE);
+    rm_isr_bh_unlocked(sp, nv);
+}
+
+static void
 nvidia_rc_timer(
     unsigned long data
 )
@@ -2362,7 +2437,71 @@
     return NV_OK;
 }
 
-/* 
+/*
+  This creates a dummy nvalloc_t for system memory addresses not known to
+  the operating system.
+*/
+NV_STATUS NV_API_CALL nv_register_user_physical_addresses(
+    nv_state_t *nv,
+    NvU64       page_count,
+    NvU64      *phys_addr,
+    void     * *priv_data
+)
+{
+    nv_alloc_t *at;
+    NvU64 i;
+    nv_linux_state_t *nvl = NV_GET_NVL_FROM_NV_STATE(nv);
+
+    nv_printf(NV_DBG_MEMINFO, "NVRM: VM: nv_register_user_physical_addresses: 0x%x\n", page_count);
+
+    at = nvos_create_alloc(nvl->dev, page_count);
+
+    if (at == NULL)
+        return NV_ERR_NO_MEMORY;
+
+    /*
+     * Anonymous memory currently must be write-back cacheable, and we can't
+     * enforce contiguity.
+     */
+    at->flags = nv_alloc_init_flags(1, 0, 0);
+    at->flags |= NV_ALLOC_TYPE_USER;
+
+    at->order = get_order(at->num_pages * PAGE_SIZE);
+
+    for (i = 0; i < page_count; i++)
+        at->page_table[i]->phys_addr = phys_addr[i];
+
+    // No struct page array exists for this memory.
+    at->user_pages = NULL;
+
+    *priv_data = at;
+
+    NV_PRINT_AT(NV_DBG_MEMINFO, at);
+
+    return NV_OK;
+}
+
+NV_STATUS NV_API_CALL nv_unregister_user_physical_addresses(
+    nv_state_t *nv,
+    NvU64       page_count,
+    void *      priv_data
+)
+{
+    nv_alloc_t *at = priv_data;
+
+    nv_printf(NV_DBG_MEMINFO, "NVRM: VM: nv_unregister_user_physical_addresses: 0x%x\n", page_count);
+
+    NV_PRINT_AT(NV_DBG_MEMINFO, at);
+
+    WARN_ON(!NV_ALLOC_MAPPING_USER(at->flags));
+
+    nvos_free_alloc(at);
+
+    return NV_OK;
+}
+
+
+/*
  * By registering user pages, we create a dummy nv_alloc_t for it, so that the
  * rest of the RM can treat it like any other alloc.
  *
@@ -2390,7 +2529,7 @@
         return NV_ERR_NO_MEMORY;
     }
 
-    /* 
+    /*
      * Anonymous memory currently must be write-back cacheable, and we can't
      * enforce contiguity.
      */
@@ -2701,6 +2840,69 @@
     wake_up_interruptible(&nvfp->waitqueue);
 }
 
+nv_fd_memdesc_t* NV_API_CALL nv_get_fd_memdesc(
+    void       *file
+)
+{
+    nv_file_private_t *nvfp = file;
+
+    if (!file)
+        return NULL;
+    
+    return &(nvfp->fd_memdesc);
+}
+
+NV_STATUS NV_API_CALL nv_add_fd_memdesc_to_fd(
+    NvU32                  fd,
+    const nv_fd_memdesc_t *pFdMemdesc
+)
+{
+    struct file *filp = NULL;
+    nv_file_private_t *nvfp = NULL;
+    dev_t rdev = 0;
+    NV_STATUS status = NV_ERR_INVALID_ARGUMENT;
+
+    filp = fget(fd);
+    
+    if (filp == NULL)
+    {
+        return status;
+    }
+    if (NV_FILE_INODE(filp))
+    {
+        rdev = (NV_FILE_INODE(filp))->i_rdev;
+    }
+    else
+    {
+        goto done;
+    }
+    // Only allow adding the fd memdesc if the struct file is an open of
+    // the NVIDIA control device file. Note it is not safe to interpret
+    // the file private data as an nv_file_private_t until it passes this check.
+    if ((MAJOR(rdev) != NV_MAJOR_DEVICE_NUMBER) ||
+        (MINOR(rdev) != NV_CONTROL_DEVICE_MINOR))
+    {
+        goto done;
+    }
+    
+    nvfp = NV_GET_FILE_PRIVATE(filp);
+    if (!nvfp->fd_memdesc.bValid)
+    {
+        nvfp->fd_memdesc = *pFdMemdesc;
+        status = NV_OK;
+    }
+    else
+    {
+        status = NV_ERR_STATE_IN_USE;
+    }
+
+done:
+    
+    fput(filp);
+    
+    return status;
+}
+
 int NV_API_CALL nv_get_event(
     nv_state_t *nv,
     void       *file,
@@ -2851,6 +3053,27 @@
     return count;
 }
 
+NvBool nvos_is_chipset_io_coherent(void)
+{
+    if (nv_chipset_is_io_coherent == NV_TRISTATE_INDETERMINATE)
+    {
+        nvidia_stack_t *sp = NULL;
+        if (nv_kmem_cache_alloc_stack(&sp) != 0)
+        {
+            nv_printf(NV_DBG_ERRORS,
+              "NVRM: cannot allocate stack for platform coherence check callback \n");
+            WARN_ON(1);
+            return NV_FALSE; 
+        }
+        
+        nv_chipset_is_io_coherent = rm_is_chipset_io_coherent(sp);
+
+        nv_kmem_cache_free_stack(sp);
+    }
+
+    return nv_chipset_is_io_coherent;
+}
+
 static BOOL nv_treat_missing_irq_as_error(void)
 {
 #if defined(NV_LINUX_PCIE_MSI_SUPPORTED)
@@ -3102,7 +3325,7 @@
 
     pci_set_master(dev);
 
-#if defined(CONFIG_VGA_ARB)
+#if defined(CONFIG_VGA_ARB) && !defined(NVCPU_PPC64LE)
 #if defined(VGA_DEFAULT_DEVICE)
     vga_tryget(VGA_DEFAULT_DEVICE, VGA_RSRC_LEGACY_MASK);
 #endif
@@ -3111,7 +3334,7 @@
 
     if (rm_get_cpu_type(sp, &nv_cpu_type) != NV_OK)
         nv_printf(NV_DBG_ERRORS, "NVRM: error retrieving cpu type\n");
- 
+
     if (NV_IS_GVI_DEVICE(nv))
     {
         if (!rm_gvi_init_private_state(sp, nv))
@@ -3208,7 +3431,7 @@
         tnvl->next = nvl;
     }
     UNLOCK_NV_LINUX_DEVICES();
-    if (nvidia_frontend_add_device((void *)&nv_fops, nvl) != 0) 
+    if (nvidia_frontend_add_device((void *)&nv_fops, nvl) != 0)
         goto err_zero_dev;
 
 #if defined(NV_PM_VT_SWITCH_REQUIRED_PRESENT)
@@ -3217,6 +3440,11 @@
 
     nv_procfs_add_gpu(nvl);
 
+#if defined(NV_VGPU_KVM_BUILD)
+    if (nvidia_vgpu_vfio_probe(nvl->dev) != NV_OK)
+        nv_printf(NV_DBG_ERRORS, "NVRM: Failed to register device to vGPU VFIO module");
+#endif
+
     nv_kmem_cache_free_stack(sp);
 
     return 0;
@@ -3295,7 +3523,11 @@
     pm_vt_switch_unregister(&dev->dev);
 #endif
 
-    /* Update the frontend data structures */
+#if defined(NV_VGPU_KVM_BUILD)
+    nvidia_vgpu_vfio_remove(nvl->dev);
+#endif
+
+   /* Update the frontend data structures */
     nvidia_frontend_remove_device((void *)&nv_fops, nvl);
 
 #if defined(NV_UVM_ENABLE)
@@ -3402,6 +3634,11 @@
             {
                 tasklet_kill(&nvl->tasklet);
             }
+
+            if (!(nv->flags & NV_FLAG_PERSISTENT_SW_STATE))
+            {
+                nv_kthread_q_stop(&nvl->bottom_half_q);
+            }
             nv_disable_pat_support();
             nv_pci_save_state(dev);
             break;
@@ -3415,6 +3652,14 @@
             {
                 tasklet_init(&nvl->tasklet, nvidia_isr_tasklet_bh, (NvUPtr)NV_STATE_PTR(nvl));
             }
+
+            if (!(nv->flags & NV_FLAG_PERSISTENT_SW_STATE))
+            {
+                nv_kthread_q_item_init(&nvl->bottom_half_q_item, nvidia_isr_bh_unlocked, (void *)nv);
+                status = nv_kthread_q_init(&nvl->bottom_half_q, nv_device_name);
+                if (status != NV_OK)
+                    break;
+            }
             status = rm_power_management(sp, nv, 0, power_state);
             break;
 
@@ -3758,7 +4003,7 @@
      */
     if ((dma_addr & DMA_BIT_MASK(32)) != 0)
     {
-        /* 
+        /*
          * Huge DDW not available - page 0 mapped to non-zero address below
          * the 32-bit line.
          */
diff -ur 367.57/kernel/nvidia/nv-frontend.c 375.20/kernel/nvidia/nv-frontend.c
--- 367.57/kernel/nvidia/nv-frontend.c	2016-11-09 18:48:50.693838916 +0300
+++ 375.20/kernel/nvidia/nv-frontend.c	2016-11-25 17:45:06.322245906 +0300
@@ -36,9 +36,11 @@
 // minor number table
 nvidia_module_t *nv_minor_num_table[NV_FRONTEND_CONTROL_DEVICE_MINOR_MAX + 1];
 
+#if (NV_BUILD_MODULE_INSTANCES != 0)
+#if defined(CONFIG_PROC_FS)
 static struct proc_dir_entry *nv_proc_topdir;
-
-#if (NV_BUILD_MODULE_INSTANCES == 0)
+#endif
+#else
 int nvidia_init_module(void);
 void nvidia_exit_module(void);
 #endif
@@ -341,6 +343,33 @@
     return rc;
 }
 
+static int __init nv_init_module(void)
+{
+    int status = 0;
+#if (NV_BUILD_MODULE_INSTANCES != 0)
+#if defined(CONFIG_PROC_FS)
+    // Create proc topdir entry
+    nv_proc_topdir = NV_CREATE_PROC_DIR("driver/nvidia", NULL);
+    if (nv_proc_topdir == NULL)
+        printk("NVRM: failed to register procfs!\n");
+#endif
+#else
+    status = nvidia_init_module();
+#endif
+    return status;
+}
+
+static void nv_exit_module(void)
+{
+#if (NV_BUILD_MODULE_INSTANCES != 0)
+#if defined(CONFIG_PROC_FS)
+    if ((nv_num_instances == 0) && (nv_proc_topdir != NULL))
+        NV_REMOVE_PROCE_ENTRY(nv_proc_topdir);
+#endif
+#else
+    nvidia_exit_module();
+#endif
+}
 
 static int __init nvidia_frontend_init_module(void)
 {
@@ -351,50 +380,35 @@
     memset(nv_minor_num_table, 0, sizeof(nv_minor_num_table));
     NV_INIT_MUTEX(&nv_module_table_lock);
 
-    // register char device
-    status = register_chrdev(NV_MAJOR_DEVICE_NUMBER, "nvidia-frontend", &nv_frontend_fops);
+    status = nv_init_module();
     if (status < 0)
     {
-        printk("NVRM: register_chrdev() failed!\n");
         return status;
     }
 
-#if (NV_BUILD_MODULE_INSTANCES != 0)
-#if defined(CONFIG_PROC_FS) 
-    // Create proc topdir entry
-    nv_proc_topdir = NV_CREATE_PROC_DIR("driver/nvidia", NULL);
-    if (nv_proc_topdir == NULL)
-        printk("NVRM: failed to register procfs!\n");
-#endif
-#else
-    status = nvidia_init_module();
+    // register char device
+    status = register_chrdev(NV_MAJOR_DEVICE_NUMBER, "nvidia-frontend", &nv_frontend_fops);
     if (status < 0)
     {
-        printk("NVRM: NVIDIA init module failed!\n");
-        unregister_chrdev(NV_MAJOR_DEVICE_NUMBER, "nvidia-frontend");
+        printk("NVRM: register_chrdev() failed!\n");
+        nv_exit_module();
     }
-#endif
 
     return status;
 }
 
 static void __exit nvidia_frontend_exit_module(void)
 {
-#if (NV_BUILD_MODULE_INSTANCES == 0)
-    nvidia_exit_module();
-#endif
-
-    // if no nvidia_module registered, cleanup and unregister char dev
-    if (nv_num_instances == 0)
+    /* 
+     * If this is the last nvidia_module to be unregistered, cleanup and
+     * unregister char dev
+     */
+    if (nv_num_instances == 1)
     {
         unregister_chrdev(NV_MAJOR_DEVICE_NUMBER, "nvidia-frontend");
-#if defined(CONFIG_PROC_FS) 
-        if (NV_BUILD_MODULE_INSTANCES != 0 && (nv_proc_topdir != NULL))
-        {
-            NV_REMOVE_PROC_ENTRY(nv_proc_topdir);
-        }
-#endif
     }
+
+    nv_exit_module();
 }
 
 module_init(nvidia_frontend_init_module);
diff -ur 367.57/kernel/nvidia/nv_gpu_ops.h 375.20/kernel/nvidia/nv_gpu_ops.h
--- 367.57/kernel/nvidia/nv_gpu_ops.h	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/nvidia/nv_gpu_ops.h	2016-11-16 02:53:40.000000000 +0300
@@ -286,6 +286,10 @@
 
 NV_STATUS nvGpuOpsDestroyFaultInfo(struct gpuAddressSpace *vaSpace, gpuFaultInfo *pFaultInfo);
 
+NV_STATUS nvGpuOpsHasPendingNonReplayableFaults(gpuFaultInfo *pFaultInfo, NvBool *hasPendingFaults);
+
+NV_STATUS nvGpuOpsGetNonReplayableFaults(gpuFaultInfo *pFaultInfo, void *faultBuffer, NvU32 *numFaults);
+
 NV_STATUS nvGpuOpsGetPageLevelInfo(struct gpuAddressSpace *vaSpace, NvU64 vAddr, struct gpuPageLevelInfo *pPageLevelInfo);
 
 NV_STATUS nvGpuOpsGetChannelPhysInfo(NvHandle hClient, NvHandle hChannel, struct gpuChannelPhysInfo *pChannelInfo);
diff -ur 367.57/kernel/nvidia/nvidia.Kbuild 375.20/kernel/nvidia/nvidia.Kbuild
--- 367.57/kernel/nvidia/nvidia.Kbuild	2016-10-04 05:58:49.000000000 +0300
+++ 375.20/kernel/nvidia/nvidia.Kbuild	2016-11-16 03:10:51.000000000 +0300
@@ -128,6 +128,7 @@
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += proc_remove
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += pm_vt_switch_required
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += drm_driver_has_set_busid
+NV_CONFTEST_FUNCTION_COMPILE_TESTS += drm_driver_has_gem_prime_res_obj
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += xen_ioemu_inject_msi
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += phys_to_dma
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += get_dma_ops
@@ -136,6 +137,8 @@
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += for_each_online_node
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += node_end_pfn
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += pci_bus_address
+NV_CONFTEST_FUNCTION_COMPILE_TESTS += pci_stop_and_remove_bus_device
+NV_CONFTEST_FUNCTION_COMPILE_TESTS += pci_remove_bus_device
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += request_threaded_irq
 
 NV_CONFTEST_TYPE_COMPILE_TESTS += i2c_adapter
@@ -160,6 +163,7 @@
 NV_CONFTEST_TYPE_COMPILE_TESTS += noncoherent_swiotlb_dma_ops
 
 NV_CONFTEST_GENERIC_COMPILE_TESTS += dom0_kernel_present
+NV_CONFTEST_GENERIC_COMPILE_TESTS += nvidia_vgpu_kvm_build
 NV_CONFTEST_GENERIC_COMPILE_TESTS += drm_available
 NV_CONFTEST_GENERIC_COMPILE_TESTS += nvidia_grid_build
 NV_CONFTEST_GENERIC_COMPILE_TESTS += get_user_pages_remote
diff -ur 367.57/kernel/nvidia/nvidia-sources.Kbuild 375.20/kernel/nvidia/nvidia-sources.Kbuild
--- 367.57/kernel/nvidia/nvidia-sources.Kbuild	2016-10-04 05:58:49.000000000 +0300
+++ 375.20/kernel/nvidia/nvidia-sources.Kbuild	2016-11-16 05:15:52.000000000 +0300
@@ -22,8 +22,10 @@
 NVIDIA_SOURCES += nvidia/os-usermap.c
 NVIDIA_SOURCES += nvidia/nv-modeset-interface.c
 NVIDIA_SOURCES += nvidia/nv-pci-table.c
+NVIDIA_SOURCES += nvidia/nv-kthread-q.c
+NVIDIA_SOURCES += nvidia/nv-kthread-q-selftest.c
 NVIDIA_SOURCES += nvidia/nv_uvm_interface.c
-NVIDIA_SOURCES += nvidia/nvlink_linux.c
-NVIDIA_SOURCES += nvidia/nvlink_pci.c
 NVIDIA_SOURCES += nvidia/ebridge_linux.c
 NVIDIA_SOURCES += nvidia/ibmnpu_linux.c
+NVIDIA_SOURCES += nvidia/nvlink_linux.c
+NVIDIA_SOURCES += nvidia/nvlink_pci.c
Binary files 367.57/kernel/nvidia/nv-kernel.o_binary and 375.20/kernel/nvidia/nv-kernel.o_binary differ
Только в 375.20/kernel/nvidia: nv-kthread-q.c
Только в 375.20/kernel/nvidia: nv-kthread-q-selftest.c
diff -ur 367.57/kernel/nvidia/nvlink_common.h 375.20/kernel/nvidia/nvlink_common.h
--- 367.57/kernel/nvidia/nvlink_common.h	2016-10-04 05:40:14.000000000 +0300
+++ 375.20/kernel/nvidia/nvlink_common.h	2016-11-16 02:44:18.000000000 +0300
@@ -68,7 +68,7 @@
     NvU8    bus;
     NvU8    device;
     NvU8    function;
-    NvU16   pciDeviceId;
+    NvU32   pciDeviceId;
     NvU32   irq;
     NvBool  intHooked;
     struct  nvlink_pci_bar_info bars[MAX_NVLINK_BARS];
diff -ur 367.57/kernel/nvidia/nvlink_proto.h 375.20/kernel/nvidia/nvlink_proto.h
--- 367.57/kernel/nvidia/nvlink_proto.h	2016-10-04 05:40:14.000000000 +0300
+++ 375.20/kernel/nvidia/nvlink_proto.h	2016-11-16 02:44:18.000000000 +0300
@@ -1,5 +1,5 @@
 /*******************************************************************************
-    Copyright (c) 2015 NVidia Corporation
+    Copyright (c) 2015-2016 NVidia Corporation
 
     Permission is hereby granted, free of charge, to any person obtaining a copy
     of this software and associated documentation files (the "Software"), to
diff -ur 367.57/kernel/nvidia/nv-mmap.c 375.20/kernel/nvidia/nv-mmap.c
--- 367.57/kernel/nvidia/nv-mmap.c	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/nvidia/nv-mmap.c	2016-11-16 02:53:45.000000000 +0300
@@ -283,22 +283,34 @@
         NvU32 remap_prot_extra = 0;
         NvU64 access_start = mmap_start;
         NvU64 access_len = mmap_len;
+        NvBool page_isolation_required;
 
-        if (rmStatus == NV_OK && mmap_context != NULL)
+        rmStatus = rm_gpu_need_4k_page_isolation(nv, &page_isolation_required);
+        if (rmStatus != NV_OK)
+        {
+            os_free_mem(mmap_context);
+            return -EINVAL;
+        }
+
+        if (mmap_context != NULL)
         {
             /*
              * Do verification and cache encoding based on the original
              * (ostensibly smaller) mmap request, since accesses should be
              * restricted to that range.
              */
+            if (page_isolation_required)
+            {
 #if defined(NV_4K_PAGE_ISOLATION_PRESENT)
-            nv_mmap_isolation_t *nvmi = mmap_context;
-            remap_prot_extra = NV_PROT_4K_PAGE_ISOLATION;
-            access_start = (NvU64)nvmi->access_start;
-            access_len = nvmi->access_len;
-            mmap_start = (NvU64)nvmi->mmap_start;
-            mmap_len = nvmi->mmap_len;
+                nv_mmap_isolation_t *nvmi = mmap_context;
+                remap_prot_extra = NV_PROT_4K_PAGE_ISOLATION;
+                access_start = (NvU64)nvmi->access_start;
+                access_len = nvmi->access_len;
+                mmap_start = (NvU64)nvmi->mmap_start;
+                mmap_len = nvmi->mmap_len;
 #endif
+            }
+
             os_free_mem(mmap_context);
         }
 
@@ -410,8 +422,21 @@
         for (j = page_index; j < (page_index + pages); j++)
         {
 #if defined(NV_VM_INSERT_PAGE_PRESENT)
-            if (NV_VM_INSERT_PAGE(vma, start,
-                    NV_GET_PAGE_STRUCT(at->page_table[j]->phys_addr)))
+            int ret;
+#if defined(NV_VGPU_KVM_BUILD)
+            if (NV_ALLOC_MAPPING_GUEST(at->flags))
+            {
+                ret = nv_remap_page_range(vma, start, at->page_table[j]->phys_addr,
+                                        PAGE_SIZE, vma->vm_page_prot);
+
+            }
+            else
+#endif
+            {
+                ret = NV_VM_INSERT_PAGE(vma, start,
+                                      NV_GET_PAGE_STRUCT(at->page_table[j]->phys_addr));
+            }
+            if (ret)
 #else
             if (nv_remap_page_range(vma, start, at->page_table[j]->phys_addr,
                     PAGE_SIZE, vma->vm_page_prot) != 0)
diff -ur 367.57/kernel/nvidia/nv-pat.c 375.20/kernel/nvidia/nv-pat.c
--- 367.57/kernel/nvidia/nv-pat.c	2016-11-13 19:44:57.567621064 +0300
+++ 375.20/kernel/nvidia/nv-pat.c	2016-11-16 02:53:45.000000000 +0300
@@ -94,11 +94,8 @@
             "NVRM: PAT configuration unsupported.\n");
         return NV_PAT_MODE_DISABLED;
     }
-    else {
-        nv_printf(NV_DBG_SETUP,
-            "NVRM: PAT configuration enabled.\n");
+    else
         return NV_PAT_MODE_BUILTIN;
-    }
 }
 
 static void nv_setup_pat_entries(void *info)
diff -ur 367.57/kernel/nvidia/nv-reg.h 375.20/kernel/nvidia/nv-reg.h
--- 367.57/kernel/nvidia/nv-reg.h	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/nvidia/nv-reg.h	2016-11-16 02:53:47.000000000 +0300
@@ -480,7 +480,7 @@
 NV_DEFINE_REG_ENTRY(__NV_ENABLE_PCIE_GEN3, 0);
 NV_DEFINE_REG_ENTRY(__NV_ENABLE_MSI, 1);
 NV_DEFINE_REG_ENTRY(__NV_TCE_BYPASS_MODE, NV_TCE_BYPASS_MODE_DEFAULT);
-NV_DEFINE_REG_ENTRY(__NV_USE_THREADED_INTERRUPTS, 0);
+NV_DEFINE_REG_ENTRY(__NV_USE_THREADED_INTERRUPTS, 1);
 NV_DEFINE_REG_ENTRY_GLOBAL(__NV_MEMORY_POOL_SIZE, 0);
 
 NV_DEFINE_REG_STRING_ENTRY(__NV_REGISTRY_DWORDS, NULL);
diff -ur 367.57/kernel/nvidia/nv-usermap.c 375.20/kernel/nvidia/nv-usermap.c
--- 367.57/kernel/nvidia/nv-usermap.c	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/nvidia/nv-usermap.c	2016-11-16 02:53:45.000000000 +0300
@@ -46,6 +46,10 @@
 {
     NV_STATUS status = NV_OK;
 
+#if defined(NV_4K_PAGE_ISOLATION_PRESENT)
+    NvBool    bGpuNeed4kPageIsolation;
+#endif
+
     *ppPrivate = NULL;
 
     if (NV_IS_CTL_DEVICE(nv))
@@ -73,30 +77,40 @@
     else
     {
 #if defined(NV_4K_PAGE_ISOLATION_PRESENT)
+    status = rm_gpu_need_4k_page_isolation(nv, &bGpuNeed4kPageIsolation);
+    if (status != NV_OK)
+        return status;
+
+    if (bGpuNeed4kPageIsolation)
         /* We only need to use page isolation for the BAR mappings. */
-        if (NV_4K_PAGE_ISOLATION_REQUIRED(address, size))
+        if (IS_REG_OFFSET(nv, address, size) ||
+            IS_FB_OFFSET(nv, address, size) ||
+            IS_IMEM_OFFSET(nv, address, size))
         {
-            nv_mmap_isolation_t *mmap_isolation = NULL;
-            status = os_alloc_mem(ppPrivate, sizeof(nv_mmap_isolation_t));
-            if (status != NV_OK)
+            if (NV_4K_PAGE_ISOLATION_REQUIRED(address, size))
             {
-                return status;
-            }
+                nv_mmap_isolation_t *mmap_isolation = NULL;
+                status = os_alloc_mem(ppPrivate, sizeof(nv_mmap_isolation_t));
+                if (status != NV_OK)
+                {
+                    return status;
+                }
 
-            /*
-             * Given the platform-specific isolation mechanism, the user will
-             * only be able to access the isolated range, even though the
-             * user will pass a wider range to mmap().
-             */
-            mmap_isolation = (nv_mmap_isolation_t *)*ppPrivate;
-            mmap_isolation->access_start =
-                NV_4K_PAGE_ISOLATION_ACCESS_START(address);
-            mmap_isolation->access_len =
-                NV_4K_PAGE_ISOLATION_ACCESS_LEN(address, size);
-            mmap_isolation->mmap_start =
-                NV_4K_PAGE_ISOLATION_MMAP_ADDR(address);
-            mmap_isolation->mmap_len =
-                NV_4K_PAGE_ISOLATION_MMAP_LEN(size);
+                /*
+                 * Given the platform-specific isolation mechanism, the user will
+                 * only be able to access the isolated range, even though the
+                 * user will pass a wider range to mmap().
+                 */
+                mmap_isolation = (nv_mmap_isolation_t *)*ppPrivate;
+                mmap_isolation->access_start =
+                    NV_4K_PAGE_ISOLATION_ACCESS_START(address);
+                mmap_isolation->access_len =
+                    NV_4K_PAGE_ISOLATION_ACCESS_LEN(address, size);
+                mmap_isolation->mmap_start =
+                    NV_4K_PAGE_ISOLATION_MMAP_ADDR(address);
+                mmap_isolation->mmap_len =
+                    NV_4K_PAGE_ISOLATION_MMAP_LEN(size);
+            }
         }
 #endif
     }
diff -ur 367.57/kernel/nvidia/nv_uvm_interface.c 375.20/kernel/nvidia/nv_uvm_interface.c
--- 367.57/kernel/nvidia/nv_uvm_interface.c	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/nvidia/nv_uvm_interface.c	2016-11-16 02:53:45.000000000 +0300
@@ -77,7 +77,9 @@
 // Testing code to force use of the global stack every now and then
 static NvBool forceGlobalStack(void)
 {
-    if (DEBUG_GLOBAL_STACK)
+    // Make sure that we do not try to allocate memory in interrupt or atomic
+    // context
+    if (DEBUG_GLOBAL_STACK || !NV_MAY_SLEEP())
     {
         if ((atomic_inc_return(&g_debugGlobalStackCount) %
              DEBUG_GLOBAL_STACK_THRESHOLD) == 0)
@@ -955,6 +957,25 @@
                                        (gpuAddressSpaceHandle)vaSpace,
                                        pFaultInfo);
 
+    // Preallocate a stack for functions called from ISR top half
+    pFaultInfo->nonReplayable.isr_sp = NULL;
+    if (status == NV_OK)
+    {
+        nvidia_stack_t *isr_sp;
+        if (nv_kmem_cache_alloc_stack(&isr_sp) == -ENOMEM)
+        {
+            rm_gpu_ops_destroy_fault_info(sp,
+                                          (gpuAddressSpaceHandle)vaSpace,
+                                          pFaultInfo);
+
+            status = NV_ERR_NO_MEMORY;
+        }
+        else
+        {
+            pFaultInfo->nonReplayable.isr_sp = isr_sp;
+        }
+    }
+
     nv_kmem_cache_free_stack(sp);
     return status;
 }
@@ -1030,6 +1051,10 @@
     nvidia_stack_t *sp = nvUvmGetSafeStack();
     NV_STATUS status;
 
+    // Free the preallocated stack for functions called from ISR
+    if (pFaultInfo->nonReplayable.isr_sp != NULL)
+        nv_kmem_cache_free_stack((nvidia_stack_t *)pFaultInfo->nonReplayable.isr_sp);
+
     status = rm_gpu_ops_destroy_fault_info(sp,
                                           (gpuAddressSpaceHandle)vaSpace,
                                           pFaultInfo);
@@ -1039,6 +1064,41 @@
 }
 EXPORT_SYMBOL(nvUvmInterfaceDestroyFaultInfo);
 
+NV_STATUS nvUvmInterfaceHasPendingNonReplayableFaults(UvmGpuFaultInfo *pFaultInfo,
+                                                      NvBool *hasPendingFaults)
+{
+    NV_STATUS status;
+
+    if (pFaultInfo->nonReplayable.isr_sp == NULL)
+        return NV_ERR_INVALID_ARGUMENT;
+
+    status = rm_gpu_ops_has_pending_non_replayable_faults(pFaultInfo->nonReplayable.isr_sp,
+                                                          pFaultInfo,
+                                                          hasPendingFaults);
+
+    return status;
+}
+EXPORT_SYMBOL(nvUvmInterfaceHasPendingNonReplayableFaults);
+
+NV_STATUS nvUvmInterfaceGetNonReplayableFaults(UvmGpuFaultInfo *pFaultInfo,
+                                               void *pFaultBuffer,
+                                               NvU32 *numFaults)
+{
+    nvidia_stack_t *sp = nvUvmGetSafeStack();
+    NV_STATUS status;
+
+    status = rm_gpu_ops_get_non_replayable_faults(sp,
+                                                  pFaultInfo,
+                                                  pFaultBuffer,
+                                                  numFaults);
+
+    nvUvmFreeSafeStack(sp);
+
+    return status;
+
+}
+EXPORT_SYMBOL(nvUvmInterfaceGetNonReplayableFaults);
+
 NV_STATUS nvUvmInterfaceDestroyAccessCntrInfo(uvmGpuAddressSpaceHandle vaSpace,
     UvmGpuAccessCntrInfo *pAccessCntrInfo)
 {
diff -ur 367.57/kernel/nvidia/os-interface.c 375.20/kernel/nvidia/os-interface.c
--- 367.57/kernel/nvidia/os-interface.c	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/nvidia/os-interface.c	2016-11-16 02:53:45.000000000 +0300
@@ -1,6 +1,6 @@
 /* _NVRM_COPYRIGHT_BEGIN_
  *
- * Copyright 1999-2013 by NVIDIA Corporation.  All rights reserved.  All
+ * Copyright 1999-2016 by NVIDIA Corporation.  All rights reserved.  All
  * information contained herein is proprietary and confidential to NVIDIA
  * Corporation.  Any use, reproduction, or disclosure without the written
  * permission of NVIDIA Corporation is prohibited.
@@ -121,19 +121,12 @@
     up(os_mutex);
 }
 
-typedef struct os_semaphore_s
-{
-    struct completion  completion;
-    nv_spinlock_t      lock;
-    NvS32              count;
-    NvS32              limit;
-} os_semaphore_t;
+typedef struct semaphore os_semaphore_t;
 
 
 void* NV_API_CALL os_alloc_semaphore
 (
-    NvU32 initialValue,
-    NvU32 limit
+    NvU32 initialValue
 )
 {
     NV_STATUS rmStatus;
@@ -146,10 +139,7 @@
         return NULL;
     }
 
-    init_completion(&os_sema->completion);
-    NV_SPIN_LOCK_INIT(&os_sema->lock);
-    os_sema->count = initialValue;
-    os_sema->limit = limit;
+    NV_INIT_SEMA(os_sema, initialValue);
 
     return (void *)os_sema;
 }
@@ -170,21 +160,12 @@
 )
 {
     os_semaphore_t *os_sema = (os_semaphore_t *)pSema;
-    unsigned long old_irq;
 
-    NV_SPIN_LOCK_IRQSAVE(&os_sema->lock, old_irq);
-    if (os_sema->count <= 0)
-    {
-        os_sema->count--;
-        NV_SPIN_UNLOCK_IRQRESTORE(&os_sema->lock, old_irq);
-        wait_for_completion(&os_sema->completion);
-    }
-    else
+    if (!NV_MAY_SLEEP())
     {
-        os_sema->count--;
-        NV_SPIN_UNLOCK_IRQRESTORE(&os_sema->lock, old_irq);
+        return NV_ERR_INVALID_REQUEST;
     }
-
+    down(os_sema);
     return NV_OK;
 }
 
@@ -194,20 +175,7 @@
 )
 {
     os_semaphore_t *os_sema = (os_semaphore_t *)pSema;
-    unsigned long old_irq;
-    BOOL doWakeup = FALSE;
-
-    NV_SPIN_LOCK_IRQSAVE(&os_sema->lock, old_irq);
-    if (os_sema->count < 0)
-    {
-        doWakeup = TRUE;
-    }
-    os_sema->count++;
-    NV_SPIN_UNLOCK_IRQRESTORE(&os_sema->lock, old_irq);
-
-    if (doWakeup)
-        complete(&os_sema->completion);
-
+    up(os_sema);
     return NV_OK;
 }
 
@@ -264,6 +232,16 @@
     return strlen(str);
 }
 
+NvU32 NV_API_CALL os_strtoul(const char *str, char **endp, NvU32 base)
+{
+    return (NvU32)simple_strtoul(str, endp, base);
+}
+
+NvS32 NV_API_CALL os_string_compare(const char *str1, const char *str2)
+{
+    return strcmp(str1, str2);
+}
+
 NvU8* NV_API_CALL os_mem_copy(
     NvU8       *dst,
     const NvU8 *src,
@@ -1146,13 +1124,13 @@
 
 void NV_API_CALL os_get_screen_info(
     NvU64 *pPhysicalAddress,
-    NvU64 *pFbSize
+    NvU16 *pFbWidth,
+    NvU16 *pFbHeight,
+    NvU16 *pFbDepth,
+    NvU16 *pFbPitch
 )
 {
 #if (defined(NVCPU_X86) || defined(NVCPU_X86_64))
-    NvU64 height = screen_info.lfb_height;
-    NvU64 pitch = screen_info.lfb_linelength;
-
     //
     // If there is not a framebuffer console, return 0 size.
     //
@@ -1163,17 +1141,21 @@
     if (screen_info.orig_video_isVGA <= 1)
     {
         *pPhysicalAddress = 0;
-        *pFbSize = 0;
+        *pFbWidth = *pFbHeight = *pFbDepth = *pFbPitch = 0;
+        return;
     }
 
     *pPhysicalAddress = screen_info.lfb_base;
 #if defined(VIDEO_CAPABILITY_64BIT_BASE)
     *pPhysicalAddress |= (NvU64)screen_info.ext_lfb_base << 32;
 #endif
-    *pFbSize = height * pitch;
+    *pFbWidth = screen_info.lfb_width;
+    *pFbHeight = screen_info.lfb_height;
+    *pFbDepth = screen_info.lfb_depth;
+    *pFbPitch = screen_info.lfb_linelength;
 #else
     *pPhysicalAddress = 0;
-    *pFbSize = 0;
+    *pFbWidth = *pFbHeight = *pFbDepth = *pFbPitch = 0;
 #endif
 }
 
@@ -1583,3 +1565,11 @@
 void NV_API_CALL os_delete_record_for_crashLog(void *pbuffer)
 {
 }
+
+#if !defined(NV_VGPU_KVM_BUILD)
+NV_STATUS NV_API_CALL os_call_vgpu_vfio(void *pvgpu_vfio_info, NvU32 cmd_type)
+{
+    return NV_ERR_NOT_SUPPORTED;
+}
+#endif
+
diff -ur 367.57/kernel/nvidia/os-mlock.c 375.20/kernel/nvidia/os-mlock.c
--- 367.57/kernel/nvidia/os-mlock.c	2016-11-11 18:27:50.088777250 +0300
+++ 375.20/kernel/nvidia/os-mlock.c	2016-11-16 02:53:45.000000000 +0300
@@ -96,10 +96,9 @@
     struct mm_struct *mm = current->mm;
     struct page **user_pages;
     NvU64 i, pinned;
-    NvBool write = 1;
+    NvBool write = 1, force = 0;
     int ret;
 
-
     if (!NV_MAY_SLEEP())
     {
         nv_printf(NV_DBG_ERRORS,
@@ -118,7 +117,7 @@
 
     down_read(&mm->mmap_sem);
     ret = NV_GET_USER_PAGES((unsigned long)address,
-                            page_count, write ? FOLL_WRITE : 0, user_pages, NULL);
+                            page_count, write, force, user_pages, NULL);
     up_read(&mm->mmap_sem);
     pinned = ret;
 
diff -ur 367.57/kernel/nvidia/os-pci.c 375.20/kernel/nvidia/os-pci.c
--- 367.57/kernel/nvidia/os-pci.c	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/nvidia/os-pci.c	2016-11-16 02:53:45.000000000 +0300
@@ -134,4 +134,37 @@
     return NV_OK;
 }
 
+NvBool NV_API_CALL os_remove_gpu_supported(void)
+{
+#if defined NV_PCI_STOP_AND_REMOVE_BUS_DEVICE
+    return NV_TRUE;
+#else
+    return NV_FALSE;
+#endif
+}
+
+void NV_API_CALL os_remove_gpu
+(
+    NvU32   domain,
+    NvU8    bus,
+    NvU8    slot
+)
+{
+#if defined(NV_PCI_STOP_AND_REMOVE_BUS_DEVICE)
+    nv_state_t          *nv;
+    nv_linux_state_t    *nvl;
 
+    nv = nv_get_adapter_state(domain, bus, slot);
+
+    if (nv != NULL)
+    {
+        nvl = NV_GET_NVL_FROM_NV_STATE(nv);
+        NV_PCI_STOP_AND_REMOVE_BUS_DEVICE(nvl->dev);
+    }
+#elif defined(DEBUG)
+    nv_printf(NV_DBG_ERRORS,
+            "NVRM: %s() is called even though NV_PCI_STOP_AND_REMOVE_BUS_DEVICE is not defined\n",
+            __FUNCTION__);
+    os_dbg_breakpoint();
+#endif
+}
diff -ur 367.57/kernel/nvidia/os-registry.c 375.20/kernel/nvidia/os-registry.c
--- 367.57/kernel/nvidia/os-registry.c	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/nvidia/os-registry.c	2016-11-16 02:53:45.000000000 +0300
@@ -1,6 +1,6 @@
 /* _NVRM_COPYRIGHT_BEGIN_
  *
- * Copyright 2000-2001 by NVIDIA Corporation.  All rights reserved.  All
+ * Copyright 2000-2016 by NVIDIA Corporation.  All rights reserved.  All
  * information contained herein is proprietary and confidential to NVIDIA
  * Corporation.  Any use, reproduction, or disclosure without the written
  * permission of NVIDIA Corporation is prohibited.
@@ -14,82 +14,6 @@
 #include "os-interface.h"
 #include "nv-linux.h"
 #include "nv-reg.h"
-
-static char *remove_spaces(const char *in)
-{
-    unsigned int len = strlen(in) + 1;
-    const char *in_ptr;
-    char *out, *out_ptr;
-
-    if (os_alloc_mem((void **)&out, len) != NV_OK)
-        return NULL;
-
-    in_ptr = in;
-    out_ptr = out;
-
-    while (*in_ptr != '\0')
-    {
-        if (!isspace(*in_ptr)) *out_ptr++ = *in_ptr;
-        in_ptr++;
-    }
-    *out_ptr = '\0';
-
-    return out;
-}
-
-static void parse_option_string(nvidia_stack_t *sp)
-{
-    unsigned int i;
-    nv_parm_t *entry;
-    char *option_string = NULL;
-    char *ptr, *token;
-    char *name, *value;
-    NvU32 data;
-
-    if (NVreg_RegistryDwords != NULL)
-    {
-        if ((option_string = remove_spaces(NVreg_RegistryDwords)) == NULL)
-        {
-            return;
-        }
-
-        ptr = option_string;
-
-        while ((token = strsep(&ptr, ";")) != NULL)
-        {
-            if (!(name = strsep(&token, "=")) || !strlen(name))
-            {
-                continue;
-            }
-
-            if (!(value = strsep(&token, "=")) || !strlen(value))
-            {
-                continue;
-            }
-
-            if (strsep(&token, "=") != NULL)
-            {
-                continue;
-            }
-
-            data = (NvU32)simple_strtoul(value, NULL, 0);
-
-            for (i = 0; (entry = &nv_parms[i])->name != NULL; i++)
-            {
-                if (strcmp(entry->name, name) == 0)
-                    break;
-            }
-
-            if (!entry->name)
-                rm_write_registry_dword(sp, NULL, "NVreg", name, data);
-            else
-                *entry->data = data;
-        }
-
-        os_free_mem(option_string);
-    }
-}
-
 static NvBool parse_assign_gpus_string(void)
 {
     char *option_string = NULL;
@@ -100,7 +24,7 @@
         return NV_FALSE;
     }
 
-    if ((option_string = remove_spaces(NVreg_AssignGpus)) == NULL)
+    if ((option_string = rm_remove_spaces(NVreg_AssignGpus)) == NULL)
     {
         return NV_FALSE;
     }
@@ -252,7 +176,7 @@
                                  NVreg_AssignGpus, strlen(NVreg_AssignGpus));
     }
 
-    parse_option_string(sp);
+    rm_parse_option_string(sp, NVreg_RegistryDwords);
 
     detect_virtualization_and_apply_defaults(sp);
 
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-connector.c 375.20/kernel/nvidia-drm/nvidia-drm-connector.c
--- 367.57/kernel/nvidia-drm/nvidia-drm-connector.c	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-connector.c	2016-11-16 02:44:48.000000000 +0300
@@ -299,8 +299,16 @@
     return count;
 }
 
+static int nvidia_drm_atomic_helper_connector_dpms(
+    struct drm_connector *connector,
+    int mode)
+{
+    /* TODO */
+    return -EPERM;
+}
+
 static struct drm_connector_funcs nv_connector_funcs = {
-    .dpms                   = drm_atomic_helper_connector_dpms,
+    .dpms                   = nvidia_drm_atomic_helper_connector_dpms,
     .destroy                = nvidia_connector_destroy,
     .reset                  = drm_atomic_helper_connector_reset,
     .detect                 = nvidia_connector_detect,
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-crtc.h 375.20/kernel/nvidia-drm/nvidia-drm-crtc.h
--- 367.57/kernel/nvidia-drm/nvidia-drm-crtc.h	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-crtc.h	2016-11-16 02:44:48.000000000 +0300
@@ -35,10 +35,10 @@
 {
     NvU32 head;
 
+#if !defined(NV_DRM_ATOMIC_MODESET_NONBLOCKING_COMMIT_AVAILABLE)
     atomic_t has_pending_commit;
-
-    struct NvKmsKapiHeadModeSetConfig modeset_config;
-    struct NvKmsKapiPlaneConfig plane_config[NVKMS_KAPI_PLANE_MAX];
+    atomic_t has_pending_flip_event;
+#endif
 
     struct drm_crtc base;
 };
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-drv.c 375.20/kernel/nvidia-drm/nvidia-drm-drv.c
--- 367.57/kernel/nvidia-drm/nvidia-drm-drv.c	2016-11-13 19:24:22.249924918 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-drv.c	2016-11-16 02:44:48.000000000 +0300
@@ -30,6 +30,7 @@
 #include "nvidia-drm-gem.h"
 #include "nvidia-drm-crtc.h"
 #include "nvidia-drm-mmap.h"
+#include "nvidia-drm-fence.h"
 
 #if defined(NV_DRM_AVAILABLE)
 
@@ -70,6 +71,11 @@
 static const struct drm_mode_config_funcs nv_mode_config_funcs = {
     .fb_create = nvidia_drm_framebuffer_create,
 
+#if defined(NV_DRM_MODE_CONFIG_FUNCS_HAS_ATOMIC_STATE_ALLOC)
+    .atomic_state_alloc = nvidia_drm_atomic_state_alloc,
+    .atomic_state_clear = nvidia_drm_atomic_state_clear,
+    .atomic_state_free  = nvidia_drm_atomic_state_free,
+#endif
     .atomic_check  = nvidia_drm_atomic_check,
     .atomic_commit = nvidia_drm_atomic_commit,
 
@@ -123,6 +129,11 @@
     const struct NvKmsKapiDeviceResourcesInfo *pResInfo
 )
 {
+#if defined(NV_DRM_ATOMIC_MODESET_NONBLOCKING_COMMIT_AVAILABLE)
+    static struct drm_mode_config_helper_funcs nv_mode_config_helper = {
+        .atomic_commit_tail = nvidia_drm_atomic_helper_commit_tail,
+    };
+#endif
     struct drm_device *dev = nv_dev->dev;
 
     drm_mode_config_init(dev);
@@ -153,6 +164,10 @@
 
     dev->mode_config.async_page_flip = true;
 
+#if defined(NV_DRM_ATOMIC_MODESET_NONBLOCKING_COMMIT_AVAILABLE)
+    dev->mode_config.helper_private = &nv_mode_config_helper;
+#endif
+
     /* Initialize output polling support */
 
     drm_kms_helper_poll_init(dev);
@@ -344,7 +359,10 @@
 
     atomic_set(&nv_dev->enable_event_handling, true);
 
+#if !defined(NV_DRM_ATOMIC_MODESET_NONBLOCKING_COMMIT_AVAILABLE)
     init_waitqueue_head(&nv_dev->pending_commit_queue);
+    init_waitqueue_head(&nv_dev->pending_flip_queue);
+#endif
 
     mutex_unlock(&nv_dev->lock);
 
@@ -421,9 +439,14 @@
     return 0;
 }
 
+#if defined(NV_DRM_MASTER_DROP_HAS_FROM_RELEASE_ARG)
 static
 void nvidia_drm_master_drop(struct drm_device *dev,
                             struct drm_file *file_priv, bool from_release)
+#else
+static
+void nvidia_drm_master_drop(struct drm_device *dev, struct drm_file *file_priv)
+#endif
 {
     struct nvidia_drm_device *nv_dev = dev->dev_private;
     int ret;
@@ -444,48 +467,8 @@
                                          void *data,
                                          struct drm_file *file_priv)
 {
-    struct nvidia_drm_device *nv_dev = dev->dev_private;
-    struct drm_nvidia_migrate_modeset_ownership_params *params = data;
-    int ret = -EINVAL;
-
-    if (!nvidia_drm_modeset_enabled(dev))
-    {
-        return -EINVAL;
-    }
-
-    mutex_lock(&dev->master_mutex);
-
-    if (!file_priv->is_master ||
-        !file_priv->master)
-    {
-        goto done;
-    }
-
-    if (!nvKms->migrateOwnership(nv_dev->pDevice,
-                                 params->nvKmsFd,
-                                 params->nvKmsDeviceHandle))
-    {
-       ret = -EPERM;
-       goto done;
-    }
-
-    /*
-     * NVKMS modeset ownership has now been migrated to the NVKMS client
-     * indicated by (nvKmsFd, nvKmsDeviceHandle).  Update DRM to reflect that
-     * it no longer has master privileges.  Subsequent attempts to acquire
-     * DRM master will fail as long as another NVKMS client has
-     * NVKMS modeset ownership, because nvidia_drm_master_set()'s call to
-     * grabOwnership() will fail.
-     */
-    drm_master_put(&file_priv->master);
-    file_priv->is_master = 0;
-
-    ret = 0;
-
-done:
-    mutex_unlock(&dev->master_mutex);
-
-    return ret;
+    /* TODO: This ioctl not required at this moment, revisit its necessity. */
+    return -EPERM;
 }
 #endif /* NV_DRM_ATOMIC_MODESET_AVAILABLE */
 
@@ -594,6 +577,21 @@
     DRM_IOCTL_DEF_DRV(NVIDIA_GET_DEV_INFO,
                       nvidia_drm_get_dev_info,
                       DRM_CONTROL_ALLOW|DRM_RENDER_ALLOW|DRM_UNLOCKED),
+    DRM_IOCTL_DEF_DRV(NVIDIA_GEM_PRIME_FENCE_SUPPORTED,
+                      nvidia_drm_gem_prime_fence_supported,
+                      DRM_CONTROL_ALLOW|DRM_RENDER_ALLOW|DRM_UNLOCKED),
+    DRM_IOCTL_DEF_DRV(NVIDIA_GEM_PRIME_FENCE_INIT,
+                      nvidia_drm_gem_prime_fence_init,
+                      DRM_CONTROL_ALLOW|DRM_RENDER_ALLOW|DRM_UNLOCKED),
+    DRM_IOCTL_DEF_DRV(NVIDIA_GEM_PRIME_FENCE_ATTACH,
+                      nvidia_drm_gem_prime_fence_attach,
+                      DRM_CONTROL_ALLOW|DRM_RENDER_ALLOW|DRM_UNLOCKED),
+    DRM_IOCTL_DEF_DRV(NVIDIA_GEM_PRIME_FENCE_FORCE_SIGNAL,
+                      nvidia_drm_gem_prime_fence_force_signal,
+                      DRM_CONTROL_ALLOW|DRM_RENDER_ALLOW|DRM_UNLOCKED),
+    DRM_IOCTL_DEF_DRV(NVIDIA_GEM_PRIME_FENCE_FINI,
+                      nvidia_drm_gem_prime_fence_fini,
+                      DRM_CONTROL_ALLOW|DRM_RENDER_ALLOW|DRM_UNLOCKED),
 };
 
 static struct drm_driver nv_drm_driver = {
@@ -611,6 +609,10 @@
     .gem_prime_vmap         = nvidia_drm_gem_prime_vmap,
     .gem_prime_vunmap       = nvidia_drm_gem_prime_vunmap,
 
+#if defined(NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ)
+    .gem_prime_res_obj      = nvidia_drm_gem_prime_res_obj,
+#endif
+
 #if defined(NV_DRM_DRIVER_HAS_SET_BUSID)
     .set_busid              = nvidia_drm_pci_set_busid,
 #endif
@@ -648,8 +650,7 @@
 {
 #if defined(NV_DRM_ATOMIC_MODESET_AVAILABLE)
 
-    if (!nvidia_drm_modeset_module_param)
-    {
+    if (!nvidia_drm_modeset_module_param) {
         return;
     }
 
@@ -662,6 +663,7 @@
     nv_drm_driver.dumb_map_offset  = nvidia_drm_dumb_map_offset;
     nv_drm_driver.dumb_destroy     = drm_gem_dumb_destroy;
 
+    nv_drm_driver.gem_vm_ops       = &nv_drm_gem_vma_ops;
 #endif /* NV_DRM_ATOMIC_MODESET_AVAILABLE */
 }
 
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-fb.c 375.20/kernel/nvidia-drm/nvidia-drm-fb.c
--- 367.57/kernel/nvidia-drm/nvidia-drm-fb.c	2016-11-14 23:20:15.267023090 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-fb.c	2016-11-16 02:44:48.000000000 +0300
@@ -114,7 +114,7 @@
      * We don't support any planar format, pick up first buffer only.
      */
 
-    gem = nvidia_drm_gem_object_lookup(file, cmd->handles[0]);
+    gem = nvidia_drm_gem_object_lookup(dev, file, cmd->handles[0]);
 
     if (gem == NULL)
     {
Только в 375.20/kernel/nvidia-drm: nvidia-drm-fence.c
Только в 375.20/kernel/nvidia-drm: nvidia-drm-fence.h
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-gem.c 375.20/kernel/nvidia-drm/nvidia-drm-gem.c
--- 367.57/kernel/nvidia-drm/nvidia-drm-gem.c	2016-11-14 23:23:13.772320017 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-gem.c	2016-11-16 02:44:48.000000000 +0300
@@ -26,6 +26,7 @@
 
 #include "nvidia-drm-priv.h"
 #include "nvidia-drm-ioctl.h"
+#include "nvidia-drm-fence.h"
 #include "nvidia-drm-gem.h"
 
 static struct nvidia_drm_gem_object *nvidia_drm_gem_new
@@ -78,6 +79,10 @@
 
     nv_gem->handle = *handle;
 
+#if defined(NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ)
+    reservation_object_init(&nv_gem->fenceContext.resv);
+#endif
+
     NV_DRM_DEV_LOG_DEBUG(nv_dev, "Created buffer with GEM handle 0x%x", *handle);
 
     return nv_gem;
@@ -87,6 +92,11 @@
 {
     struct drm_device *dev = gem->dev;
 
+#if defined(NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ) || \
+    defined(NV_DRM_ATOMIC_MODESET_AVAILABLE)
+    struct nvidia_drm_device *nv_dev = dev->dev_private;
+#endif
+
     struct nvidia_drm_gem_object *nv_gem =
                     DRM_GEM_OBJECT_TO_NV_GEM_OBJECT(gem);
 
@@ -96,16 +106,22 @@
 
     drm_gem_object_release(&nv_gem->base);
 
+#if defined(NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ)
+    /* Make sure fencing gets torn down if client died before it could do it */
+    nvidia_drm_gem_prime_fence_teardown(nv_dev, nv_gem);
+
+    reservation_object_fini(&nv_gem->fenceContext.resv);
+#endif
+
     switch (nv_gem->type)
     {
 #if defined(NV_DRM_ATOMIC_MODESET_AVAILABLE)
         case NV_DRM_GEM_OBJECT_TYPE_NVKMS_MEMORY:
         {
-            struct nvidia_drm_device *nv_dev = dev->dev_private;
-
             if (nv_gem->u.nvkms_memory.mapped) {
                 nvKms->unmapMemory(nv_dev->pDevice,
                                    nv_gem->u.nvkms_memory.pMemory,
+                                   NVKMS_KAPI_MAPPING_TYPE_USER,
                                    nv_gem->u.nvkms_memory.pLinearAddress);
             }
 
@@ -260,6 +276,18 @@
     nvidia_drm_vunmap(address);
 }
 
+#if defined(NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ)
+struct reservation_object* nvidia_drm_gem_prime_res_obj
+(
+    struct drm_gem_object *obj
+)
+{
+    struct nvidia_drm_gem_object *nv_gem = DRM_GEM_OBJECT_TO_NV_GEM_OBJECT(obj);
+
+    return &nv_gem->fenceContext.resv;
+}
+#endif
+
 #if defined(NV_DRM_ATOMIC_MODESET_AVAILABLE)
 
 int nvidia_drm_dumb_create
@@ -296,6 +324,7 @@
 
     if (!nvKms->mapMemory(nv_dev->pDevice,
                           nv_gem_union.nvkms_memory.pMemory,
+                          NVKMS_KAPI_MAPPING_TYPE_USER,
                           &nv_gem_union.nvkms_memory.pLinearAddress))
     {
         ret = -ENOMEM;
@@ -334,6 +363,7 @@
     {
         nvKms->unmapMemory(nv_dev->pDevice,
                            nv_gem_union.nvkms_memory.pMemory,
+                           NVKMS_KAPI_MAPPING_TYPE_USER,
                            nv_gem_union.nvkms_memory.pLinearAddress);
         nv_gem_union.nvkms_memory.mapped = false;
     }
@@ -393,58 +423,60 @@
     return 0;
 }
 
-int nvidia_drm_dumb_map_offset
-(
-    struct drm_file *file,
-    struct drm_device *dev, uint32_t handle, uint64_t *offset
-)
+int nvidia_drm_dumb_map_offset(struct drm_file *file,
+                               struct drm_device *dev, uint32_t handle,
+                               uint64_t *offset)
 {
     struct nvidia_drm_device *nv_dev = dev->dev_private;
 
-    struct drm_gem_object *gem;
-    struct nvidia_drm_gem_object *nv_gem;
+    struct drm_gem_object *gem = NULL;
+    struct nvidia_drm_gem_object *nv_gem = NULL;
 
     int ret = -EINVAL;
 
-    mutex_lock(&dev->struct_mutex);
 
-    gem = nvidia_drm_gem_object_lookup(file, handle);
-
-    if (gem == NULL)
-    {
+    if ((gem = nvidia_drm_gem_object_lookup(dev, file, handle)) == NULL) {
         NV_DRM_DEV_LOG_ERR(
             nv_dev,
-            "Failed to lookup gem object for mapping: 0x%08x", handle);
-        goto unlock_struct_mutex;
+            "Failed to lookup gem object for mapping: 0x%08x",
+            handle);
+        goto done;
     }
 
     nv_gem = DRM_GEM_OBJECT_TO_NV_GEM_OBJECT(gem);
 
-    if (nv_gem->type != NV_DRM_GEM_OBJECT_TYPE_NVKMS_MEMORY)
-    {
+    if (nv_gem->type != NV_DRM_GEM_OBJECT_TYPE_NVKMS_MEMORY) {
         NV_DRM_DEV_LOG_ERR(
             nv_dev,
-            "Invalid gem object type for mapping: 0x%08x", handle);
-        goto unlock_gem_object;
+            "Invalid gem object type for mapping: 0x%08x",
+            handle);
+        goto done;
     }
 
-    if (!nv_gem->u.nvkms_memory.mapped)
-    {
+    if (!nv_gem->u.nvkms_memory.mapped) {
         NV_DRM_DEV_LOG_ERR(
             nv_dev,
-            "Invalid gem object for mapping: 0x%08x", handle);
-        goto unlock_gem_object;
+            "Invalid gem object for mapping: 0x%08x",
+            handle);
+        goto done;
     }
 
-    *offset = (uint64_t)(uintptr_t)nv_gem->u.nvkms_memory.pLinearAddress;
+    if ((ret = drm_gem_create_mmap_offset(gem)) < 0) {
+        NV_DRM_DEV_LOG_ERR(
+            nv_dev,
+            "drm_gem_create_mmap_offset failed with error code %d",
+            ret);
+        goto done;
+    }
 
-    ret = 0;
+    *offset = drm_vma_node_offset_addr(&gem->vma_node);
 
-unlock_gem_object:
-    drm_gem_object_unreference_unlocked(&nv_gem->base);
+    ret = 0;
 
-unlock_struct_mutex:
-    mutex_unlock(&dev->struct_mutex);
+done:
+    if (gem != NULL) {
+        drm_gem_object_unreference_unlocked(gem);
+    }
 
     return ret;
 }
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-gem.h 375.20/kernel/nvidia-drm/nvidia-drm-gem.h
--- 367.57/kernel/nvidia-drm/nvidia-drm-gem.h	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-gem.h	2016-11-16 02:44:48.000000000 +0300
@@ -27,6 +27,8 @@
 
 #if defined(NV_DRM_AVAILABLE)
 
+#include "nvidia-drm-priv.h"
+
 #include <drm/drmP.h>
 #include "nvkms-kapi.h"
 
@@ -38,11 +40,16 @@
     NV_DRM_GEM_OBJECT_TYPE_USERSPACE_MEMORY = 2,
 };
 
+#if defined(NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ)
+struct nvidia_drm_gem_prime_soft_event_args;
+#endif
+
 struct nvidia_drm_gem_object
 {
     uint32_t handle;
 
     struct drm_gem_object base;
+
     enum nvidia_drm_gem_object_type type;
 
     union nvidia_drm_gem_object_union
@@ -66,6 +73,34 @@
             unsigned long pages_count;
         } userspace_memory;
     } u;
+
+#if defined(NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ)
+    struct
+    {
+        bool init; /* Whether we are initialized for fencing */
+
+        struct reservation_object resv;
+
+        uint32_t context;
+
+        NvU64 fenceSemIndex; /* Index into semaphore surface */
+
+        /* Mapped semaphore surface */
+        struct NvKmsKapiMemory *pSemSurface;
+        NvU32 *pLinearAddress;
+        /*
+         * Whether pLinearAddress is valid (0 is technically a valid
+         * physical address, so we cannot rely on pLinearAddress==NULL
+         * checks.
+         */
+        bool mapped;
+
+        /* Software signaling structures */
+        struct NvKmsKapiChannelEvent *cb;
+        struct nvidia_drm_gem_prime_soft_fence_event_args *cbArgs;
+        struct fence *softFence; /* Fence for software signaling */
+    } fenceContext;
+#endif
 };
 
 #define DRM_GEM_OBJECT_TO_NV_GEM_OBJECT(__gem) \
@@ -110,6 +145,13 @@
 
 void nvidia_drm_gem_prime_vunmap(struct drm_gem_object *gem, void *address);
 
+#if defined(NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ)
+struct reservation_object* nvidia_drm_gem_prime_res_obj
+(
+    struct drm_gem_object *obj
+);
+#endif
+
 #if defined(NV_DRM_ATOMIC_MODESET_AVAILABLE)
 
 int nvidia_drm_dumb_create
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-ioctl.h 375.20/kernel/nvidia-drm/nvidia-drm-ioctl.h
--- 367.57/kernel/nvidia-drm/nvidia-drm-ioctl.h	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-ioctl.h	2016-11-16 02:44:48.000000000 +0300
@@ -30,6 +30,11 @@
 #define DRM_NVIDIA_GEM_IMPORT_USERSPACE_MEMORY      0x02
 #define DRM_NVIDIA_GET_DEV_INFO                     0x03
 #define DRM_NVIDIA_MIGRATE_MODESET_OWNERSHIP        0x04
+#define DRM_NVIDIA_GEM_PRIME_FENCE_SUPPORTED        0x05
+#define DRM_NVIDIA_GEM_PRIME_FENCE_INIT             0x06
+#define DRM_NVIDIA_GEM_PRIME_FENCE_ATTACH           0x07
+#define DRM_NVIDIA_GEM_PRIME_FENCE_FORCE_SIGNAL     0x08
+#define DRM_NVIDIA_GEM_PRIME_FENCE_FINI             0x09
 
 #define DRM_IOCTL_NVIDIA_GEM_IMPORT_NVKMS_MEMORY                           \
     DRM_IOWR((DRM_COMMAND_BASE + DRM_NVIDIA_GEM_IMPORT_NVKMS_MEMORY),      \
@@ -51,6 +56,35 @@
     DRM_IOW((DRM_COMMAND_BASE + DRM_NVIDIA_MIGRATE_MODESET_OWNERSHIP),     \
             struct drm_nvidia_migrate_modeset_ownership_params)
 
+/*
+ * XXX Solaris compiler has issues with DRM_IO. None of this is supported on
+ * Solaris anyway, so just skip it.
+ *
+ * 'warning: suggest parentheses around arithmetic in operand of |'
+ */
+#if defined(NV_LINUX)
+#define DRM_IOCTL_NVIDIA_GEM_PRIME_FENCE_SUPPORTED                         \
+    DRM_IO(DRM_COMMAND_BASE + DRM_NVIDIA_GEM_PRIME_FENCE_SUPPORTED)
+#else
+#define DRM_IOCTL_NVIDIA_GEM_PRIME_FENCE_SUPPORTED 0
+#endif
+
+#define DRM_IOCTL_NVIDIA_GEM_PRIME_FENCE_INIT                              \
+    DRM_IOW((DRM_COMMAND_BASE + DRM_NVIDIA_GEM_PRIME_FENCE_INIT),          \
+            struct drm_nvidia_gem_prime_fence_init_params)
+
+#define DRM_IOCTL_NVIDIA_GEM_PRIME_FENCE_ATTACH                            \
+    DRM_IOW((DRM_COMMAND_BASE + DRM_NVIDIA_GEM_PRIME_FENCE_ATTACH),        \
+            struct drm_nvidia_gem_prime_fence_attach_params)
+
+#define DRM_IOCTL_NVIDIA_GEM_PRIME_FENCE_FORCE_SIGNAL                      \
+    DRM_IOW((DRM_COMMAND_BASE + DRM_NVIDIA_GEM_PRIME_FENCE_FORCE_SIGNAL),  \
+            struct drm_nvidia_gem_prime_fence_force_signal_params)
+
+#define DRM_IOCTL_NVIDIA_GEM_PRIME_FENCE_FINI                              \
+    DRM_IOW((DRM_COMMAND_BASE + DRM_NVIDIA_GEM_PRIME_FENCE_FINI),          \
+            struct drm_nvidia_gem_prime_fence_fini_params)
+
 struct drm_nvidia_gem_import_nvkms_memory_params {
     uint64_t mem_size;           /* IN */
 
@@ -86,4 +120,32 @@
     uint32_t nvKmsDeviceHandle;  /* IN */
 };
 
+struct drm_nvidia_gem_prime_fence_init_params {
+    uint32_t handle;            /* IN GEM handle to initialize */
+
+    uint32_t index;             /* IN Index of semaphore to use for fencing */
+    uint64_t size;              /* IN Size of semaphore surface in bytes */
+
+    /* Params for importing userspace semaphore surface */
+    uint64_t import_mem_nvkms_params_ptr;  /* IN */
+    uint64_t import_mem_nvkms_params_size; /* IN */
+
+    /* Params for creating software signaling event */
+    uint64_t event_nvkms_params_ptr;  /* IN */
+    uint64_t event_nvkms_params_size; /* IN */
+};
+
+struct drm_nvidia_gem_prime_fence_attach_params {
+    uint32_t handle;        /* IN GEM handle to attach fence to */
+    uint32_t sem_thresh;    /* IN Semaphore value to reach before signal */
+};
+
+struct drm_nvidia_gem_prime_fence_force_signal_params {
+    uint32_t handle;        /* IN GEM handle to force signal */
+};
+
+struct drm_nvidia_gem_prime_fence_fini_params {
+    uint32_t handle;    /* IN GEM handle to finalize */
+};
+
 #endif /* _UAPI_NVIDIA_DRM_IOCTL_H_ */
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm.Kbuild 375.20/kernel/nvidia-drm/nvidia-drm.Kbuild
--- 367.57/kernel/nvidia-drm/nvidia-drm.Kbuild	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm.Kbuild	2016-11-16 02:44:48.000000000 +0300
@@ -17,6 +17,7 @@
 NVIDIA_DRM_SOURCES += nvidia-drm/nvidia-drm-fb.c
 NVIDIA_DRM_SOURCES += nvidia-drm/nvidia-drm-modeset.c
 NVIDIA_DRM_SOURCES += nvidia-drm/nvidia-drm-mmap.c
+NVIDIA_DRM_SOURCES += nvidia-drm/nvidia-drm-fence.c
 NVIDIA_DRM_SOURCES += nvidia-drm/nvidia-drm-linux.c
 NVIDIA_DRM_SOURCES += nvidia-drm/nv-pci-table.c
 
@@ -48,6 +49,7 @@
 
 NV_CONFTEST_GENERIC_COMPILE_TESTS += drm_available
 NV_CONFTEST_GENERIC_COMPILE_TESTS += drm_atomic_available
+NV_CONFTEST_GENERIC_COMPILE_TESTS += drm_atomic_modeset_nonblocking_commit_available
 
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += drm_dev_unref
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += remap_pfn_range
@@ -67,4 +69,6 @@
 NV_CONFTEST_TYPE_COMPILE_TESTS += drm_init_functions_have_name_arg
 NV_CONFTEST_TYPE_COMPILE_TESTS += drm_mode_connector_list_update_has_merge_type_bits_arg
 NV_CONFTEST_TYPE_COMPILE_TESTS += drm_helper_mode_fill_fb_struct_has_const_mode_cmd_arg
+NV_CONFTEST_TYPE_COMPILE_TESTS += drm_master_drop_has_from_release_arg
+NV_CONFTEST_TYPE_COMPILE_TESTS += drm_mode_config_funcs_has_atomic_state_alloc
 
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-linux.c 375.20/kernel/nvidia-drm/nvidia-drm-linux.c
--- 367.57/kernel/nvidia-drm/nvidia-drm-linux.c	2016-11-09 18:48:50.747824484 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-linux.c	2016-11-25 17:45:06.295253461 +0300
@@ -32,7 +32,6 @@
 #if defined(NV_DRM_AVAILABLE)
 
 #include "nv-mm.h"
-#include "nv-pgprot.h"
 
 MODULE_PARM_DESC(
     modeset,
@@ -67,30 +66,6 @@
     return p;
 }
 
-int nvidia_drm_encode_pgprot(enum nvidia_drm_memory_cache_type cache_type,
-                             pgprot_t *prot)
-{
-    switch (cache_type) {
-        case NV_DRM_MEMORY_CACHE_TYPE_UNCACHED_WEAK:
-#if defined(NV_PGPROT_UNCACHED_WEAK)
-            *prot = NV_PGPROT_UNCACHED_WEAK(*prot);
-            break;
-#endif
-        case NV_DRM_MEMORY_CACHE_TYPE_UNCACHED:
-            *prot = NV_PGPROT_UNCACHED_DEVICE(*prot);
-            break;
-        case NV_DRM_MEMORY_CACHE_TYPE_WRITECOMBINED:
-#if defined(NV_PGPROT_WRITE_COMBINED_DEVICE)
-            *prot = NV_PGPROT_WRITE_COMBINED_DEVICE(*prot);
-            break;
-#endif
-        default:
-            return -EINVAL;
-    }
-
-    return 0;
-}
-
 #if defined(NVCPU_X86) || defined(NVCPU_X86_64)
   #define WRITE_COMBINE_FLUSH()    asm volatile("sfence":::"memory")
 #elif defined(NVCPU_FAMILY_ARM)
@@ -125,6 +100,7 @@
     struct mm_struct *mm = current->mm;
     struct page **user_pages;
     const int write = 1;
+    const int force = 0;
     int pages_pinned;
 
     user_pages = nvidia_drm_calloc(pages_count, sizeof(*user_pages));
@@ -136,7 +112,8 @@
 
     down_read(&mm->mmap_sem);
 
-    pages_pinned = NV_GET_USER_PAGES(address, pages_count, write ? FOLL_WRITE : 0, user_pages, NULL);
+    pages_pinned = NV_GET_USER_PAGES(address, pages_count, write, force,
+                                     user_pages, NULL);
     up_read(&mm->mmap_sem);
 
     if (pages_pinned < 0 || (unsigned)pages_pinned < pages_count)
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-mmap.c 375.20/kernel/nvidia-drm/nvidia-drm-mmap.c
--- 367.57/kernel/nvidia-drm/nvidia-drm-mmap.c	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-mmap.c	2016-11-16 02:44:48.000000000 +0300
@@ -42,7 +42,7 @@
     drm_gem_object_unreference_unlocked(gem);
 }
 
-static struct vm_operations_struct nv_drm_vma_ops = {
+const struct vm_operations_struct nv_drm_gem_vma_ops = {
     .open  = nvidia_drm_vma_open,
     .close = nvidia_drm_vma_release,
 };
@@ -50,128 +50,48 @@
 int nvidia_drm_gem_mmap(struct file *filp, struct vm_area_struct *vma)
 {
     struct drm_file *file_priv = (struct drm_file*)filp->private_data;
-    struct drm_device *dev = file_priv->minor->dev;
 
+    struct drm_device *dev = file_priv->minor->dev;
     struct nvidia_drm_device *nv_dev = dev->dev_private;
 
-    u32 handle = 0;
-
     struct drm_gem_object *gem = NULL;
     struct nvidia_drm_gem_object *nv_gem = NULL;
+    unsigned long size = vma->vm_end - vma->vm_start;
+    unsigned long page_offset;
+    int ret = -EINVAL;
 
-    enum nvidia_drm_memory_cache_type cache_type;
-
-    unsigned long vma_size = vma->vm_end - vma->vm_start;
-    int ret = 0;
-
-    if (!nvidia_drm_modeset_enabled(dev))
-    {
-        return -EINVAL;
-    }
-
-    mutex_lock(&dev->struct_mutex);
-
-    /* Look up the GEM object based on the offset passed in vma->vm_pgoff */
-
-    idr_for_each_entry(&file_priv->object_idr, gem, handle)
-    {
-        unsigned long offset;
-
-        nv_gem = DRM_GEM_OBJECT_TO_NV_GEM_OBJECT(gem);
-
-        if (nv_gem->type != NV_DRM_GEM_OBJECT_TYPE_NVKMS_MEMORY)
-        {
-            continue;
-        }
-
-        if (!nv_gem->u.nvkms_memory.mapped)
-        {
-            continue;
-        }
-
-        offset = (unsigned long)(uintptr_t)nv_gem->u.nvkms_memory.pLinearAddress;
-        offset = offset >> PAGE_SHIFT;
-
-        if (offset == vma->vm_pgoff)
-        {
-            break;
-        }
+    if (!nvidia_drm_modeset_enabled(dev)) {
+        goto done;
     }
 
-    if (gem == NULL)
-    {
-        ret = -EINVAL;
-
+    if ((ret = drm_gem_mmap(filp, vma)) < 0) {
         NV_DRM_DEV_LOG_ERR(
             nv_dev,
-            "Failed to lookup gem object for vm_pgoff=0x%lx",
-            vma->vm_pgoff);
-        goto unlock_and_return;
-    }
-
-    drm_gem_object_reference(gem);
-
-    /* Check the caller has been granted access to the buffer object */
-
-    if (!drm_vma_node_is_allowed(&gem->vma_node, filp))
-    {
-        ret = -EACCES;
+            "drm_gem_mmap failed with error code %d",
+            ret);
+        goto done;
+    }
+
+    gem = vma->vm_private_data;
+    nv_gem = DRM_GEM_OBJECT_TO_NV_GEM_OBJECT(gem);
+
+    page_offset =
+        (unsigned long)(uintptr_t)nv_gem->u.nvkms_memory.pLinearAddress;
+    page_offset = page_offset >> PAGE_SHIFT;
+
+    if ((ret = nvidia_drm_remap_pfn_range(vma,
+                                          vma->vm_start,
+                                          page_offset, size,
+                                          vma->vm_page_prot)) < 0) {
+        drm_gem_object_reference(gem);
 
         NV_DRM_DEV_LOG_ERR(
             nv_dev,
-            "Invalid access to gem object 0x%p", gem);
-        goto failed;
+            "remap_pfn_range failed with error code %d",
+            ret);
     }
 
-    /* Validate vma size */
-
-    if (gem->size < vma_size)
-    {
-        ret = -EINVAL;
-
-        NV_DRM_DEV_LOG_ERR(
-            nv_dev,
-            "Trying to map gem object 0x%p on larger virtual memory region",
-            gem);
-        goto failed;
-    }
-
-    cache_type = NV_DRM_MEMORY_CACHE_TYPE_UNCACHED_WEAK;
-
-    if (nvKms->systemInfo.bAllowWriteCombining) {
-        cache_type = NV_DRM_MEMORY_CACHE_TYPE_WRITECOMBINED;
-    }
-
-    ret = nvidia_drm_encode_pgprot(cache_type, &vma->vm_page_prot);
-
-    if (ret != 0) {
-        NV_DRM_DEV_LOG_ERR(nv_dev, "Failed to encode pgprot");
-        goto failed;
-    }
-
-    ret = nvidia_drm_remap_pfn_range(vma,
-                                     vma->vm_start, vma->vm_pgoff, vma_size,
-                                     vma->vm_page_prot);
-
-    if (ret != 0)
-    {
-        NV_DRM_DEV_LOG_ERR(nv_dev, "Failed to mmap() gem object 0x%p", gem);
-        goto failed;
-    }
-
-    vma->vm_flags |= VM_IO;
-    vma->vm_private_data = gem;
-    vma->vm_ops = &nv_drm_vma_ops;
-
-    goto unlock_and_return;
-
-failed:
-
-    drm_gem_object_unreference_unlocked(gem);
-
-unlock_and_return:
-
-    mutex_unlock(&dev->struct_mutex);
+done:
 
     return ret;
 }
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-mmap.h 375.20/kernel/nvidia-drm/nvidia-drm-mmap.h
--- 367.57/kernel/nvidia-drm/nvidia-drm-mmap.h	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-mmap.h	2016-11-16 02:44:48.000000000 +0300
@@ -29,6 +29,8 @@
 
 #include <drm/drmP.h>
 
+extern const struct vm_operations_struct nv_drm_gem_vma_ops;
+
 int nvidia_drm_gem_mmap(struct file *filp, struct vm_area_struct *vma);
 
 #endif /* NV_DRM_ATOMIC_MODESET_AVAILABLE */
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-modeset.c 375.20/kernel/nvidia-drm/nvidia-drm-modeset.c
--- 367.57/kernel/nvidia-drm/nvidia-drm-modeset.c	2016-11-13 19:28:07.940578908 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-modeset.c	2016-11-16 02:44:48.000000000 +0300
@@ -37,6 +37,45 @@
 #include <drm/drm_atomic_helper.h>
 #include <drm/drm_crtc.h>
 
+#if defined(NV_DRM_MODE_CONFIG_FUNCS_HAS_ATOMIC_STATE_ALLOC)
+struct nvidia_drm_atomic_state {
+    struct NvKmsKapiRequestedModeSetConfig config;
+    struct drm_atomic_state base;
+};
+
+static inline struct nvidia_drm_atomic_state *to_nv_atomic_state(
+    struct drm_atomic_state *state)
+{
+    return container_of(state, struct nvidia_drm_atomic_state, base);
+}
+
+struct drm_atomic_state *nvidia_drm_atomic_state_alloc(struct drm_device *dev)
+{
+    struct nvidia_drm_atomic_state *nv_state =
+            nvidia_drm_calloc(1, sizeof(*nv_state));
+
+    if (nv_state == NULL || drm_atomic_state_init(dev, &nv_state->base) < 0) {
+        nvidia_drm_free(nv_state);
+        return NULL;
+    }
+
+    return &nv_state->base;
+}
+
+void nvidia_drm_atomic_state_clear(struct drm_atomic_state *state)
+{
+    drm_atomic_state_default_clear(state);
+}
+
+void nvidia_drm_atomic_state_free(struct drm_atomic_state *state)
+{
+    struct nvidia_drm_atomic_state *nv_state =
+                    to_nv_atomic_state(state);
+    drm_atomic_state_default_release(state);
+    nvidia_drm_free(nv_state);
+}
+#endif
+
 /*
  * In kernel versions before the addition of
  * drm_crtc_state::connectors_changed, connector changes were
@@ -60,6 +99,130 @@
            crtc_state->mode_changed;
 }
 
+static int head_modeset_config_attach_connector(
+    const struct drm_connector *connector,
+    struct NvKmsKapiHeadModeSetConfig *head_modeset_config)
+{
+    struct nvidia_drm_connector *nv_connector =
+            DRM_CONNECTOR_TO_NV_CONNECTOR(connector);
+    struct nvidia_drm_encoder *nv_encoder = nv_connector->nv_detected_encoder;
+
+    if (nv_encoder == NULL) {
+        struct nvidia_drm_device *nv_dev = connector->dev->dev_private;
+
+        NV_DRM_DEV_LOG_DEBUG(
+            nv_dev,
+            "Connector(%u) has no connected encoder",
+            nv_connector->physicalIndex);
+        return -EINVAL;
+    }
+
+    head_modeset_config->displays[head_modeset_config->numDisplays++] =
+        nv_encoder->hDisplay;
+
+    return 0;
+}
+
+static int setup_plane_config(struct drm_plane_state *plane_state,
+                              struct NvKmsKapiPlaneConfig *plane_config)
+{
+    struct nvidia_drm_framebuffer *nv_fb = NULL;
+
+    if (plane_state->fb == NULL) {
+        return 0;
+    }
+
+    nv_fb = DRM_FRAMEBUFFER_TO_NV_FRAMEBUFFER(plane_state->fb);
+
+    if (nv_fb == NULL || nv_fb->pSurface == NULL) {
+        struct nvidia_drm_device *nv_dev = plane_state->plane->dev->dev_private;
+
+        NV_DRM_DEV_LOG_DEBUG(
+            nv_dev,
+            "Invalid framebuffer object 0x%p",
+            nv_fb);
+        return -EINVAL;
+    }
+
+    plane_config->surface = nv_fb->pSurface;
+
+    /* Source values are 16.16 fixed point */
+
+    plane_config->srcX = plane_state->src_x >> 16;
+    plane_config->srcY = plane_state->src_y >> 16;
+    plane_config->srcWidth  = plane_state->src_w >> 16;
+    plane_config->srcHeight = plane_state->src_h >> 16;
+
+    plane_config->dstX = plane_state->crtc_x;
+    plane_config->dstY = plane_state->crtc_y;
+    plane_config->dstWidth  = plane_state->crtc_w;
+    plane_config->dstHeight = plane_state->crtc_h;
+
+    return 0;
+}
+
+static int setup_requested_head_modeset_config(
+    struct drm_crtc *crtc,
+    struct NvKmsKapiHeadRequestedConfig *head_requested_config)
+{
+    struct NvKmsKapiHeadModeSetConfig *head_modeset_config =
+        &head_requested_config->modeSetConfig;
+    struct drm_device *dev = crtc->dev;
+    struct drm_connector *connector;
+    struct drm_plane *plane;
+    int ret = 0;
+
+    list_for_each_entry(connector, &dev->mode_config.connector_list, head) {
+        struct drm_connector_state *connector_state = connector->state;
+
+        if (connector_state->crtc != crtc) {
+            continue;
+        }
+
+        if ((ret = head_modeset_config_attach_connector(
+                                        connector,
+                                        head_modeset_config)) < 0) {
+            goto done;
+        }
+    }
+
+    if (head_modeset_config->numDisplays == 0) {
+        goto done;
+    }
+
+    drm_mode_to_nvkms_display_mode(&crtc->state->mode,
+                                   &head_modeset_config->mode);
+
+    list_for_each_entry(plane, &dev->mode_config.plane_list, head) {
+        struct drm_plane_state *plane_state = plane->state;
+        NvKmsKapiPlaneType type;
+
+        if (!drm_plane_type_to_nvkms_plane_type(plane->type, &type)) {
+            struct nvidia_drm_device *nv_dev = dev->dev_private;
+
+            NV_DRM_DEV_LOG_DEBUG(
+                nv_dev,
+                "Unsupported drm plane type 0x%08x",
+                plane->type);
+            continue;
+        }
+
+        if (plane_state->crtc != crtc) {
+            continue;
+        }
+
+        if ((ret = setup_plane_config(
+              plane_state,
+              &head_requested_config->planeRequestedConfig[type].config)) < 0) {
+            goto done;
+        }
+    }
+
+done:
+
+    return ret;
+}
+
 static int drm_atomic_state_to_nvkms_requested_config(
     struct drm_atomic_state *state,
     struct NvKmsKapiRequestedModeSetConfig *requested_config)
@@ -96,8 +259,6 @@
     for_each_crtc_in_state(state, crtc, crtc_state, i)
     {
         struct nvidia_drm_crtc *nv_crtc;
-        unsigned int plane;
-
         struct NvKmsKapiHeadRequestedConfig *head_requested_config;
 
         /* Is this crtc is enabled and has anything changed? */
@@ -114,15 +275,12 @@
         head_requested_config =
             &requested_config->headRequestedConfig[nv_crtc->head];
 
-        /* Copy present configuration */
-
-        head_requested_config->modeSetConfig = nv_crtc->modeset_config;
-
-        for (plane = 0; plane < ARRAY_SIZE(nv_crtc->plane_config); plane++) {
-            struct NvKmsKapiPlaneConfig *plane_config =
-                &head_requested_config->planeRequestedConfig[plane].config;
+        /* Setup present configuration */
 
-            *plane_config = nv_crtc->plane_config[plane];
+        if ((ret = setup_requested_head_modeset_config(
+                                    crtc,
+                                    head_requested_config)) < 0) {
+            return ret;
         }
 
         /* Set mode-timing changes */
@@ -156,32 +314,20 @@
 
             head_requested_config->flags.displaysChanged = NV_TRUE;
 
-            for_each_connector_in_state(state, connector, connector_state, j)
-            {
-                struct nvidia_drm_connector *nv_connector;
-                struct nvidia_drm_encoder *nv_encoder;
-
-                if (connector_state->crtc != crtc)
-                {
+            for_each_connector_in_state(state, connector, connector_state, j) {
+                if (connector_state->crtc != crtc) {
                     continue;
                 }
 
-                nv_connector = DRM_CONNECTOR_TO_NV_CONNECTOR(connector);
-                nv_encoder = nv_connector->nv_detected_encoder;
-
-                if (nv_encoder == NULL)
-                {
-                    NV_DRM_DEV_LOG_DEBUG(
-                        nv_dev,
-                        "Connector(%u) has no connected encoder",
-                        nv_connector->physicalIndex);
-                    return -EINVAL;
+                if ((ret = head_modeset_config_attach_connector(
+                                                connector,
+                                                head_modeset_config)) < 0) {
+                    return ret;
                 }
-
-                head_modeset_config->displays[0] = nv_encoder->hDisplay;
-                head_modeset_config->numDisplays = 1;
-                break;
             }
+
+            crtc_state->active =
+                (head_modeset_config->numDisplays != 0);
         }
     }
 
@@ -253,39 +399,13 @@
 
         /* Disable plane if there is no display attached to crtc */
 
-        if (head_requested_config->modeSetConfig.numDisplays == 0 || disable)
-        {
+        if (head_requested_config->modeSetConfig.numDisplays == 0 || disable) {
             plane_config->surface = NULL;
         }
-        else
-        {
-            struct nvidia_drm_framebuffer *nv_fb =
-                DRM_FRAMEBUFFER_TO_NV_FRAMEBUFFER(plane_state->fb);
-
-            if (nv_fb == NULL || nv_fb->pSurface == NULL)
-            {
-                NV_DRM_DEV_LOG_DEBUG(
-                    nv_dev,
-                    "Invalid framebuffer object 0x%p",
-                    nv_fb);
-                return -EINVAL;
-            }
-
-            plane_config->surface = nv_fb->pSurface;
+        else if ((ret = setup_plane_config(plane_state, plane_config)) < 0) {
+            return ret;
         }
 
-        /* Source values are 16.16 fixed point */
-
-        plane_config->srcX = plane_state->src_x >> 16;
-        plane_config->srcY = plane_state->src_y >> 16;
-        plane_config->srcWidth  = plane_state->src_w >> 16;
-        plane_config->srcHeight = plane_state->src_h >> 16;
-
-        plane_config->dstX = plane_state->crtc_x;
-        plane_config->dstY = plane_state->crtc_y;
-        plane_config->dstWidth  = plane_state->crtc_w;
-        plane_config->dstHeight = plane_state->crtc_h;
-
         /*
          * If plane surface remains NULL then ignore all other changes
          * because there is nothing to show.
@@ -295,9 +415,15 @@
             continue;
         }
 
+        /*
+         * Unconditionally mark the surface as changed, even if nothing
+         * changed, so that we always get a flip event: a DRM client may
+         * flip with the same surface and wait for a flip event.
+         */
+        plane_requested_config->flags.surfaceChanged = NV_TRUE;
+
         if (old_plane_config.surface == NULL &&
             old_plane_config.surface != plane_config->surface) {
-            plane_requested_config->flags.surfaceChanged = NV_TRUE;
             plane_requested_config->flags.srcXYChanged = NV_TRUE;
             plane_requested_config->flags.srcWHChanged = NV_TRUE;
             plane_requested_config->flags.dstXYChanged = NV_TRUE;
@@ -305,10 +431,6 @@
             continue;
         }
 
-        if (old_plane_config.surface != plane_config->surface) {
-            plane_requested_config->flags.surfaceChanged = NV_TRUE;
-        }
-
         if (old_plane_config.srcX != plane_config->srcX ||
             old_plane_config.srcY != plane_config->srcY) {
             plane_requested_config->flags.srcXYChanged = NV_TRUE;
@@ -333,7 +455,6 @@
     return 0;
 }
 
-
 int nvidia_drm_atomic_check(struct drm_device *dev,
                             struct drm_atomic_state *state)
 {
@@ -343,12 +464,16 @@
 
     int ret = 0;
 
+#if defined(NV_DRM_MODE_CONFIG_FUNCS_HAS_ATOMIC_STATE_ALLOC)
+    requested_config = &(to_nv_atomic_state(state)->config);
+#else
     requested_config = nvidia_drm_calloc(1, sizeof(*requested_config));
 
     if (requested_config == NULL)
     {
         return -ENOMEM;
     }
+#endif
 
     ret = drm_atomic_state_to_nvkms_requested_config(state, requested_config);
 
@@ -369,165 +494,81 @@
 
 done:
 
+#if !defined(NV_DRM_MODE_CONFIG_FUNCS_HAS_ATOMIC_STATE_ALLOC)
     nvidia_drm_free(requested_config);
+#endif
 
     return ret;
 }
 
-static void nvidia_drm_update_head_mode_config(
-    const struct drm_atomic_state *state,
-    const struct NvKmsKapiRequestedModeSetConfig *requested_config)
-{
-    unsigned int head;
-
-    for (head = 0;
-         head < ARRAY_SIZE(requested_config->headRequestedConfig); head++)
-    {
-        struct drm_crtc *crtc = NULL;
-        struct drm_crtc_state *crtc_state = NULL;
-
-        int i;
+#if defined(NV_DRM_ATOMIC_MODESET_NONBLOCKING_COMMIT_AVAILABLE)
 
-        if ((requested_config->headsMask & (1 << head)) == 0x0)
-        {
-            continue;
-        }
-
-        for_each_crtc_in_state(state, crtc, crtc_state, i)
-        {
-            struct nvidia_drm_crtc *nv_crtc = DRM_CRTC_TO_NV_CRTC(crtc);
-
-            if (nv_crtc->head == head)
-            {
-                const struct NvKmsKapiHeadRequestedConfig
-                    *head_requested_config =
-                     &requested_config->headRequestedConfig[head];
-                unsigned plane;
-
-                nv_crtc->modeset_config =
-                    head_requested_config->modeSetConfig;
-
-                for (plane = 0;
-                     plane < ARRAY_SIZE(head_requested_config->planeRequestedConfig);
-                     plane++)
-                {
-                    nv_crtc->plane_config[plane] =
-                        head_requested_config->
-                            planeRequestedConfig[plane].config;
-                }
-
-                break;
-            }
-        }
-    }
-}
-
-static bool nvidia_drm_has_pending_flip(struct drm_device *dev,
-                                        struct drm_atomic_state *state)
+void nvidia_drm_atomic_helper_commit_tail(struct drm_atomic_state *state)
 {
+    struct drm_device *dev = state->dev;
     struct nvidia_drm_device *nv_dev =
         (struct nvidia_drm_device*)dev->dev_private;
+    struct NvKmsKapiRequestedModeSetConfig *requested_config =
+        &(to_nv_atomic_state(state)->config);
 
     int i;
-    struct drm_plane *plane;
-    struct drm_plane_state *plane_state;
+    struct drm_crtc *crtc;
+    struct drm_crtc_state *crtc_state;
 
-    for_each_plane_in_state(state, plane, plane_state, i) {
-        NvBool has_pending_flip;
-        NvKmsKapiPlaneType nv_plane;
-        struct nvidia_drm_crtc *nv_crtc;
-        struct drm_crtc *crtc;
+    if (nvKms->systemInfo.bAllowWriteCombining) {
+        /*
+         * XXX This call is required only if dumb buffer is going
+         * to be presented.
+         */
+         nvidia_drm_write_combine_flush();
+    }
 
-        crtc = plane->state->crtc;
-        if (crtc == NULL) {
-            /*
-             * Plane state is changing from active ---> disabled or
-             * from disabled ---> active.
-             */
-            crtc = plane_state->crtc;
-        }
+    if (!nvKms->applyModeSetConfig(nv_dev->pDevice,
+                                   requested_config, NV_TRUE)) {
+        NV_DRM_DEV_LOG_ERR(
+            nv_dev,
+            "Failed to commit NvKmsKapiModeSetConfig");
+    }
 
-        if (crtc == NULL) {
-            continue;
+    for_each_crtc_in_state(state, crtc, crtc_state, i) {
+        struct nvidia_drm_crtc *nv_crtc = DRM_CRTC_TO_NV_CRTC(crtc);
+        struct drm_crtc_commit *commit;
+
+        spin_lock(&crtc->commit_lock);
+        commit = list_first_entry_or_null(&crtc->commit_list,
+                                          struct drm_crtc_commit, commit_entry);
+        if (commit) {
+            drm_crtc_commit_get(commit);
         }
+        spin_unlock(&crtc->commit_lock);
 
-        if (!drm_plane_type_to_nvkms_plane_type(plane->type, &nv_plane)) {
-            NV_DRM_DEV_LOG_ERR(
-                nv_dev,
-                "Unsupported drm plane type 0x%08x",
-                plane->type);
+        if (!commit) {
             continue;
         }
 
-        nv_crtc = DRM_CRTC_TO_NV_CRTC(crtc);
-
-        if (!nvKms->getFlipPendingStatus(nv_dev->pDevice,
-                                  nv_crtc->head, nv_plane, &has_pending_flip)) {
+        if (wait_for_completion_timeout(&commit->flip_done, 3*HZ) == 0) {
             NV_DRM_DEV_LOG_ERR(
                 nv_dev,
-                "->getFlipPendingStatus() failed for head = %u and "
-                "plane = 0x%08x",
-                nv_crtc->head,
-                nv_plane);
-            continue;
-
+                "Flip event timeout on head %u", nv_crtc->head);
         }
 
-        if (has_pending_flip) {
-            return true;
-        }
+        drm_crtc_commit_put(commit);
     }
 
-    return false;
-}
-
-static void nvidia_drm_wait_pending_flip(struct drm_device *dev,
-                                         struct drm_atomic_state *state)
-{
-    struct nvidia_drm_device *nv_dev =
-        (struct nvidia_drm_device*)dev->dev_private;
-
-    bool has_flip_complete = false;
-
-    uint64_t timeout = nvidia_drm_get_time_usec() + 3000000 /* 3 seconds */;
-
-    do {
-        if (!nvidia_drm_has_pending_flip(dev, state)) {
-            has_flip_complete = true;
-            break;
-        }
-    } while (nvidia_drm_get_time_usec() < timeout);
-
+    drm_atomic_helper_commit_hw_done(state);
 
-    if (!has_flip_complete) {
-        NV_DRM_DEV_LOG_ERR(
-            nv_dev,
-            "Flip completion timeout occurred");
-    }
+    drm_atomic_helper_cleanup_planes(dev, state);
 }
 
-static int nvidia_drm_wait_for_pending_commit(struct drm_crtc *crtc)
-{
-    struct nvidia_drm_crtc *nv_crtc = DRM_CRTC_TO_NV_CRTC(crtc);
-
-    struct drm_device *dev = crtc->dev;
-    struct nvidia_drm_device *nv_dev = dev->dev_private;
-
-    if (wait_event_timeout(nv_dev->pending_commit_queue,
-                           !atomic_read(&nv_crtc->has_pending_commit),
-                           3 * HZ /* 3 second */) == 0) {
-        return -EBUSY;
-    }
-
-    return 0;
-}
+#else
 
 struct nvidia_drm_atomic_commit_task {
     struct drm_device *dev;
     struct drm_atomic_state *state;
-    bool async;
 
+#if !defined(NV_DRM_MODE_CONFIG_FUNCS_HAS_ATOMIC_STATE_ALLOC)
     struct NvKmsKapiRequestedModeSetConfig *requested_config;
+#endif
 
     struct work_struct work;
 };
@@ -552,13 +593,16 @@
 
     struct drm_device *dev = nv_commit_task->dev;
     struct drm_atomic_state *state = nv_commit_task->state;
-    bool async = nv_commit_task->async;
 
     struct nvidia_drm_device *nv_dev =
         (struct nvidia_drm_device*)dev->dev_private;
 
     struct NvKmsKapiRequestedModeSetConfig *requested_config =
+    #if defined(NV_DRM_MODE_CONFIG_FUNCS_HAS_ATOMIC_STATE_ALLOC)
+        &(to_nv_atomic_state(state)->config);
+    #else
         nv_commit_task->requested_config;
+    #endif
 
     int i;
     struct drm_crtc *crtc;
@@ -579,11 +623,17 @@
             "Failed to commit NvKmsKapiModeSetConfig");
     }
 
-    /*
-     * Wait for flip completion if synchronous commit is requested.
-     */
-    if (!async) {
-        nvidia_drm_wait_pending_flip(dev, state);
+    for_each_crtc_in_state(state, crtc, crtc_state, i) {
+        struct nvidia_drm_crtc *nv_crtc = DRM_CRTC_TO_NV_CRTC(crtc);
+
+        if (wait_event_timeout(
+                nv_dev->pending_flip_queue,
+                !atomic_read(&nv_crtc->has_pending_flip_event),
+                3 * HZ /* 3 second */) == 0) {
+            NV_DRM_DEV_LOG_ERR(
+                nv_dev,
+                "Flip event timeout on head %u", nv_crtc->head);
+        }
     }
 
     drm_atomic_helper_cleanup_planes(dev, state);
@@ -597,13 +647,17 @@
 
     drm_atomic_state_free(state);
 
+#if !defined(NV_DRM_MODE_CONFIG_FUNCS_HAS_ATOMIC_STATE_ALLOC)
     nvidia_drm_free(requested_config);
+#endif
 
     nvidia_drm_free(nv_commit_task);
 }
 
-int nvidia_drm_atomic_commit(struct drm_device *dev,
-                             struct drm_atomic_state *state, bool async)
+static int nvidia_drm_atomic_commit_internal(
+    struct drm_device *dev,
+    struct drm_atomic_state *state,
+    bool async)
 {
     int ret = 0;
 
@@ -622,6 +676,17 @@
         goto failed;
     }
 
+#if defined(NV_DRM_MODE_CONFIG_FUNCS_HAS_ATOMIC_STATE_ALLOC)
+    /*
+     * Not required to convert convert drm_atomic_state to
+     * NvKmsKapiRequestedModeSetConfig because it has been already
+     * happened in nvidia_drm_atomic_check().
+     *
+     * Core DRM guarantees to call into nvidia_drm_atomic_check() before
+     * calling into nvidia_drm_atomic_commit().
+     */
+    requested_config = &(to_nv_atomic_state(state)->config);
+#else
     requested_config = nvidia_drm_calloc(1, sizeof(*requested_config));
 
     if (requested_config == NULL)
@@ -637,57 +702,108 @@
         NV_DRM_LOG_ERR("Failed to convert atomic state to NvKmsKapiModeSetConfig");
         goto failed;
     }
+#endif
 
     /*
-     * Wait for previous flips to complete if synchronous commit is requested.
+     * drm_mode_config_funcs::atomic_commit() mandates to return -EBUSY
+     * for asynchronous commit if previous updates (commit tasks/flip event) are
+     * pending. In case of synchronous commits it mandates to wait for previous
+     * updates to complete.
      */
     if (!async) {
-        nvidia_drm_wait_pending_flip(dev, state);
+        /*
+         * Serialize commits and flip events on crtc, in order to avoid race
+         * condition between two/more nvKms->applyModeSetConfig() on single
+         * crtc and generate flip events in correct order.
+         */
+        for_each_crtc_in_state(state, crtc, crtc_state, i) {
+            struct nvidia_drm_device *nv_dev =
+                (struct nvidia_drm_device*)dev->dev_private;
+            struct nvidia_drm_crtc *nv_crtc = DRM_CRTC_TO_NV_CRTC(crtc);
+
+            if (wait_event_timeout(
+                    nv_dev->pending_flip_queue,
+                    !atomic_read(&nv_crtc->has_pending_flip_event),
+                    3 * HZ /* 3 second */) == 0) {
+                ret = -EBUSY;
+                goto failed;
+            }
+
+            if (wait_event_timeout(
+                    nv_dev->pending_commit_queue,
+                    !atomic_read(&nv_crtc->has_pending_commit),
+                    3 * HZ /* 3 second */) == 0) {
+                ret = -EBUSY;
+                goto failed;
+            }
+        }
+    } else {
+        for_each_crtc_in_state(state, crtc, crtc_state, i) {
+            struct nvidia_drm_crtc *nv_crtc = DRM_CRTC_TO_NV_CRTC(crtc);
+
+            if (atomic_read(&nv_crtc->has_pending_commit) ||
+                atomic_read(&nv_crtc->has_pending_flip_event)) {
+                ret = -EBUSY;
+                goto failed;
+            }
+        }
     }
 
-    /*
-     * DRM mandates to return EBUSY error if previous flip is not completed yet.
-     *
-     * DRM client has to listen DRM_MODE_PAGE_FLIP_EVENT otherwise
-     * use synchronous ioctl.
-     */
+    ret = drm_atomic_helper_prepare_planes(dev, state);
 
-    if (nvidia_drm_has_pending_flip(dev, state)) {
-        ret = -EBUSY;
+    if (ret != 0)
+    {
         goto failed;
     }
 
     /*
-     * Serialize commits on crtc, wait for pending commits to finish.
+     * Mark all affected crtcs which will have pending commits and/or
+     * flip events.
      */
+
     for_each_crtc_in_state(state, crtc, crtc_state, i) {
-        ret = nvidia_drm_wait_for_pending_commit(crtc);
+        struct nvidia_drm_crtc *nv_crtc = DRM_CRTC_TO_NV_CRTC(crtc);
+
+        int j;
+        struct drm_plane *plane;
+        struct drm_plane_state *plane_state;
 
-        if (ret != 0) {
-            goto failed;
+        atomic_set(&nv_crtc->has_pending_commit, true);
+
+        if (!crtc->state->active && !crtc_state->active) {
+            continue;
         }
-    }
 
-    ret = drm_atomic_helper_prepare_planes(dev, state);
+        for_each_plane_in_state(state, plane, plane_state, j) {
+            /*
+             * Plane state is changing from active ---> disabled or
+             * from disabled ---> active.
+             */
 
-    if (ret != 0)
-    {
-        goto failed;
+            if (crtc == plane->state->crtc || crtc == plane_state->crtc) {
+                switch (plane->type) {
+                    case DRM_PLANE_TYPE_PRIMARY:
+                        atomic_set(&nv_crtc->has_pending_flip_event, true);
+                        break;
+                    case DRM_PLANE_TYPE_OVERLAY:
+                    case DRM_PLANE_TYPE_CURSOR:
+                        /* TODO */
+                        break;
+                }
+            }
+        }
     }
 
-    drm_atomic_helper_swap_state(state, async);
-
-    nvidia_drm_update_head_mode_config(state, requested_config);
+    drm_atomic_helper_swap_state(dev, state);
 
     NVIDIA_DRM_INIT_WORK(&nv_commit_task->work,
                          nvidia_drm_atomic_commit_task_callback);
 
     nv_commit_task->dev = dev;
-
     nv_commit_task->state = state;
-    nv_commit_task->async = async;
-
+#if !defined(NV_DRM_MODE_CONFIG_FUNCS_HAS_ATOMIC_STATE_ALLOC)
     nv_commit_task->requested_config = requested_config;
+#endif
 
     if (async)
     {
@@ -702,13 +818,27 @@
 
 failed:
 
+#if !defined(NV_DRM_MODE_CONFIG_FUNCS_HAS_ATOMIC_STATE_ALLOC)
     nvidia_drm_free(requested_config);
+#endif
 
     nvidia_drm_free(nv_commit_task);
 
     return ret;
 }
 
+#endif /* NV_DRM_ATOMIC_MODESET_NONBLOCKING_COMMIT_AVAILABLE */
+
+int nvidia_drm_atomic_commit(struct drm_device *dev,
+                             struct drm_atomic_state *state, bool async)
+{
+#if defined(NV_DRM_ATOMIC_MODESET_NONBLOCKING_COMMIT_AVAILABLE)
+    return drm_atomic_helper_commit(dev, state, async);
+#else
+    return nvidia_drm_atomic_commit_internal(dev, state, async);
+#endif
+}
+
 void nvidia_drm_handle_flip_occurred(struct nvidia_drm_device *nv_dev,
                                      NvU32 head,
                                      NvKmsKapiPlaneType plane)
@@ -719,17 +849,23 @@
     {
         case NVKMS_KAPI_PLANE_PRIMARY:
         {
-            struct drm_device *dev = nv_dev->dev;
+            struct nvidia_drm_crtc *nv_crtc = nv_dev->nv_crtc[head];
+            struct drm_crtc *crtc = &nv_crtc->base;
 
-            struct drm_crtc *crtc = &nv_dev->nv_crtc[head]->base;
             struct drm_crtc_state *crtc_state = crtc->state;
 
+            spin_lock(&nv_dev->dev->event_lock);
             if (crtc_state->event != NULL) {
-                spin_lock(&dev->event_lock);
                 drm_crtc_send_vblank_event(crtc, crtc_state->event);
-                spin_unlock(&dev->event_lock);
             }
+            crtc_state->event = NULL;
+            spin_unlock(&nv_dev->dev->event_lock);
 
+#if !defined(NV_DRM_ATOMIC_MODESET_NONBLOCKING_COMMIT_AVAILABLE)
+            WARN_ON(!atomic_read(&nv_crtc->has_pending_flip_event));
+            atomic_set(&nv_crtc->has_pending_flip_event, false);
+            wake_up_all(&nv_dev->pending_flip_queue);
+#endif
             break;
         }
 
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-modeset.h 375.20/kernel/nvidia-drm/nvidia-drm-modeset.h
--- 367.57/kernel/nvidia-drm/nvidia-drm-modeset.h	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-modeset.h	2016-11-16 02:44:48.000000000 +0300
@@ -29,6 +29,16 @@
 
 #include <drm/drmP.h>
 
+#if defined(NV_DRM_MODE_CONFIG_FUNCS_HAS_ATOMIC_STATE_ALLOC)
+struct drm_atomic_state *nvidia_drm_atomic_state_alloc(struct drm_device *dev);
+void nvidia_drm_atomic_state_clear(struct drm_atomic_state *state);
+void nvidia_drm_atomic_state_free(struct drm_atomic_state *state);
+#endif
+
+#if defined(NV_DRM_ATOMIC_MODESET_NONBLOCKING_COMMIT_AVAILABLE)
+void nvidia_drm_atomic_helper_commit_tail(struct drm_atomic_state *state);
+#endif
+
 int nvidia_drm_atomic_check(struct drm_device *dev,
                             struct drm_atomic_state *state);
 
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-os-interface.h 375.20/kernel/nvidia-drm/nvidia-drm-os-interface.h
--- 367.57/kernel/nvidia-drm/nvidia-drm-os-interface.h	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-os-interface.h	2016-11-16 02:44:48.000000000 +0300
@@ -40,15 +40,6 @@
 
 char *nvidia_drm_asprintf(const char *fmt, ...);
 
-enum nvidia_drm_memory_cache_type {
-    NV_DRM_MEMORY_CACHE_TYPE_UNCACHED_WEAK,
-    NV_DRM_MEMORY_CACHE_TYPE_UNCACHED,
-    NV_DRM_MEMORY_CACHE_TYPE_WRITECOMBINED,
-};
-
-int nvidia_drm_encode_pgprot(enum nvidia_drm_memory_cache_type cache_type,
-                             pgprot_t *prot);
-
 void nvidia_drm_write_combine_flush(void);
 
 int nvidia_drm_remap_pfn_range(struct vm_area_struct *vma,
diff -ur 367.57/kernel/nvidia-drm/nvidia-drm-priv.h 375.20/kernel/nvidia-drm/nvidia-drm-priv.h
--- 367.57/kernel/nvidia-drm/nvidia-drm-priv.h	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/nvidia-drm/nvidia-drm-priv.h	2016-11-16 02:44:48.000000000 +0300
@@ -33,6 +33,11 @@
 #include <drm/drm_gem.h>
 #endif
 
+#if defined(NV_DRM_DRIVER_HAS_GEM_PRIME_RES_OBJ)
+#include <linux/fence.h>
+#include <linux/reservation.h>
+#endif
+
 #include "nvidia-drm-os-interface.h"
 
 #include "nvkms-kapi.h"
@@ -77,6 +82,7 @@
 
     struct drm_device *dev;
 
+    struct NvKmsKapiDevice *pDevice;
 
 #if defined(NV_DRM_ATOMIC_MODESET_AVAILABLE)
     /*
@@ -103,14 +109,19 @@
      */
     struct mutex lock;
 
-    struct NvKmsKapiDevice *pDevice;
     NvU32 pitchAlignment;
 
     struct nvidia_drm_crtc *nv_crtc[NVKMS_KAPI_MAX_HEADS];
 
+
     atomic_t enable_event_handling;
 
+#if !defined(NV_DRM_ATOMIC_MODESET_NONBLOCKING_COMMIT_AVAILABLE)
+    /* Wait queue for pending tasks like modeset commit and flip event */
     wait_queue_head_t pending_commit_queue;
+    wait_queue_head_t pending_flip_queue;
+#endif
+
 #endif
 
     struct nvidia_drm_device *next;
diff -ur 367.57/kernel/nvidia-modeset/nvidia-modeset.Kbuild 375.20/kernel/nvidia-modeset/nvidia-modeset.Kbuild
--- 367.57/kernel/nvidia-modeset/nvidia-modeset.Kbuild	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/nvidia-modeset/nvidia-modeset.Kbuild	2016-11-16 02:44:49.000000000 +0300
@@ -80,6 +80,7 @@
 
 NV_CONFTEST_MACRO_COMPILE_TESTS += INIT_WORK
 NV_CONFTEST_TYPE_COMPILE_TESTS += file_inode
+NV_CONFTEST_TYPE_COMPILE_TESTS += file_operations
 NV_CONFTEST_TYPE_COMPILE_TESTS += proc_dir_entry
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += proc_create_data
 NV_CONFTEST_FUNCTION_COMPILE_TESTS += pde_data
diff -ur 367.57/kernel/nvidia-modeset/nvidia-modeset-linux.c 375.20/kernel/nvidia-modeset/nvidia-modeset-linux.c
--- 367.57/kernel/nvidia-modeset/nvidia-modeset-linux.c	2016-11-09 18:48:50.741826088 +0300
+++ 375.20/kernel/nvidia-modeset/nvidia-modeset-linux.c	2016-11-25 17:45:06.308249824 +0300
@@ -35,6 +35,19 @@
 #include "conftest.h"
 #include "nv-procfs.h"
 
+#if defined(NVCPU_X86_64) && defined(CONFIG_IA32_EMULATION) && \
+  !defined(NV_FILE_OPERATIONS_HAS_COMPAT_IOCTL)
+#  define NV_NEEDS_COMPAT_IOCTL_REGISTRATION 1
+#else
+#  define NV_NEEDS_COMPAT_IOCTL_REGISTRATION 0
+#endif
+
+
+#if NV_NEEDS_COMPAT_IOCTL_REGISTRATION
+#include <linux/syscalls.h> /* sys_ioctl() */
+#include <linux/ioctl32.h> /* register_ioctl32_conversion() */
+#endif
+
 #define NVKMS_MAJOR_DEVICE_NUMBER 195
 #define NVKMS_MINOR_DEVICE_NUMBER 254
 
@@ -548,6 +561,8 @@
 nvkms_init_timer(struct nvkms_timer_t *timer, nvkms_timer_proc_t *proc,
                  void *dataPtr, NvU32 dataU32, NvBool isRefPtr, NvU64 usec)
 {
+    unsigned long flags = 0;
+
     memset(timer, 0, sizeof(*timer));
     timer->cancel = NV_FALSE;
     timer->complete = NV_FALSE;
@@ -566,7 +581,7 @@
      * run in parallel with this, it could race against nvkms_init_timer()
      * and free the timer before its initialization is complete.
      */
-    spin_lock_bh(&nvkms_timers.lock);
+    spin_lock_irqsave(&nvkms_timers.lock, flags);
     list_add(&timer->timers_list, &nvkms_timers.list);
 
     if (usec == 0) {
@@ -579,7 +594,7 @@
         timer->kernel_timer.data = (unsigned long) timer;
         mod_timer(&timer->kernel_timer, jiffies + NVKMS_USECS_TO_JIFFIES(usec));
     }
-    spin_unlock_bh(&nvkms_timers.lock);
+    spin_unlock_irqrestore(&nvkms_timers.lock, flags);
 }
 
 nvkms_timer_handle_t*
@@ -892,7 +907,11 @@
 nvkms_sema_handle_t* NVKMS_API_CALL nvkms_sema_alloc(void)
 {
     nvkms_sema_handle_t *sema = nvkms_alloc(sizeof(*sema), NV_TRUE);
-    sema_init(&sema->os_sema, 1);
+
+    if (sema != NULL) {
+        sema_init(&sema->os_sema, 1);
+    }
+
     return sema;
 }
 
@@ -921,33 +940,40 @@
 
 struct proc_dir_entry *nvkms_proc_dir;
 
+static void NVKMS_API_CALL nv_procfs_out_string(void *data, const char *str)
+{
+    struct seq_file *s = data;
+
+    seq_puts(s, str);
+}
+
 static int nv_procfs_read_nvkms_proc(struct seq_file *s, void *arg)
 {
     char *buffer;
     nvkms_procfs_proc_t *func;
 
-#define NVKMS_PROCFS_FILE_SIZE 8196
+#define NVKMS_PROCFS_STRING_SIZE 8192
 
     func = s->private;
     if (func == NULL) {
         return 0;
     }
 
-    buffer = nvkms_alloc(NVKMS_PROCFS_FILE_SIZE, NV_TRUE);
+    buffer = nvkms_alloc(NVKMS_PROCFS_STRING_SIZE, NV_TRUE);
 
     if (buffer != NULL) {
         int status = down_interruptible(&nvkms_lock);
 
         if (status != 0) {
-            nvkms_free(buffer, NVKMS_PROCFS_FILE_SIZE);
+            nvkms_free(buffer, NVKMS_PROCFS_STRING_SIZE);
             return status;
         }
 
-        func(buffer, NVKMS_PROCFS_FILE_SIZE);
+        func(s, buffer, NVKMS_PROCFS_STRING_SIZE, &nv_procfs_out_string);
+
         up(&nvkms_lock);
 
-        seq_puts(s, buffer);
-        nvkms_free(buffer, NVKMS_PROCFS_FILE_SIZE);
+        nvkms_free(buffer, NVKMS_PROCFS_STRING_SIZE);
     }
 
     return 0;
@@ -1113,6 +1139,20 @@
                               params.size);
 }
 
+static void nvkms_register_compatible_ioctl(void)
+{
+#if NV_NEEDS_COMPAT_IOCTL_REGISTRATION
+    register_ioctl32_conversion(NVKMS_IOCTL_IOWR, (void *)sys_ioctl);
+#endif
+}
+
+static void nvkms_unregister_compatible_ioctl(void)
+{
+#if NV_NEEDS_COMPAT_IOCTL_REGISTRATION
+    unregister_ioctl32_conversion(NVKMS_IOCTL_IOWR);
+#endif
+}
+
 static unsigned int nvkms_poll(struct file *filp, poll_table *wait)
 {
     unsigned int mask = 0;
@@ -1172,6 +1212,8 @@
         return ret;
     }
 
+    nvkms_register_compatible_ioctl();
+
     down(&nvkms_lock);
     nvKmsModuleLoad();
     up(&nvkms_lock);
@@ -1233,6 +1275,8 @@
 
     flush_scheduled_work();
 
+    nvkms_unregister_compatible_ioctl();
+
     nvidia_unregister_module(&nvidia_modeset_module);
     nvkms_free_rm();
 }
diff -ur 367.57/kernel/nvidia-modeset/nvkms.h 375.20/kernel/nvidia-modeset/nvkms.h
--- 367.57/kernel/nvidia-modeset/nvkms.h	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/nvidia-modeset/nvkms.h	2016-11-16 02:44:49.000000000 +0300
@@ -26,7 +26,12 @@
 
 typedef struct nvkms_per_open nvkms_per_open_handle_t;
 
-typedef void NVKMS_API_CALL nvkms_procfs_proc_t(char *buffer, size_t size);
+typedef void NVKMS_API_CALL nvkms_procfs_out_string_func_t(void *data,
+                                                           const char *str);
+
+typedef void NVKMS_API_CALL nvkms_procfs_proc_t(void *data,
+                                                char *buffer, size_t size,
+                                                nvkms_procfs_out_string_func_t *outString);
 
 typedef struct {
     const char *name;
diff -ur 367.57/kernel/nvidia-modeset/nvkms-ioctl.h 375.20/kernel/nvidia-modeset/nvkms-ioctl.h
--- 367.57/kernel/nvidia-modeset/nvkms-ioctl.h	2016-10-04 05:40:11.000000000 +0300
+++ 375.20/kernel/nvidia-modeset/nvkms-ioctl.h	2016-11-16 02:44:49.000000000 +0300
@@ -28,6 +28,9 @@
 #define NVKMS_IOCTL_MAGIC 'm'
 #define NVKMS_IOCTL_CMD 0
 
+#define NVKMS_IOCTL_IOWR \
+    _IOWR(NVKMS_IOCTL_MAGIC, NVKMS_IOCTL_CMD, struct NvKmsIoctlParams)
+
 /*!
  * User-space pointers are always passed to NVKMS in an NvU64.
  * This user-space address is eventually passed into the platform's
Binary files 367.57/kernel/nvidia-modeset/nv-modeset-kernel.o_binary and 375.20/kernel/nvidia-modeset/nv-modeset-kernel.o_binary differ
diff -ur 367.57/kernel/nvidia-uvm/clc076.h 375.20/kernel/nvidia-uvm/clc076.h
--- 367.57/kernel/nvidia-uvm/clc076.h	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/clc076.h	2016-11-16 02:53:30.000000000 +0300
@@ -18,7 +18,7 @@
 
 #include "nvtypes.h"
 
-#define GP100_UVM_SW                                                (0x00000076)
+#define GP100_UVM_SW                                                (0x0000c076)
 
 #define NVC076_SET_OBJECT                                           (0x00000000)
 #define NVC076_NO_OPERATION                                         (0x00000100)
diff -ur 367.57/kernel/nvidia-uvm/compat.h 375.20/kernel/nvidia-uvm/compat.h
--- 367.57/kernel/nvidia-uvm/compat.h	2016-10-04 05:42:26.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/compat.h	2016-11-16 02:48:58.000000000 +0300
@@ -15,6 +15,15 @@
 #include "nvtypes.h"
 #include "nvmisc.h"
 #include "nvstatus.h"
+#if !defined(NV_UVM_ENABLE)
+#include "nvport/nvport.h"
+#endif
+
+#if defined(PORT_IS_MODULE_SUPPORTED)
+#if PORT_IS_MODULE_SUPPORTED(memory)
+#define SHR_USE_NVPORT_FUNCTIONS
+#endif
+#endif
 
 #if defined(NVMEM_MACOSX_BUILD)
 #include "nvMmuWalkCompat.h"
@@ -43,16 +52,17 @@
 #if defined(DBG)
 #define SHR_DEBUG
 #endif
-
+#ifndef SHR_USE_NVPORT_FUNCTIONS
 // function translations and/or renaming
 #define shrFreeMemory                      nvmOsFreeMem
 #define shrAllocMem                        nvmOsAllocMem
 #define shrAllocMemEx                      nvmOsAllocMem
 #define shrFreeMem                         nvmOsFreeMem
-#define shrAllocPages                      nvmOsAllocPages
-#define shrFreePages                       nvmOsFreePages
 #define shrMemSet                          nvmOsMemSet
 #define shrMemCopy                         nvmOsMemCopy
+#endif
+#define shrAllocPages                      nvmOsAllocPages
+#define shrFreePages                       nvmOsFreePages
 #define shrMapSystemMemory                 nvmOsMapSystemMemory
 #define shrUnmapSystemMemory               nvmOsUnmapSystemMemory
 #define shrGetCurrentProcess               nvmOsGetCurrentProcess
@@ -108,12 +118,14 @@
         (FMT, ## __VA_ARGS__));                                                 \
 } while (0)
 
+#ifndef SHR_USE_NVPORT_FUNCTIONS
 // function translations and/or renaming
 #define shrAllocMem                        nvShrMemAlloc
 #define shrFreeMem                         nvShrMemFree
 #define shrFreeMemory                      nvShrMemFree
 #define shrMemSet                          memset
 #define shrMemCopy                         memcpy
+#endif
 
 // assert translations
 #define ASSERT_AND_GOTO(COND, LABEL)                                            \
@@ -199,12 +211,14 @@
 NV_STATUS srtMemAlloc(void **ppMem, NvU32 size);
 void srtMemFree(void *pMem);
 
+#ifndef SHR_USE_NVPORT_FUNCTIONS
 // function translations and/or renaming
 #define shrFreeMemory                      srtMemFree
 #define shrAllocMem                        srtMemAlloc
 #define shrFreeMem                         srtMemFree
 #define shrMemSet                          memset
 #define shrMemCopy                         memcpy
+#endif
 
 #elif defined(NV_MODS) && !defined(NVRM) && !defined(NV_RMAPI_TEGRA)
 
@@ -368,22 +382,13 @@
 
 // function translations and/or renaming
 #define shrBool                            BOOL
-#define shrFreeMemory                      osFreeMem
 #if !defined(MACOS) || !defined(KERNEL)
-#define shrAllocMem                        osAllocMemInternal
-#define shrAllocMemEx                      osAllocMemExInternal
-#define shrFreeMem                         osFreeMemInternal
 #define shrAllocPages                      osAllocPagesInternal
 #define shrFreePages                       osFreePagesInternal
 #else
-#define shrAllocMem                        osAllocMem
-#define shrAllocMemEx                      osAllocMemEx
-#define shrFreeMem                         osFreeMem
 #define shrAllocPages                      osAllocPages
 #define shrFreePages                       osFreePages
 #endif
-#define shrMemSet                          osMemSet
-#define shrMemCopy                         osMemCopy
 #define shrMapSystemMemory                 osMapSystemMemory
 #define shrUnmapSystemMemory               osUnmapSystemMemory
 #define shrGetCurrentProcess               osGetCurrentProcess
@@ -472,6 +477,32 @@
     return count;
 }
 
+#if defined(SHR_USE_NVPORT_FUNCTIONS)
+/// @todo bug 1583359 - spatch replace all calls to use NvPort directly.
+static NV_FORCEINLINE NV_STATUS
+shrAllocMem(void **ppMem, NvU32 size)
+{
+    *ppMem = portMemAllocNonPaged(size);
+    return (*ppMem == NULL) ? NV_ERR_NO_MEMORY : NV_OK;
+}
+void portMemFree(void *p);
+static NV_FORCEINLINE void
+shrFreeMem(void *p)
+{
+    portMemFree(p);
+}
+static NV_FORCEINLINE void *
+shrMemSet(void *dst, NvU8 c, NvU32 size)
+{
+    return portMemSet(dst, c, size);
+}
+static NV_FORCEINLINE void *
+shrMemCopy(void *dst, const void *src, NvU32 len)
+{
+    return portMemCopy(dst, len, src, len);
+}
+#endif
+
 #ifdef __cplusplus
 }
 #endif
diff -ur 367.57/kernel/nvidia-uvm/gmmu_fmt.c 375.20/kernel/nvidia-uvm/gmmu_fmt.c
--- 367.57/kernel/nvidia-uvm/gmmu_fmt.c	2016-10-04 05:42:26.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/gmmu_fmt.c	2016-11-16 02:48:59.000000000 +0300
@@ -107,14 +107,6 @@
         case GMMU_APERTURE_SYS_COH:
         case GMMU_APERTURE_SYS_NONCOH:
             return &pPte->fldAddrSysmem;
-
-        // @note: 
-        //        NVSWITCH masquerades a topology of GPUs as a single peer.
-        //        Mid size topologies will be around ~64 nodes with research
-        //        toplogies in the thousands.
-        //
-        //        Due to increase addressing pressure, there is a per-peer
-        //        alternate format in the PTE.
         case GMMU_APERTURE_PEER:
             return &pPte->fldAddrPeer;
         case GMMU_APERTURE_VIDEO:
@@ -135,7 +127,7 @@
           NvU8            *pEntries
 )
 {
-    NvU32                  i, compPageIndex;
+    NvU32                  i, compPageIndex, endCompPageIndex;
     NvU64                  offset           = surfOffset;
     const NvU32            pageSize         = NvU64_LO32(mmuFmtLevelPageSize(pLevel));
     const NV_FIELD_DESC32 *pCtlSubIndexFld  = &pFmt->pPte->fldCompTagSubIndex;
@@ -238,10 +230,13 @@
     //
     for (i = 0; i < numPages; ++i)
     {
+        // 2MB page require 0x20 comptags to be contiguous. so check for endPage limit too.
         compPageIndex = (NvU32)(offset >> pCompr->compPageShift);
-
+        endCompPageIndex = (NvU32)((offset + pageSize -1)>> pCompr->compPageShift);
         if ((compPageIndex >= pCompr->compPageIndexLo) &&
-            (compPageIndex <= pCompr->compPageIndexHi))
+            (compPageIndex <= pCompr->compPageIndexHi) &&
+            (endCompPageIndex >= pCompr->compPageIndexLo) &&
+            (endCompPageIndex <= pCompr->compPageIndexHi))
         {
             NvU8 *pPte        = pEntries + (i * pLevel->entrySize);
             NvU32 compTagLine = (compPageIndex - pCompr->compPageIndexLo) * pCompr->compTagLineMultiplier +
diff -ur 367.57/kernel/nvidia-uvm/hwref/pascal/gp100/dev_mmu.h 375.20/kernel/nvidia-uvm/hwref/pascal/gp100/dev_mmu.h
--- 367.57/kernel/nvidia-uvm/hwref/pascal/gp100/dev_mmu.h	2016-10-04 05:41:09.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/hwref/pascal/gp100/dev_mmu.h	2016-11-16 02:46:04.000000000 +0300
@@ -1,6 +1,6 @@
 /* _NVRM_COPYRIGHT_BEGIN_
  *
- * Copyright 2003-2014 by NVIDIA Corporation.  All rights reserved.  All
+ * Copyright 2003-2016 by NVIDIA Corporation.  All rights reserved.  All
  * information contained herein is proprietary and confidential to NVIDIA
  * Corporation.  Any use, reproduction, or disclosure without the written
  * permission of NVIDIA Corporation is prohibited.
diff -ur 367.57/kernel/nvidia-uvm/nv-kthread-q.c 375.20/kernel/nvidia-uvm/nv-kthread-q.c
--- 367.57/kernel/nvidia-uvm/nv-kthread-q.c	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/nv-kthread-q.c	2016-11-16 02:53:45.000000000 +0300
@@ -121,6 +121,10 @@
 
 void nv_kthread_q_stop(nv_kthread_q_t *q)
 {
+    // check if queue has been properly initialized
+    if (unlikely(!q->q_kthread))
+        return;
+
     nv_kthread_q_flush(q);
 
     // If this assertion fires, then a caller likely either broke the API rules,
@@ -150,8 +154,15 @@
     sema_init(&q->q_sem, 0);
 
     q->q_kthread = kthread_run(_main_loop, q, q_name);
-    if (IS_ERR(q->q_kthread))
-        return PTR_ERR(q->q_kthread);
+    if (IS_ERR(q->q_kthread)) {
+        int err = PTR_ERR(q->q_kthread);
+
+        // Clear q_kthread before returning so that nv_kthread_q_stop() can be
+        // safely called on it making error handling easier.
+        q->q_kthread = NULL;
+
+        return err;
+    }
 
     return 0;
 }
Только в 367.57/kernel/nvidia-uvm: nv-kthread-q.h
diff -ur 367.57/kernel/nvidia-uvm/nv-kthread-q-selftest.c 375.20/kernel/nvidia-uvm/nv-kthread-q-selftest.c
--- 367.57/kernel/nvidia-uvm/nv-kthread-q-selftest.c	2016-10-04 05:37:52.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/nv-kthread-q-selftest.c	2016-11-16 02:53:45.000000000 +0300
@@ -100,11 +100,19 @@
     basic_start_stop_args_t start_stop_args[NUM_Q_ITEMS_IN_BASIC_TEST];
     nv_kthread_q_t local_q;
 
+    // Do a redudant stop to ensure stop is supported on zero initialized memory
+    // No crash should occur
+    memset(&local_q, 0, sizeof(nv_kthread_q_t));
+    nv_kthread_q_stop(&local_q);
+
     // Do a quick start-stop cycle first:
     result = nv_kthread_q_init(&local_q, "q_to_stop");
     TEST_CHECK_RET(result == 0);
     nv_kthread_q_stop(&local_q);
 
+    // call another q_stop and it shouldn't crash and should return fine
+    nv_kthread_q_stop(&local_q);
+
     memset(&start_stop_args, 0, sizeof(start_stop_args));
     memset(callback_values_written, 0, sizeof(callback_values_written));
 
@@ -314,6 +322,9 @@
             resched_args->test_failure = 1;
         }
     }
+
+    // Ensure thread relinquishes control else we hang in single-core environments
+    schedule();
 }
 
 // Verify that re-scheduling the same q_item, from within its own
diff -ur 367.57/kernel/nvidia-uvm/uvm8_ce_test.c 375.20/kernel/nvidia-uvm/uvm8_ce_test.c
--- 367.57/kernel/nvidia-uvm/uvm8_ce_test.c	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_ce_test.c	2016-11-16 02:53:31.000000000 +0300
@@ -378,6 +378,9 @@
     status = uvm_mem_map_cpu(sys_phys_mem, NULL);
     if (status != NV_OK)
         goto done;
+    status = uvm_mem_map_gpu_phys(sys_phys_mem, gpu);
+    if (status != NV_OK)
+        goto done;
     memset(uvm_mem_get_cpu_addr_kernel(sys_phys_mem), 0, MEM_TEST_SIZE);
     test_mem[0] = uvm_mem_gpu_address_physical(sys_phys_mem, gpu, 0, MEM_TEST_SIZE);
 
diff -ur 367.57/kernel/nvidia-uvm/uvm8_forward_decl.h 375.20/kernel/nvidia-uvm/uvm8_forward_decl.h
--- 367.57/kernel/nvidia-uvm/uvm8_forward_decl.h	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_forward_decl.h	2016-11-16 02:53:31.000000000 +0300
@@ -49,6 +49,7 @@
 typedef struct uvm_va_range_struct uvm_va_range_t;
 typedef struct uvm_va_block_struct uvm_va_block_t;
 typedef struct uvm_va_space_struct uvm_va_space_t;
+typedef struct uvm_va_space_next_data_struct uvm_va_space_next_data_t;
 
 typedef struct uvm_gpu_va_space_struct uvm_gpu_va_space_t;
 
diff -ur 367.57/kernel/nvidia-uvm/uvm8_global.c 375.20/kernel/nvidia-uvm/uvm8_global.c
--- 367.57/kernel/nvidia-uvm/uvm8_global.c	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_global.c	2016-11-16 02:53:31.000000000 +0300
@@ -67,7 +67,6 @@
 NV_STATUS uvm_global_init(void)
 {
     NV_STATUS status = NV_ERR_GENERIC;
-    int nv_kthread_q_status;
 
     status = uvm_kvmalloc_init();
     if (status != NV_OK) {
@@ -85,14 +84,12 @@
 
     uvm_spin_lock_irqsave_init(&g_uvm_global.gpu_table_lock, UVM_LOCK_ORDER_LEAF);
 
-    nv_kthread_q_status = nv_kthread_q_init(&g_uvm_global.global_q, "UVM global queue");
-    if (nv_kthread_q_status  != 0) {
-        UVM_ERR_PRINT("nv_kthread_q_init() failed: %d\n", nv_kthread_q_status);
+    status = errno_to_nv_status(nv_kthread_q_init(&g_uvm_global.global_q, "UVM global queue"));
+    if (status  != NV_OK) {
+        UVM_ERR_PRINT("nv_kthread_q_init() failed: %s\n", nvstatusToString(status));
         goto error;
     }
 
-    g_uvm_global.q_is_initialized = true;
-
     status = uvm_procfs_init();
     if (status != NV_OK) {
         UVM_ERR_PRINT("uvm_procfs_init() failed: %s\n", nvstatusToString(status));
@@ -182,8 +179,7 @@
 
     uvm_procfs_exit();
 
-    if (&g_uvm_global.q_is_initialized)
-        nv_kthread_q_stop(&g_uvm_global.global_q);
+    nv_kthread_q_stop(&g_uvm_global.global_q);
 
     uvm_thread_context_exit();
     uvm_kvmalloc_exit();
diff -ur 367.57/kernel/nvidia-uvm/uvm8_global.h 375.20/kernel/nvidia-uvm/uvm8_global.h
--- 367.57/kernel/nvidia-uvm/uvm8_global.h	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_global.h	2016-11-16 02:53:31.000000000 +0300
@@ -82,7 +82,6 @@
 
     // A single queue for deferred work that is non-GPU-specific.
     nv_kthread_q_t global_q;
-    bool q_is_initialized;
 } uvm_global_t;
 
 extern uvm_global_t g_uvm_global;
@@ -116,10 +115,20 @@
 
 static uvm_gpu_t *uvm_processor_mask_find_first_gpu(const uvm_processor_mask_t *mask)
 {
+    uvm_gpu_t *gpu;
     uvm_gpu_id_t gpu_id = uvm_processor_mask_find_next_id(mask, UVM_CPU_ID + 1);
     if (gpu_id == UVM8_MAX_PROCESSORS)
         return NULL;
-    return uvm_gpu_get(gpu_id);
+
+    gpu = uvm_gpu_get(gpu_id);
+
+    // If there is valid GPU id in the mask, assert that the corresponding
+    // uvm_gpu_t is present. Otherwise it would stop a for_each_gpu_in_mask()
+    // loop pre-maturely. Today, this could only happen in remove_gpu() because
+    // the GPU being removed is deleted from the global table very early.
+    UVM_ASSERT_MSG(gpu, "gpu_id %u\n", gpu_id);
+
+    return gpu;
 }
 
 static uvm_gpu_t *uvm_processor_mask_find_next_gpu(const uvm_processor_mask_t *mask, uvm_gpu_t *gpu)
@@ -131,11 +140,17 @@
     gpu_id = uvm_processor_mask_find_next_id(mask, gpu->id + 1);
     if (gpu_id == UVM8_MAX_PROCESSORS)
         return NULL;
-    return uvm_gpu_get(gpu_id);
+
+    gpu = uvm_gpu_get(gpu_id);
+
+    // See comment in uvm_processor_mask_find_first_gpu().
+    UVM_ASSERT_MSG(gpu, "gpu_id %u\n", gpu_id);
+
+    return gpu;
 }
 
 // Helper to iterate over all GPUs (uvm_gpu_t) set in a mask. DO NOT USE if the current GPU
-// GPUs can be destroyed within the loop
+// GPUs can be destroyed within the loop or have been already removed from the GPU table.
 #define for_each_gpu_in_mask(gpu, mask)                                 \
     for (gpu = uvm_processor_mask_find_first_gpu(mask);                 \
          gpu != NULL;                                                   \
diff -ur 367.57/kernel/nvidia-uvm/uvm8_gpu.c 375.20/kernel/nvidia-uvm/uvm8_gpu.c
--- 367.57/kernel/nvidia-uvm/uvm8_gpu.c	2016-11-14 23:30:21.833926471 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_gpu.c	2016-11-16 02:53:31.000000000 +0300
@@ -595,15 +595,15 @@
 
         nv_kthread_q_item_init(&gpu->bottom_half_q_item, uvm8_isr_bottom_half, gpu);
 
+        gpu->handling_replayable_faults = true;
+
         // This causes a (severely) truncated version of the gpu->name to show up as the
         // name of a kthread, as seen via the ps(1) utility. Example: [ID 1: GeForce G]
-        status = nv_kthread_q_init(&gpu->bottom_half_q, gpu->name);
+        status = errno_to_nv_status(nv_kthread_q_init(&gpu->bottom_half_q, gpu->name));
         if (status != NV_OK) {
-            UVM_ERR_PRINT("Failed in nv_kthread_q_init_and_run: %s, GPU %s\n", nvstatusToString(status), gpu->name);
+            UVM_ERR_PRINT("Failed in nv_kthread_q_init: %s, GPU %s\n", nvstatusToString(status), gpu->name);
             goto error;
         }
-
-        gpu->handling_replayable_faults = true;
     }
 
     // Handle any future chip or future release items:
@@ -780,6 +780,8 @@
     if (gpu->rm_address_space != 0)
         uvm_rm_locked_call_void(nvUvmInterfaceAddressSpaceDestroy(gpu->rm_address_space));
 
+    UVM_ASSERT(atomic64_read(&gpu->mapped_cpu_pages_size) == 0);
+
     // After calling nvUvmInterfaceUnregisterGpu() the reference to pci_dev may
     // not be valid any more so clear it ahead of time.
     gpu->pci_dev = NULL;
@@ -1444,6 +1446,46 @@
     }
 }
 
+NV_STATUS uvm_gpu_map_cpu_pages(uvm_gpu_t *gpu, struct page *page, size_t size, NvU64 *dma_addr_out)
+{
+    NvU64 dma_addr = pci_map_page(gpu->pci_dev, page, 0, size, PCI_DMA_BIDIRECTIONAL);
+
+    UVM_ASSERT(PAGE_ALIGNED(size));
+
+    if (NV_PCI_DMA_MAPPING_ERROR(gpu->pci_dev, dma_addr))
+        return NV_ERR_OPERATING_SYSTEM;
+
+    if (dma_addr < gpu->dma_addressable_start || dma_addr + size - 1 > gpu->dma_addressable_limit) {
+        pci_unmap_page(gpu->pci_dev, dma_addr, size, PCI_DMA_BIDIRECTIONAL);
+        UVM_ERR_PRINT_RL("PCI mapped range [0x%llx, 0x%llx) not in the addressable range [0x%llx, 0x%llx), GPU %s\n",
+                         dma_addr,
+                         dma_addr + (NvU64)size,
+                         gpu->dma_addressable_start,
+                         gpu->dma_addressable_limit + 1,
+                         gpu->name);
+        return NV_ERR_INVALID_ADDRESS;
+    }
+
+    atomic64_add(size, &gpu->mapped_cpu_pages_size);
+
+    // The GPU has its NV_PFB_XV_UPPER_ADDR register set by RM to
+    // dma_addressable_start (in bifSetupDmaWindow_IMPL()) and hence when
+    // referencing sysmem from the GPU, dma_addressable_start should be
+    // subtracted from the DMA address we get from pci_map_page().
+    *dma_addr_out = dma_addr - gpu->dma_addressable_start;
+
+    return NV_OK;
+}
+
+void uvm_gpu_unmap_cpu_pages(uvm_gpu_t *gpu, NvU64 dma_address, size_t size)
+{
+    UVM_ASSERT(PAGE_ALIGNED(size));
+
+    dma_address += gpu->dma_addressable_start;
+    pci_unmap_page(gpu->pci_dev, dma_address, size, PCI_DMA_BIDIRECTIONAL);
+    atomic64_sub(size, &gpu->mapped_cpu_pages_size);
+}
+
 // This function implements the UvmRegisterGpu API call, as described in uvm.h. Notes:
 //
 // 1. The UVM VA space has a 1-to-1 relationship with an open instance of /dev/nvidia-uvm. That, in turn, has a 1-to-1
diff -ur 367.57/kernel/nvidia-uvm/uvm8_gpu.h 375.20/kernel/nvidia-uvm/uvm8_gpu.h
--- 367.57/kernel/nvidia-uvm/uvm8_gpu.h	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_gpu.h	2016-11-16 02:53:31.000000000 +0300
@@ -331,9 +331,19 @@
     struct pci_dev *pci_dev;
 
     // The physical address range addressable by the GPU
+    //
+    // The GPU has its NV_PFB_XV_UPPER_ADDR register set by RM to
+    // dma_addressable_start (in bifSetupDmaWindow_IMPL()) and hence when
+    // referencing sysmem from the GPU, dma_addressable_start should be
+    // subtracted from the physical address. uvm_gpu_map_cpu_pages() takes care
+    // of that.
     NvU64 dma_addressable_start;
     NvU64 dma_addressable_limit;
 
+    // Total size (in bytes) of physically mapped (with uvm_gpu_map_cpu_pages)
+    // sysmem pages, used for leak detection.
+    atomic64_t mapped_cpu_pages_size;
+
     // Should be UVM_GPU_MAGIC_VALUE. Used for memory checking.
     NvU64 magic;
 
@@ -713,6 +723,27 @@
 // it's required to call uvm_gpu_check_ecc_error() to be sure.
 NV_STATUS uvm_gpu_check_ecc_error_no_rm(uvm_gpu_t *gpu);
 
+// Map size bytes of contiguous sysmem on the GPU for physical access
+//
+// size has to be aligned to PAGE_SIZE.
+//
+// Returns the physical address of the pages that can be used to access them on
+// the GPU.
+NV_STATUS uvm_gpu_map_cpu_pages(uvm_gpu_t *gpu, struct page *page, size_t size, NvU64 *dma_address_out);
+
+// Unmap num_pages pages previously mapped with uvm_gpu_map_cpu_pages().
+void uvm_gpu_unmap_cpu_pages(uvm_gpu_t *gpu, NvU64 dma_address, size_t size);
+
+static NV_STATUS uvm_gpu_map_cpu_page(uvm_gpu_t *gpu, struct page *page, NvU64 *dma_address_out)
+{
+    return uvm_gpu_map_cpu_pages(gpu, page, PAGE_SIZE, dma_address_out);
+}
+
+static void uvm_gpu_unmap_cpu_page(uvm_gpu_t *gpu, NvU64 dma_address)
+{
+    uvm_gpu_unmap_cpu_pages(gpu, dma_address, PAGE_SIZE);
+}
+
 static bool uvm_gpu_is_gk110_plus(uvm_gpu_t *gpu)
 {
     return gpu->architecture >= NV2080_CTRL_MC_ARCH_INFO_ARCHITECTURE_GK110;
diff -ur 367.57/kernel/nvidia-uvm/uvm8_gpu_page_fault.c 375.20/kernel/nvidia-uvm/uvm8_gpu_page_fault.c
--- 367.57/kernel/nvidia-uvm/uvm8_gpu_page_fault.c	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_gpu_page_fault.c	2016-11-16 02:53:31.000000000 +0300
@@ -1080,11 +1080,13 @@
     uvm_va_space_t *va_space = NULL;
     uvm_replayable_fault_buffer_info_t *replayable_faults = &gpu->fault_buffer_info.replayable;
     uvm_fault_buffer_entry_t **ordered_fault_cache = replayable_faults->ordered_fault_cache;
+    bool va_block_supported = false;
 
     UVM_ASSERT(uvm_gpu_supports_replayable_faults(gpu));
 
     for (i = 0; i < batch_context->cached_faults;) {
         uvm_va_block_t *va_block;
+        NvU32 block_faults;
         uvm_fault_buffer_entry_t *current_entry = ordered_fault_cache[i];
         uvm_fault_utlb_info_t *utlb = &replayable_faults->utlbs[current_entry->fault_source.utlb_id];
 
@@ -1137,66 +1139,82 @@
             continue;
         }
 
-        status = uvm_va_block_find_create(current_entry->va_space, current_entry->fault_address, &va_block);
-        if (status == NV_OK) {
-            NvU32 block_faults;
-
-            status = service_fault_batch_block(gpu, va_block, i, batch_context, &block_faults);
+        // For next GPUs call next fault servicing routine. It will return NV_ERR_NOT_SUPPORTED
+        // if called on unsupported platforms.
+        status = uvm_next_service_fault_entry(gpu, i, ordered_fault_cache, batch_context);
 
-            // When service_fault_batch_block returns != NV_OK something really bad happened
-            if (status != NV_OK)
-                goto fail;
+        if (status == NV_OK) {
+            va_block_supported = false;
+        }
+        else if (status == NV_ERR_NOT_SUPPORTED) {
+            status = uvm_va_block_find_create(current_entry->va_space, current_entry->fault_address, &va_block);
+            if (status == NV_OK) {
+                va_block_supported = true;
+                status = service_fault_batch_block(gpu, va_block, i, batch_context, &block_faults);
 
-            // Don't issue replays in cancel mode
-            if (service_mode != FAULT_SERVICE_MODE_CANCEL &&
-                replayable_faults->replay_policy == UVM_PERF_FAULT_REPLAY_POLICY_BLOCK) {
-                status = push_replay_on_gpu(gpu, UVM_FAULT_REPLAY_TYPE_START, batch_context);
+                // When service_fault_batch_block returns != NV_OK something really bad happened
                 if (status != NV_OK)
                     goto fail;
-
-                // Increment the batch id if UVM_PERF_FAULT_REPLAY_POLICY_BLOCK
-                // is used, as we issue a replay after servicing each VA block
-                // and we can service a number of VA blocks before returning.
-                ++batch_context->batch_id;
             }
+            else  {
+                // Avoid dropping fault events when the VA block is not found or cannot be created
+                uvm_perf_event_data_t event_data;
+
+                event_data.fault.block = NULL;
+                event_data.fault.space = va_space;
+                event_data.fault.proc_id = gpu->id;
+                event_data.fault.gpu.buffer_entry = current_entry;
+
+                uvm_perf_event_notify(&va_space->perf_events, UVM_PERF_EVENT_FAULT, &event_data);
+
+                UVM_ASSERT(utlb->num_pending_faults > 0);
+                --utlb->num_pending_faults;
+
+                if (status == NV_ERR_INVALID_ADDRESS) {
+                    // If the VA block cannot be found, set the fatal fault flag, unless it is a prefetch fault
+                    if (current_entry->fault_access_type == UVM_FAULT_ACCESS_TYPE_PREFETCH) {
+                        ++batch_context->invalid_prefetch_faults;
+                    }
+                    else {
+                        current_entry->is_fatal = true;
+                        current_entry->fatal_reason = uvm_tools_status_to_fatal_fault_reason(status);
+                        ++batch_context->fatal_faults;
+                        ++utlb->num_fatal_faults;
+                    }
 
-            i += block_faults;
-        }
-        else {
-            // Avoid dropping fault events when the VA block is not found or cannot be created
-            uvm_perf_event_data_t event_data;
-
-            event_data.fault.block = NULL;
-            event_data.fault.space = va_space;
-            event_data.fault.proc_id = gpu->id;
-            event_data.fault.gpu.buffer_entry = current_entry;
-
-            uvm_perf_event_notify(&va_space->perf_events, UVM_PERF_EVENT_FAULT, &event_data);
-
-            UVM_ASSERT(utlb->num_pending_faults > 0);
-            --utlb->num_pending_faults;
-
-            if (status == NV_ERR_INVALID_ADDRESS) {
-                // If the VA block cannot be found, set the fatal fault flag, unless it is a prefetch fault
-                if (current_entry->fault_access_type == UVM_FAULT_ACCESS_TYPE_PREFETCH) {
-                    ++batch_context->invalid_prefetch_faults;
+                    // Do not exit early due to logical errors
+                    status = NV_OK;
                 }
-                else {
-                    current_entry->is_fatal = true;
-                    current_entry->fatal_reason = uvm_tools_status_to_fatal_fault_reason(status);
-                    ++batch_context->fatal_faults;
-                    ++utlb->num_fatal_faults;
+                else if (status != NV_OK) {
+                    goto fail;
                 }
 
-                // Do not exit early due to logical errors
-                status = NV_OK;
+                ++i;
+                continue;
             }
-            else if (status != NV_OK) {
+        }
+        else {
+            // Fault servicing failure in next mode, exit
+            goto fail;
+        }
+
+        // Don't issue replays in cancel mode
+        if (service_mode != FAULT_SERVICE_MODE_CANCEL &&
+            replayable_faults->replay_policy == UVM_PERF_FAULT_REPLAY_POLICY_BLOCK) {
+            status = push_replay_on_gpu(gpu, UVM_FAULT_REPLAY_TYPE_START, batch_context);
+            if (status != NV_OK)
                 goto fail;
-            }
 
-            ++i;
+            // Increment the batch id if UVM_PERF_FAULT_REPLAY_POLICY_BLOCK
+            // is used, as we issue a replay after servicing each VA block
+            // and we can service a number of VA blocks before returning.
+            ++batch_context->batch_id;
         }
+
+        if (va_block_supported)
+            i += block_faults;
+        else
+            ++i;
     }
 
 fail:
@@ -1611,11 +1629,7 @@
         else if (status != NV_OK)
             break;
 
-        // If UVM_NEXT is servicing the fault buffer, it will return something *other*
-        // than NV_ERR_NOT_SUPPORTED.
-        status = uvm_next_service_fault_batch(gpu, batch_context);
-        if (status == NV_ERR_NOT_SUPPORTED)
-            status = service_fault_batch(gpu, FAULT_SERVICE_MODE_REGULAR, batch_context);
+        status = service_fault_batch(gpu, FAULT_SERVICE_MODE_REGULAR, batch_context);
 
         // We may have issued replays even if status != NV_OK if
         // UVM_PERF_FAULT_REPLAY_POLICY_BLOCK is being used or the fault buffer
diff -ur 367.57/kernel/nvidia-uvm/uvm8_mem.c 375.20/kernel/nvidia-uvm/uvm8_mem.c
--- 367.57/kernel/nvidia-uvm/uvm8_mem.c	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_mem.c	2016-11-16 02:53:31.000000000 +0300
@@ -475,6 +475,55 @@
     uvm_processor_mask_clear(&mem->mapped_on, UVM_CPU_ID);
 }
 
+static void unmap_gpu_sysmem_iommu(uvm_mem_t *mem, uvm_gpu_t *gpu)
+{
+    NvU64 *dma_addrs = mem->sysmem.dma_addrs[gpu->id - 1];
+    NvU32 i;
+
+    UVM_ASSERT(uvm_mem_is_sysmem(mem));
+    UVM_ASSERT(dma_addrs);
+
+    for (i = 0; i < mem->chunks_count; ++i) {
+        if (dma_addrs[i] == 0) {
+            // The DMA address can only be 0 when cleaning up after a failed
+            // partial map_gpu_sysmem_iommu() operation.
+            break;
+        }
+        uvm_gpu_unmap_cpu_pages(gpu, dma_addrs[i], mem->chunk_size);
+        dma_addrs[i] = 0;
+    }
+
+    uvm_kvfree(dma_addrs);
+    mem->sysmem.dma_addrs[gpu->id - 1] = NULL;
+}
+
+static NV_STATUS map_gpu_sysmem_iommu(uvm_mem_t *mem, uvm_gpu_t *gpu)
+{
+    NV_STATUS status;
+    NvU64 *dma_addrs;
+    size_t i;
+
+    UVM_ASSERT(uvm_mem_is_sysmem(mem));
+
+    dma_addrs = uvm_kvmalloc_zero(sizeof(*dma_addrs) * mem->chunks_count);
+    if (!dma_addrs)
+        return NV_ERR_NO_MEMORY;
+
+    mem->sysmem.dma_addrs[gpu->id - 1] = dma_addrs;
+
+    for (i = 0; i < mem->chunks_count; ++i) {
+        status = uvm_gpu_map_cpu_pages(gpu, mem->sysmem.pages[i], mem->chunk_size, &dma_addrs[i]);
+        if (status != NV_OK)
+            goto error;
+    }
+
+    return NV_OK;
+
+error:
+    unmap_gpu_sysmem_iommu(mem, gpu);
+    return status;
+}
+
 static uvm_gpu_phys_address_t uvm_mem_gpu_physical_vidmem(uvm_mem_t *mem, size_t offset)
 {
     size_t chunk_index = uvm_div_pow2_64(offset, mem->chunk_size);
@@ -485,13 +534,15 @@
     return uvm_gpu_phys_address(UVM_APERTURE_VID, mem->vidmem.chunks[chunk_index]->address + chunk_offset);
 }
 
-static uvm_gpu_phys_address_t uvm_mem_gpu_physical_sysmem(uvm_mem_t *mem, size_t offset)
+static uvm_gpu_phys_address_t uvm_mem_gpu_physical_sysmem(uvm_mem_t *mem, uvm_gpu_t *gpu, size_t offset)
 {
-    struct page *page = uvm_mem_cpu_page(mem, UVM_PAGE_ALIGN_DOWN(offset));
+    NvU64 *dma_addrs = mem->sysmem.dma_addrs[gpu->id - 1];
+    NvU64 dma_addr = dma_addrs[offset / mem->chunk_size];
 
     UVM_ASSERT(uvm_mem_is_sysmem(mem));
+    UVM_ASSERT(uvm_processor_mask_test(&mem->mapped_phys_on, gpu->id));
 
-    return uvm_gpu_phys_address(UVM_APERTURE_SYS, page_to_phys(page) + offset % PAGE_SIZE);
+    return uvm_gpu_phys_address(UVM_APERTURE_SYS, dma_addr + offset % mem->chunk_size);
 }
 
 uvm_gpu_phys_address_t uvm_mem_gpu_physical(uvm_mem_t *mem, uvm_gpu_t *gpu, NvU64 offset, NvU64 size)
@@ -504,7 +555,7 @@
     if (uvm_mem_is_vidmem(mem))
         return uvm_mem_gpu_physical_vidmem(mem, offset);
     else
-        return uvm_mem_gpu_physical_sysmem(mem, offset);
+        return uvm_mem_gpu_physical_sysmem(mem, gpu, offset);
 }
 
 typedef struct uvm_mem_pte_maker_data_struct
@@ -590,6 +641,10 @@
     if (uvm_processor_mask_test(&mem->mapped_on, gpu->id))
         return NV_OK;
 
+    status = uvm_mem_map_gpu_phys(mem, gpu);
+    if (status != NV_OK)
+        return status;
+
     gpu_va = reserved_gpu_va(mem, gpu);
     status = map_gpu(mem, gpu, gpu_va, &gpu->address_space_tree, &attrs);
     if (status != NV_OK)
@@ -610,6 +665,10 @@
     if (uvm_processor_mask_test(&mem->mapped_on, gpu->id))
         return NV_OK;
 
+    status = uvm_mem_map_gpu_phys(mem, gpu);
+    if (status != NV_OK)
+        return status;
+
     uvm_assert_rwsem_locked(&mem->user.va_space->lock);
     gpu_va_space = uvm_gpu_va_space_get(mem->user.va_space, gpu);
     status = map_gpu(mem, gpu, (NvU64)mem->user.addr, &gpu_va_space->page_tables, attrs);
@@ -635,6 +694,46 @@
     uvm_processor_mask_clear(&mem->mapped_on, gpu->id);
 }
 
+NV_STATUS uvm_mem_map_gpu_phys(uvm_mem_t *mem, uvm_gpu_t *gpu)
+{
+    NV_STATUS status;
+
+    if (!uvm_mem_is_sysmem(mem))
+        return NV_OK;
+
+    if (uvm_processor_mask_test(&mem->mapped_phys_on, gpu->id)) {
+        // Already mapped
+        return NV_OK;
+    }
+
+    status = map_gpu_sysmem_iommu(mem, gpu);
+    if (status != NV_OK)
+        return status;
+
+    uvm_processor_mask_set(&mem->mapped_phys_on, gpu->id);
+
+    return NV_OK;
+}
+
+void uvm_mem_unmap_gpu_phys(uvm_mem_t *mem, uvm_gpu_t *gpu)
+{
+    UVM_ASSERT(mem);
+    UVM_ASSERT(gpu);
+
+    if (!uvm_mem_is_sysmem(mem))
+        return;
+
+    uvm_mem_unmap_gpu(mem, gpu);
+
+    if (!uvm_processor_mask_test(&mem->mapped_phys_on, gpu->id)) {
+        // Already unmapped
+        return;
+    }
+
+    unmap_gpu_sysmem_iommu(mem, gpu);
+    uvm_processor_mask_clear(&mem->mapped_phys_on, gpu->id);
+}
+
 void uvm_mem_free(uvm_mem_t *mem)
 {
     uvm_gpu_t *gpu;
@@ -648,6 +747,9 @@
     for_each_gpu_in_mask(gpu, &mem->mapped_on)
         uvm_mem_unmap_gpu(mem, gpu);
 
+    for_each_gpu_in_mask(gpu, &mem->mapped_phys_on)
+        uvm_mem_unmap_gpu_phys(mem, gpu);
+
     if (!mem->is_user_allocation && mem->kernel.range_alloc.node)
         uvm_range_allocator_free(&g_free_ranges, &mem->kernel.range_alloc);
 
diff -ur 367.57/kernel/nvidia-uvm/uvm8_mem.h 375.20/kernel/nvidia-uvm/uvm8_mem.h
--- 367.57/kernel/nvidia-uvm/uvm8_mem.h	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_mem.h	2016-11-16 02:53:31.000000000 +0300
@@ -65,7 +65,6 @@
 //  - Per processor caching attributes (longer term, the envisioned use-case is
 //    for GPU semaphore caching, which requires the first limitation below to be
 //    lifted)
-//  - TODO: Bug 1765195: IOMMU support (whenever it's added elsewhere)
 //
 // Limitations:
 //  - On Pascal+ limited to VAs over 40bit due to how the internal VA is shared
@@ -131,13 +130,19 @@
     // Size of the physical chunks.
     NvU32 chunk_size;
 
-    union {
-        struct {
+    union
+    {
+        struct
+        {
             uvm_gpu_chunk_t **chunks;
         } vidmem;
 
-        struct {
+        struct
+        {
             struct page **pages;
+
+            // Per GPU IOMMU mappings of the pages
+            NvU64 *dma_addrs[UVM8_MAX_GPUS];
         } sysmem;
     };
 
@@ -155,6 +160,7 @@
     bool is_user_allocation;
 
     // Mask of processors the memory is mapped on
+    uvm_processor_mask_t mapped_phys_on;
     uvm_processor_mask_t mapped_on;
 
     // Page table ranges for all GPUs
@@ -244,6 +250,12 @@
 NV_STATUS uvm_mem_map_cpu(uvm_mem_t *mem, struct vm_area_struct *vma);
 void uvm_mem_unmap_cpu(uvm_mem_t *mem);
 
+// Map/Unmap sysmem for physical access on a GPU
+//
+//
+NV_STATUS uvm_mem_map_gpu_phys(uvm_mem_t *mem, uvm_gpu_t *gpu);
+void uvm_mem_unmap_gpu_phys(uvm_mem_t *mem, uvm_gpu_t *gpu);
+
 // Map/Unmap on a GPU
 //
 // Mapping is supported for:
diff -ur 367.57/kernel/nvidia-uvm/uvm8_migrate.c 375.20/kernel/nvidia-uvm/uvm8_migrate.c
--- 367.57/kernel/nvidia-uvm/uvm8_migrate.c	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_migrate.c	2016-11-16 02:53:31.000000000 +0300
@@ -63,6 +63,7 @@
                                                            dest_id,
                                                            region,
                                                            NULL,
+                                                           NULL,
                                                            UvmEventMigrationCauseUser);
     }
     else {
@@ -72,6 +73,7 @@
                                             dest_id,
                                             region,
                                             NULL,
+                                            NULL,
                                             UvmEventMigrationCauseUser);
     }
     if (status != NV_OK)
@@ -168,28 +170,24 @@
     return NV_OK;
 }
 
-static NV_STATUS uvm_migrate(uvm_va_space_t *va_space,
-                             NvU64 base,
-                             NvU64 length,
-                             uvm_processor_id_t dest_id,
-                             NvU32 migrate_flags,
-                             uvm_tracker_t *out_tracker)
+static NV_STATUS uvm_migrate_ranges(uvm_va_space_t *va_space,
+                                    uvm_va_block_context_t *va_block_context,
+                                    uvm_va_range_t *first_va_range,
+                                    NvU64 base,
+                                    NvU64 length,
+                                    uvm_processor_id_t dest_id,
+                                    NvU32 migrate_flags,
+                                    uvm_tracker_t *out_tracker)
 {
     uvm_va_range_t *va_range, *va_range_last;
     NvU64 end = base + length - 1;
     NV_STATUS status = NV_OK;
     bool skipped_migrate = false;
-    uvm_va_block_context_t *va_block_context;
-
-    uvm_assert_mmap_sem_locked(&current->mm->mmap_sem);
-    uvm_assert_rwsem_locked(&va_space->lock);
 
-    va_block_context = uvm_va_block_context_alloc();
-    if (!va_block_context)
-        return NV_ERR_NO_MEMORY;
+    UVM_ASSERT(first_va_range == uvm_va_space_iter_first(va_space, base, base));
 
     va_range_last = NULL;
-    uvm_for_each_va_range_in_contig(va_range, va_space, base, end) {
+    uvm_for_each_va_range_in_contig_from(va_range, va_space, first_va_range, end) {
         uvm_range_group_range_iter_t iter;
         va_range_last = va_range;
 
@@ -234,7 +232,6 @@
         }
     }
 
-    uvm_va_block_context_free(va_block_context);
 
     if (status != NV_OK)
         return status;
@@ -249,6 +246,91 @@
     return NV_OK;
 }
 
+static bool needs_two_passes(uvm_va_range_t *first_va_range,
+                             NvU64 base,
+                             NvU64 length,
+                             uvm_processor_id_t dest_id,
+                             NvU32 migrate_flags)
+{
+    NvU64 end = base + length - 1;
+
+    if (dest_id != UVM_CPU_ID)
+        return false;
+
+    if (migrate_flags & UVM_MIGRATE_FLAG_SKIP_CPU_MAP)
+        return false;
+
+    if (end > first_va_range->node.end)
+        return true;
+
+    if (uvm_va_range_block_index(first_va_range, base) ==
+        uvm_va_range_block_index(first_va_range, end))
+        return false;
+
+    return true;
+}
+
+static NV_STATUS uvm_migrate(uvm_va_space_t *va_space,
+                             NvU64 base,
+                             NvU64 length,
+                             uvm_processor_id_t dest_id,
+                             NvU32 migrate_flags,
+                             uvm_tracker_t *out_tracker)
+{
+    NV_STATUS status = NV_OK;
+    uvm_va_range_t *first_va_range = uvm_va_space_iter_first(va_space, base, base);
+    uvm_va_block_context_t *va_block_context;
+    bool do_two_passes;
+
+    uvm_assert_mmap_sem_locked(&current->mm->mmap_sem);
+    uvm_assert_rwsem_locked(&va_space->lock);
+
+    if (!first_va_range || first_va_range->type != UVM_VA_RANGE_TYPE_MANAGED)
+        return NV_ERR_INVALID_ADDRESS;
+
+    va_block_context = uvm_va_block_context_alloc();
+    if (!va_block_context)
+        return NV_ERR_NO_MEMORY;
+
+    // For migrations to CPU, perform two passes (unless the migration only covers a
+    // single VA block in a single VA range). This is because adding CPU mappings is
+    // synchronous and delays the migration of the next VA blocks.
+    //
+    // 1- Transfer all VA blocks (do not add mappings)
+    // 2- Go block by block reexecuting the transfer (in case someone moved it since the
+    // first pass), and adding the CPU mappings.
+    do_two_passes = needs_two_passes(first_va_range, base, length, dest_id, migrate_flags);
+
+    status = uvm_migrate_ranges(va_space,
+                                va_block_context,
+                                first_va_range,
+                                base,
+                                length,
+                                dest_id,
+                                do_two_passes?
+                                    migrate_flags | UVM_MIGRATE_FLAG_SKIP_CPU_MAP:
+                                    migrate_flags,
+                                out_tracker);
+
+    if (status == NV_OK && do_two_passes) {
+        status = uvm_migrate_ranges(va_space,
+                                    va_block_context,
+                                    first_va_range,
+                                    base,
+                                    length,
+                                    dest_id,
+                                    migrate_flags,
+                                    out_tracker);
+    }
+
+    uvm_va_block_context_free(va_block_context);
+
+    if (status != NV_OK)
+        return status;
+
+    return NV_OK;
+}
+
 static NV_STATUS uvm_push_async_user_sem_release(uvm_gpu_t *release_from_gpu,
                                                  uvm_va_range_semaphore_pool_t *sema_va_range,
                                                  NvU64 sema_user_addr,
@@ -257,7 +339,13 @@
 {
     uvm_push_t push;
     NV_STATUS status;
-    uvm_gpu_address_t sema_phys_addr = uvm_mem_gpu_address_physical(sema_va_range->mem, release_from_gpu,
+    uvm_gpu_address_t sema_phys_addr;
+
+    status = uvm_mem_map_gpu_phys(sema_va_range->mem, release_from_gpu);
+    if (status != NV_OK)
+        return status;
+
+    sema_phys_addr = uvm_mem_gpu_address_physical(sema_va_range->mem, release_from_gpu,
             sema_user_addr - (NvU64)(uintptr_t)sema_va_range->mem->user.addr, 4);
 
     status = uvm_push_begin_acquire(release_from_gpu->channel_manager,
diff -ur 367.57/kernel/nvidia-uvm/uvm8_mmu.c 375.20/kernel/nvidia-uvm/uvm8_mmu.c
--- 367.57/kernel/nvidia-uvm/uvm8_mmu.c	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_mmu.c	2016-11-16 02:53:31.000000000 +0300
@@ -84,13 +84,24 @@
 
 static NV_STATUS phys_mem_allocate_sysmem(uvm_page_tree_t *tree, NvLength size, uvm_mmu_page_table_alloc_t *out)
 {
+    NV_STATUS status = NV_OK;
+    NvU64 dma_addr;
     out->handle.page = alloc_pages(NV_UVM_GFP_FLAGS | __GFP_ZERO, get_order(size));
     if (out->handle.page == NULL)
         return NV_ERR_NO_MEMORY;
 
-    // TODO: Bug 1765195: This address needs to be the physical address from the
-    // GPU's perspective which will be different when there's an IOMMU
-    out->addr = uvm_gpu_phys_address(UVM_APERTURE_SYS, page_to_phys(out->handle.page));
+    // Check for fake GPUs from the unit test
+    if (tree->gpu->pci_dev)
+        status = uvm_gpu_map_cpu_pages(tree->gpu, out->handle.page, UVM_PAGE_ALIGN_UP(size), &dma_addr);
+    else
+        dma_addr = page_to_phys(out->handle.page);
+
+    if (status != NV_OK) {
+        __free_pages(out->handle.page, get_order(size));
+        return status;
+    }
+
+    out->addr = uvm_gpu_phys_address(UVM_APERTURE_SYS, dma_addr);
     out->size = size;
 
     return NV_OK;
@@ -103,14 +114,8 @@
     uvm_tracker_t local_tracker = UVM_TRACKER_INIT();
 
     status = uvm_pmm_gpu_alloc_kernel(&gpu->pmm, 1, size, pmm_flags, &out->handle.chunk, &local_tracker);
-    if (status != NV_OK) {
-        if (pmm_flags & UVM_PMM_ALLOC_FLAGS_EVICT) {
-            UVM_DBG_PRINT("pmm_gpu_alloc_kernel(count=1, size=0x%x, flags=0x%x) failed: %s, GPU %s\n",
-                    (NvU32)size, pmm_flags, nvstatusToString(status), gpu->name);
-        }
-
+    if (status != NV_OK)
         return status;
-    }
 
     if (!uvm_tracker_is_empty(&local_tracker)) {
         uvm_mutex_lock(&tree->lock);
@@ -162,6 +167,8 @@
         UVM_ASSERT(status == uvm_global_get_status());
 
     UVM_ASSERT(ptr->addr.aperture == UVM_APERTURE_SYS);
+    if (tree->gpu->pci_dev)
+        uvm_gpu_unmap_cpu_pages(tree->gpu, ptr->addr.address, UVM_PAGE_ALIGN_UP(ptr->size));
     __free_pages(ptr->handle.page, get_order(ptr->size));
 }
 
@@ -246,6 +253,11 @@
         return NULL;
 
     status = phys_mem_allocate(tree, phys_alloc_size, location, pmm_flags, &dir->phys_alloc);
+    if (status == NV_ERR_NO_MEMORY && location == UVM_APERTURE_VID && (pmm_flags & UVM_PMM_ALLOC_FLAGS_EVICT) != 0) {
+        // Fall back to sysmem if allocating page tables in vidmem with eviction fails
+        status = phys_mem_allocate(tree, phys_alloc_size, UVM_APERTURE_SYS, pmm_flags, &dir->phys_alloc);
+    }
+
     if (status != NV_OK) {
         uvm_kvfree(dir);
         return NULL;
diff -ur 367.57/kernel/nvidia-uvm/uvm8_next_decl.h 375.20/kernel/nvidia-uvm/uvm8_next_decl.h
--- 367.57/kernel/nvidia-uvm/uvm8_next_decl.h	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_next_decl.h	2016-11-16 02:53:31.000000000 +0300
@@ -30,7 +30,10 @@
 
 NV_STATUS uvm_next_add_gpu(uvm_gpu_t *gpu);
 void uvm_next_remove_gpu(uvm_gpu_t *gpu);
-NV_STATUS uvm_next_service_fault_batch(uvm_gpu_t *gpu, uvm_fault_service_batch_context_t *batch_context);
+void uvm_next_va_space_init(uvm_va_space_t *va_space);
+NV_STATUS uvm_next_service_fault_entry(uvm_gpu_t *gpu, NvU32 entry_index,
+                                       uvm_fault_buffer_entry_t **ordered_fault_cache,
+                                       uvm_fault_service_batch_context_t *batch_context);
 bool uvm_next_is_hmm_compatible(void);
 NV_STATUS uvm_next_cancel_faults_precise(uvm_gpu_t *gpu);
 bool uvm_hal_fault_buffer_class_supports_next_faults(NvU32 fault_buffer_class);
@@ -51,7 +54,14 @@
     return NV_OK;
 }
 
-static NV_STATUS uvm_next_service_fault_batch(uvm_gpu_t *gpu, uvm_fault_service_batch_context_t *batch_context)
+static void uvm_next_va_space_init(uvm_va_space_t *va_space)
+{
+    return;
+}
+
+static NV_STATUS uvm_next_service_fault_entry(uvm_gpu_t *gpu, NvU32 entry_index,
+                                              uvm_fault_buffer_entry_t **ordered_fault_cache,
+                                              uvm_fault_service_batch_context_t *batch_context)
 {
     return NV_ERR_NOT_SUPPORTED;
 }
diff -ur 367.57/kernel/nvidia-uvm/uvm8_pascal_fault_buffer.h 375.20/kernel/nvidia-uvm/uvm8_pascal_fault_buffer.h
--- 367.57/kernel/nvidia-uvm/uvm8_pascal_fault_buffer.h	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_pascal_fault_buffer.h	2016-11-16 02:53:31.000000000 +0300
@@ -24,14 +24,13 @@
 #ifndef __UVM8_HAL_PASCAL_FAULT_BUFFER_H__
 #define __UVM8_HAL_PASCAL_FAULT_BUFFER_H__
 
-// When NV_CHIP_TPCARB_NUM_TEX_PORTS is defined to 2, TPCn has TPCCSn, PEn, TEXp, and TEXq, where p=2*n and q=p+1.
-// When NV_CHIP_TPCARB_NUM_TEX_PORTS is not defined or is defined to 1, TPCn has TPCCSn, PEn, TEXn.
+// There are up to 5 TPCs per GPC in Pascal, and there is 1 LTP uTLB per TPC. Besides, there is one RGG uTLB per GPC.
+// Each TPC has a number of clients that can make requests to its uTLB: 1xTPCCS, 1xPE, 2xT1. The client ids are local
+// to their GPC and the id mapping is linear across TPCs:
+// TPC_n has TPCCS_n, PE_n, T1_p, and T1_q, where p=2*n and q=p+1.
 //
-// NV_PFAULT_CLIENT_GPC_LTP_UTLB_n and NV_PFAULT_CLIENT_GPC_RGG_UTLB enums can be ignored. These will never be reported in a
-// fault message, and should never be used in an invalidate.
-//
-// There is 1 LTP uTLB per TPC. There are up to 5 TPCs per GPC. In Pascal 2 TEX/L1 units per TPC
-// (NV_CHIP_TPCARB_NUM_TEX_PORTS = 2)
+// NV_PFAULT_CLIENT_GPC_LTP_UTLB_n and NV_PFAULT_CLIENT_GPC_RGG_UTLB enums can be ignored. These will never be reported
+// in a fault message, and should never be used in an invalidate. Therefore, we define our own values.
 typedef enum {
     UVM_PASCAL_GPC_UTLB_ID_LTP0 = 0,
     UVM_PASCAL_GPC_UTLB_ID_LTP1 = 1,
diff -ur 367.57/kernel/nvidia-uvm/uvm8_perf_events.h 375.20/kernel/nvidia-uvm/uvm8_perf_events.h
--- 367.57/kernel/nvidia-uvm/uvm8_perf_events.h	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_perf_events.h	2016-11-16 02:53:31.000000000 +0300
@@ -74,9 +74,6 @@
     // Locking: uvm_va_space: at least in read mode, uvm_va_block: exclusive
     UVM_PERF_EVENT_FIRST_TOUCH,
 
-    // Locking: uvm_va_space: at least in read mode, uvm_va_block: exclusive
-    UVM_PERF_EVENT_BLOCK_MIGRATION_BEGIN,
-
     UVM_PERF_EVENT_COUNT,
 } uvm_perf_event_t;
 
diff -ur 367.57/kernel/nvidia-uvm/uvm8_perf_prefetch.c 375.20/kernel/nvidia-uvm/uvm8_perf_prefetch.c
--- 367.57/kernel/nvidia-uvm/uvm8_perf_prefetch.c	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_perf_prefetch.c	2016-11-16 02:53:31.000000000 +0300
@@ -72,8 +72,8 @@
 // Valid values 1-100
 static unsigned uvm_perf_prefetch_threshold  = UVM_PREFETCH_THRESHOLD_DEFAULT;
 
-#define UVM_PREFETCH_MIN_FAULTS_MIN     2
-#define UVM_PREFETCH_MIN_FAULTS_DEFAULT 3
+#define UVM_PREFETCH_MIN_FAULTS_MIN     1
+#define UVM_PREFETCH_MIN_FAULTS_DEFAULT 1
 #define UVM_PREFETCH_MIN_FAULTS_MAX     20
 
 // Minimum number of faults on a block in order to enable the prefetching
@@ -104,7 +104,8 @@
         uvm_va_block_region_t subregion = uvm_va_block_bitmap_tree_iter_get_range(bitmap_tree, &iter);
         NvU16 subregion_pages = uvm_va_block_region_num_pages(subregion);
 
-        if (counter < subregion_pages && counter * 100 > subregion_pages * g_uvm_perf_prefetch_threshold)
+        UVM_ASSERT(counter <= subregion_pages);
+        if (counter * 100 > subregion_pages * g_uvm_perf_prefetch_threshold)
             prefetch_region = subregion;
     }
 
@@ -184,37 +185,33 @@
     return NULL;
 }
 
-static void grow_fault_migrations_if_no_thrashing(block_prefetch_info_t *prefetch_info,
-                                                  uvm_va_block_region_t region,
-                                                  const long unsigned *faulted_pages,
-                                                  const long unsigned *thrashing_pages)
+static void grow_fault_granularity_if_no_thrashing(block_prefetch_info_t *prefetch_info,
+                                                   uvm_va_block_region_t region,
+                                                   const long unsigned *faulted_pages,
+                                                   const long unsigned *thrashing_pages)
 {
     if (!uvm_page_mask_region_empty(faulted_pages, region) &&
         (!thrashing_pages || uvm_page_mask_region_empty(thrashing_pages, region))) {
-        uvm_page_mask_region_fill(prefetch_info->migrate_pages, region);
+        region.first += prefetch_info->first_page;
+        region.outer += prefetch_info->first_page;
+        uvm_page_mask_region_fill(prefetch_info->bitmap_tree.pages, region);
     }
 }
 
-static void initialize_migration_mask(uvm_va_block_t *va_block,
-                                      block_prefetch_info_t *prefetch_info,
-                                      const long unsigned *faulted_pages,
-                                      const long unsigned *thrashing_pages)
+static void grow_fault_granularity(uvm_va_block_t *va_block,
+                                   block_prefetch_info_t *prefetch_info,
+                                   const long unsigned *faulted_pages,
+                                   const long unsigned *thrashing_pages)
 {
     size_t num_big_pages;
     size_t big_page_index;
     uvm_va_block_region_t block_region = uvm_va_block_region_from_block(va_block);
 
-    // Do not update the tree with faults on pages that are thrashing
-    if (thrashing_pages)
-        uvm_page_mask_andnot(prefetch_info->migrate_pages, faulted_pages, thrashing_pages);
-    else
-        uvm_page_mask_copy(prefetch_info->migrate_pages, faulted_pages);
-
     // Migrate whole "prefix" if no page in it is thrashing
     if (prefetch_info->big_pages_region.first > 0) {
         uvm_va_block_region_t prefix_region = uvm_va_block_region(0, prefetch_info->big_pages_region.first);
 
-        grow_fault_migrations_if_no_thrashing(prefetch_info, prefix_region, faulted_pages, thrashing_pages);
+        grow_fault_granularity_if_no_thrashing(prefetch_info, prefix_region, faulted_pages, thrashing_pages);
     }
 
     // Migrate whole big pages if they are not thrashing
@@ -224,7 +221,7 @@
                                                                         big_page_index,
                                                                         prefetch_info->big_page_size);
 
-        grow_fault_migrations_if_no_thrashing(prefetch_info, big_region, faulted_pages, thrashing_pages);
+        grow_fault_granularity_if_no_thrashing(prefetch_info, big_region, faulted_pages, thrashing_pages);
     }
 
     // Migrate whole "suffix" if no page in it is thrashing
@@ -232,7 +229,7 @@
         uvm_va_block_region_t suffix_region = uvm_va_block_region(prefetch_info->big_pages_region.outer,
                                                                   block_region.outer);
 
-        grow_fault_migrations_if_no_thrashing(prefetch_info, suffix_region, faulted_pages, thrashing_pages);
+        grow_fault_granularity_if_no_thrashing(prefetch_info, suffix_region, faulted_pages, thrashing_pages);
     }
 }
 
@@ -301,7 +298,15 @@
 
     thrashing_pages = uvm_perf_thrashing_get_thrashing_pages(va_block);
 
-    initialize_migration_mask(va_block, prefetch_info, faulted_pages, thrashing_pages);
+    // Assume big pages by default. Prefetch the rest of 4KB subregions within the big page
+    // region unless there is thrashing.
+    grow_fault_granularity(va_block, prefetch_info, faulted_pages, thrashing_pages);
+
+    // Do not compute prefetch regions with faults on pages that are thrashing
+    if (thrashing_pages)
+        uvm_page_mask_andnot(prefetch_info->migrate_pages, faulted_pages, thrashing_pages);
+    else
+        uvm_page_mask_copy(prefetch_info->migrate_pages, faulted_pages);
 
     // Update the tree using the migration mask to compute the pages to prefetch
     uvm_page_mask_zero(prefetch_info->prefetch_pages);
@@ -315,12 +320,6 @@
             break;
     }
 
-    // Do not prefetch pages that are going to be migrated due to a fault or are already resident in
-    // the destination processor
-    uvm_page_mask_andnot(prefetch_info->prefetch_pages,
-                         prefetch_info->prefetch_pages,
-                         prefetch_info->bitmap_tree.pages);
-
     // Adjust prefetching page mask
     if (prefetch_info->first_page > 0) {
         bitmap_shift_right(prefetch_info->prefetch_pages,
@@ -329,6 +328,17 @@
                            PAGES_PER_UVM_VA_BLOCK);
     }
 
+    // Do not prefetch pages that are going to be migrated due to a fault or are already resident in
+    // the destination processor
+    uvm_page_mask_andnot(prefetch_info->prefetch_pages,
+                         prefetch_info->prefetch_pages,
+                         faulted_pages);
+    if (resident_mask) {
+        uvm_page_mask_andnot(prefetch_info->prefetch_pages,
+                             prefetch_info->prefetch_pages,
+                             resident_mask);
+    }
+
     // Avoid prefetching pages that are thrashing
     if (thrashing_pages) {
         uvm_page_mask_andnot(prefetch_info->prefetch_pages,
diff -ur 367.57/kernel/nvidia-uvm/uvm8_policy.c 375.20/kernel/nvidia-uvm/uvm8_policy.c
--- 367.57/kernel/nvidia-uvm/uvm8_policy.c	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_policy.c	2016-11-16 02:53:31.000000000 +0300
@@ -363,6 +363,7 @@
                                                            src_id,
                                                            uvm_va_block_region_from_block(va_block),
                                                            resident_mask,
+                                                           NULL,
                                                            UvmEventMigrationCauseUser);
         if (status != NV_OK)
             return status;
@@ -420,6 +421,7 @@
                                                 va_range->preferred_location,
                                                 block_region,
                                                 va_block->read_duplicated_pages,
+                                                NULL,
                                                 UvmEventMigrationCauseUser);
             if (status != NV_OK)
                 return status;
@@ -448,6 +450,7 @@
                                             processor_id,
                                             block_region,
                                             break_read_duplication_pages,
+                                            NULL,
                                             UvmEventMigrationCauseUser);
         if (status != NV_OK)
             return status;
diff -ur 367.57/kernel/nvidia-uvm/uvm8_range_group.c 375.20/kernel/nvidia-uvm/uvm8_range_group.c
--- 367.57/kernel/nvidia-uvm/uvm8_range_group.c	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_range_group.c	2016-11-16 02:53:31.000000000 +0300
@@ -192,6 +192,7 @@
                                                            va_range->preferred_location,
                                                            region,
                                                            NULL,
+                                                           NULL,
                                                            UvmEventMigrationCauseCoherence);
     }
     else {
@@ -201,6 +202,7 @@
                                             va_range->preferred_location,
                                             region,
                                             NULL,
+                                            NULL,
                                             UvmEventMigrationCauseCoherence);
     }
     if (status != NV_OK)
diff -ur 367.57/kernel/nvidia-uvm/uvm8_rm_mem.c 375.20/kernel/nvidia-uvm/uvm8_rm_mem.c
--- 367.57/kernel/nvidia-uvm/uvm8_rm_mem.c	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_rm_mem.c	2016-11-16 02:53:31.000000000 +0300
@@ -169,7 +169,7 @@
 
 void uvm_rm_mem_free(uvm_rm_mem_t *rm_mem)
 {
-    uvm_gpu_t *gpu;
+    uvm_gpu_id_t gpu_id;
 
     if (rm_mem == NULL)
         return;
@@ -177,11 +177,19 @@
     if (uvm_processor_mask_test(&rm_mem->mapped_on, UVM_CPU_ID))
         uvm_rm_mem_unmap_cpu(rm_mem);
 
-    for_each_gpu_in_mask(gpu, &rm_mem->mapped_on)
-        if (gpu != rm_mem->gpu_owner)
-            uvm_rm_mem_unmap_gpu(rm_mem, gpu);
+    // Don't use for_each_gpu_in_mask() as the owning GPU might be being
+    // destroyed and already removed from the global GPU array causing the iteration
+    // to stop prematurely.
+    for_each_gpu_id_in_mask(gpu_id, &rm_mem->mapped_on)
+        if (gpu_id != rm_mem->gpu_owner->id)
+            uvm_rm_mem_unmap_gpu(rm_mem, uvm_gpu_get(gpu_id));
 
     uvm_rm_locked_call_void(nvUvmInterfaceMemoryFree(rm_mem->gpu_owner->rm_address_space, rm_mem->vas[rm_mem->gpu_owner->id]));
+    uvm_processor_mask_clear(&rm_mem->mapped_on, rm_mem->gpu_owner->id);
+    rm_mem->vas[rm_mem->gpu_owner->id] = 0;
+
+    UVM_ASSERT_MSG(uvm_processor_mask_get_count(&rm_mem->mapped_on) == 0,
+                   "Left-over %u mappings in rm_mem\n", uvm_processor_mask_get_count(&rm_mem->mapped_on));
 
     uvm_kvfree(rm_mem);
 }
diff -ur 367.57/kernel/nvidia-uvm/uvm8_tools.c 375.20/kernel/nvidia-uvm/uvm8_tools.c
--- 367.57/kernel/nvidia-uvm/uvm8_tools.c	2016-11-11 18:37:27.662397788 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_tools.c	2016-11-16 02:53:31.000000000 +0300
@@ -39,1745 +39,1593 @@
 // over and over again in an attempt to overflow the refcount.
 #define MAX_PAGE_COUNT (1 << 20)
 
-typedef struct {
-	NvU32 get_ahead;
-	NvU32 get_behind;
-	NvU32 put_ahead;
-	NvU32 put_behind;
+typedef struct
+{
+    NvU32 get_ahead;
+    NvU32 get_behind;
+    NvU32 put_ahead;
+    NvU32 put_behind;
 } uvm_tools_queue_snapshot_t;
 
-typedef struct {
-	uvm_spinlock_t lock;
-	NvU64 subscribed_queues;
-	struct list_head queue_nodes[UvmEventNumTypes];
-
-	struct page **queue_buffer_pages;
-	UvmEventEntry *queue;
-	NvU32 queue_buffer_count;
-	NvU32 notification_threshold;
-
-	struct page **control_buffer_pages;
-	UvmToolsEventControlData *control;
-
-	wait_queue_head_t wait_queue;
-	bool is_wakeup_get_valid;
-	NvU32 wakeup_get;
+typedef struct
+{
+    uvm_spinlock_t lock;
+    NvU64 subscribed_queues;
+    struct list_head queue_nodes[UvmEventNumTypes];
+
+    struct page **queue_buffer_pages;
+    UvmEventEntry *queue;
+    NvU32 queue_buffer_count;
+    NvU32 notification_threshold;
+
+    struct page **control_buffer_pages;
+    UvmToolsEventControlData *control;
+
+    wait_queue_head_t wait_queue;
+    bool is_wakeup_get_valid;
+    NvU32 wakeup_get;
 } uvm_tools_queue_t;
 
-typedef struct {
-	struct list_head counter_nodes[UVM_TOTAL_COUNTERS];
-	NvU64 subscribed_counters;
+typedef struct
+{
+    struct list_head counter_nodes[UVM_TOTAL_COUNTERS];
+    NvU64 subscribed_counters;
 
-	struct page **counter_buffer_pages;
-	NvU64 *counters;
+    struct page **counter_buffer_pages;
+    NvU64 *counters;
 
-	bool all_processors;
-	NvProcessorUuid processor;
+    bool all_processors;
+    NvProcessorUuid processor;
 } uvm_tools_counter_t;
 
 // private_data for /dev/nvidia-uvm-tools
-typedef struct {
-	bool is_queue;
-	struct file *uvm_file;
-	union {
-		uvm_tools_queue_t queue;
-		uvm_tools_counter_t counter;
-	};
+typedef struct
+{
+    bool is_queue;
+    struct file *uvm_file;
+    union
+    {
+        uvm_tools_queue_t queue;
+        uvm_tools_counter_t counter;
+    };
 } uvm_tools_event_tracker_t;
 
-typedef struct {
-	// Part of a list rooted at va_space->tools.channel_list
-	// which is a list of channels with pending pushes that have events associated with them
-	struct list_head channel_list_node;
-	uvm_channel_t *channel;
-
-	// The lifetime of this object depends on two things:
-	// 1) whether pending_event_count is zero.  If it is, then this
-	//    object does not need to be in the list channels with pending events.
-	// 2) whether the parent block_migration_data_t has been fully
-	//    processed, resulting in parent_alive being cleared.
-	// Iff both of these conditions are true, the object can be freed.
-	// These objects are allocated together for efficiency.
-	NvU64 pending_event_count;
-	bool parent_alive;
+typedef struct
+{
+    // Part of a list rooted at va_space->tools.channel_list
+    // which is a list of channels with pending pushes that have events associated with them
+    struct list_head channel_list_node;
+    uvm_channel_t *channel;
+
+    // The lifetime of this object depends on two things:
+    // 1) whether pending_event_count is zero.  If it is, then this
+    //    object does not need to be in the list channels with pending events.
+    // 2) whether the parent block_migration_data_t has been fully
+    //    processed, resulting in parent_alive being cleared.
+    // Iff both of these conditions are true, the object can be freed.
+    // These objects are allocated together for efficiency.
+    NvU64 pending_event_count;
+    bool parent_alive;
 } tools_channel_entry_t;
 
-typedef struct {
-	nv_kthread_q_item_t queue_item;
-	uvm_processor_id_t dst;
-	uvm_processor_id_t src;
-	uvm_va_space_t *va_space;
-
-	// The block_migration_data_t is used as a channel entry if it is the first
-	// entry for that channel when it is enqueued.  In this situation, its self_channel_entry
-	// field becomes the channel entry, and its channel_entry field points to self_channel_entry.
-	// This migration will become the channel_entry for all subsequent events for this channel.
-	// Otherwise, there is an existing channel entry for that channel, so this
-	// block_migration_data_t's channel_entry field points to the existing entry.
-	tools_channel_entry_t *channel_entry;
-	tools_channel_entry_t self_channel_entry;
-	struct list_head events;
-	NvU64 start_timestamp_cpu;
-	NvU64 end_timestamp_cpu;
-	NvU64 *start_timestamp_gpu_addr;
-	NvU64 start_timestamp_gpu;
-	NvU64 range_group_id;
-	UvmEventMigrationCause cause;
+typedef struct
+{
+    nv_kthread_q_item_t queue_item;
+    uvm_processor_id_t dst;
+    uvm_processor_id_t src;
+    uvm_va_space_t *va_space;
+
+    // The block_migration_data_t is used as a channel entry if it is the first
+    // entry for that channel when it is enqueued.  In this situation, its self_channel_entry
+    // field becomes the channel entry, and its channel_entry field points to self_channel_entry.
+    // This migration will become the channel_entry for all subsequent events for this channel.
+    // Otherwise, there is an existing channel entry for that channel, so this
+    // block_migration_data_t's channel_entry field points to the existing entry.
+    tools_channel_entry_t *channel_entry;
+    tools_channel_entry_t self_channel_entry;
+    struct list_head events;
+    NvU64 start_timestamp_cpu;
+    NvU64 end_timestamp_cpu;
+    NvU64 *start_timestamp_gpu_addr;
+    NvU64 start_timestamp_gpu;
+    NvU64 range_group_id;
 } block_migration_data_t;
 
-typedef struct {
-	struct list_head events_node;
-	NvU64 bytes;
-	NvU64 address;
-	NvU64 *end_timestamp_gpu_addr;
-	NvU64 end_timestamp_gpu;
+typedef struct
+{
+    struct list_head events_node;
+    NvU64 bytes;
+    NvU64 address;
+    NvU64 *end_timestamp_gpu_addr;
+    NvU64 end_timestamp_gpu;
+    UvmEventMigrationCause cause;
 } migration_data_t;
 
 static struct cdev g_uvm_tools_cdev;
 static struct kmem_cache *g_tools_event_tracker_cache __read_mostly = NULL;
 static LIST_HEAD(g_tools_va_space_list);
 static uvm_rw_semaphore_t g_tools_va_space_list_lock;
-static struct kmem_cache *g_tools_block_migration_data_cache __read_mostly =
-    NULL;
+static struct kmem_cache *g_tools_block_migration_data_cache __read_mostly = NULL;
 static struct kmem_cache *g_tools_migration_data_cache __read_mostly = NULL;
 static nv_kthread_q_t g_tools_queue;
 
-static void uvm_tools_record_fault(uvm_perf_event_t event_id,
-				   uvm_perf_event_data_t * event_data);
-static void uvm_tools_record_migration(uvm_perf_event_t event_id,
-				       uvm_perf_event_data_t * event_data);
-static NV_STATUS tools_update_status(uvm_va_space_t * va_space);
-static void uvm_tools_record_block_migration_begin(uvm_perf_event_t event_id,
-						   uvm_perf_event_data_t *
-						   event_data);
+static void uvm_tools_record_fault(uvm_perf_event_t event_id, uvm_perf_event_data_t *event_data);
+static void uvm_tools_record_migration(uvm_perf_event_t event_id, uvm_perf_event_data_t *event_data);
+static NV_STATUS tools_update_status(uvm_va_space_t *va_space);
 
 static uvm_tools_event_tracker_t *tools_event_tracker(struct file *filp)
 {
-	long event_tracker =
-	    atomic_long_read((atomic_long_t *) & filp->private_data);
+    long event_tracker = atomic_long_read((atomic_long_t *)&filp->private_data);
 
-	smp_read_barrier_depends();
-	return (uvm_tools_event_tracker_t *) event_tracker;
+    smp_read_barrier_depends();
+    return (uvm_tools_event_tracker_t *)event_tracker;
 }
 
-static bool tracker_is_queue(uvm_tools_event_tracker_t * event_tracker)
+static bool tracker_is_queue(uvm_tools_event_tracker_t *event_tracker)
 {
-	return event_tracker != NULL && event_tracker->is_queue;
+    return event_tracker != NULL && event_tracker->is_queue;
 }
 
-static bool tracker_is_counter(uvm_tools_event_tracker_t * event_tracker)
+static bool tracker_is_counter(uvm_tools_event_tracker_t *event_tracker)
 {
-	return event_tracker != NULL && !event_tracker->is_queue;
+    return event_tracker != NULL && !event_tracker->is_queue;
 }
 
 static bool file_is_nvidia_uvm(struct file *filp)
 {
-	return (filp != NULL) && (filp->f_op == &uvm_fops);
+    return (filp != NULL) && (filp->f_op == &uvm_fops);
 }
 
 static void put_user_pages(struct page **pages, NvU64 page_count)
 {
-	NvU64 i;
-	for (i = 0; i < page_count; i++)
-		put_page(pages[i]);
+    NvU64 i;
+    for (i = 0; i < page_count; i++)
+        put_page(pages[i]);
 }
 
 static void unmap_user_pages(struct page **pages, void *addr, NvU64 size)
 {
-	size = DIV_ROUND_UP(size, PAGE_SIZE);
-	vunmap((NvU8 *) addr);
-	put_user_pages(pages, size);
-	uvm_kvfree(pages);
+    size = DIV_ROUND_UP(size, PAGE_SIZE);
+    vunmap((NvU8 *)addr);
+    put_user_pages(pages, size);
+    uvm_kvfree(pages);
 }
 
 // Map virtual memory of data from [user_va, user_va + size) of current process into kernel.
 // Sets *addr to kernel mapping and *pages to the array of struct pages that contain the memory.
-static NV_STATUS map_user_pages(NvU64 user_va, NvU64 size, void **addr,
-				struct page ***pages)
+static NV_STATUS map_user_pages(NvU64 user_va, NvU64 size, void **addr, struct page ***pages)
 {
-	NV_STATUS status = NV_OK;
-	long ret = 0;
-	long num_pages;
-	long i;
-	unsigned int write = 1;
-	struct vm_area_struct **vmas = NULL;
-
-	*addr = NULL;
-	*pages = NULL;
-	num_pages = DIV_ROUND_UP(size, PAGE_SIZE);
-
-	if (uvm_api_range_invalid(user_va, num_pages * PAGE_SIZE)) {
-		status = NV_ERR_INVALID_ADDRESS;
-		goto fail;
-	}
-
-	*pages = uvm_kvmalloc(sizeof(struct page *) * num_pages);
-	if (*pages == NULL) {
-		status = NV_ERR_NO_MEMORY;
-		goto fail;
-	}
-
-	vmas = uvm_kvmalloc(sizeof(struct vm_area_struct *) * num_pages);
-	if (vmas == NULL) {
-		status = NV_ERR_NO_MEMORY;
-		goto fail;
-	}
-
-	down_read(&current->mm->mmap_sem);
-	ret =
-	    NV_GET_USER_PAGES(user_va, num_pages, (write ? FOLL_FORCE : 0),
-			      *pages, vmas);
-	up_read(&current->mm->mmap_sem);
-	if (ret != num_pages) {
-		status = NV_ERR_INVALID_ARGUMENT;
-		goto fail;
-	}
-
-	for (i = 0; i < num_pages; i++) {
-		if (page_count((*pages)[i]) > MAX_PAGE_COUNT
-		    || file_is_nvidia_uvm(vmas[i]->vm_file)) {
-			status = NV_ERR_INVALID_ARGUMENT;
-			goto fail;
-		}
-	}
-
-	*addr = vmap(*pages, num_pages, VM_MAP, PAGE_KERNEL);
-	if (*addr == NULL)
-		goto fail;
-
-	uvm_kvfree(vmas);
-	return NV_OK;
-
- fail:
-	if (*pages == NULL)
-		return status;
-
-	uvm_kvfree(vmas);
-
-	if (ret > 0)
-		put_user_pages(*pages, ret);
-	else if (ret < 0)
-		status = errno_to_nv_status(ret);
-
-	uvm_kvfree(*pages);
-	*pages = NULL;
-	return status;
+    NV_STATUS status = NV_OK;
+    long ret = 0;
+    long num_pages;
+    long i;
+    struct vm_area_struct **vmas = NULL;
+
+    *addr = NULL;
+    *pages = NULL;
+    num_pages = DIV_ROUND_UP(size, PAGE_SIZE);
+
+    if (uvm_api_range_invalid(user_va, num_pages * PAGE_SIZE)) {
+        status = NV_ERR_INVALID_ADDRESS;
+        goto fail;
+    }
+
+    *pages = uvm_kvmalloc(sizeof(struct page *) * num_pages);
+    if (*pages == NULL) {
+        status = NV_ERR_NO_MEMORY;
+        goto fail;
+    }
+
+    vmas = uvm_kvmalloc(sizeof(struct vm_area_struct *) * num_pages);
+    if (vmas == NULL) {
+        status = NV_ERR_NO_MEMORY;
+        goto fail;
+    }
+
+    down_read(&current->mm->mmap_sem);
+    ret = NV_GET_USER_PAGES(user_va, num_pages, 1, 0, *pages, vmas);
+    up_read(&current->mm->mmap_sem);
+    if (ret != num_pages) {
+        status = NV_ERR_INVALID_ARGUMENT;
+        goto fail;
+    }
+
+    for (i = 0; i < num_pages; i++) {
+        if (page_count((*pages)[i]) > MAX_PAGE_COUNT || file_is_nvidia_uvm(vmas[i]->vm_file)) {
+            status = NV_ERR_INVALID_ARGUMENT;
+            goto fail;
+        }
+    }
+
+    *addr = vmap(*pages, num_pages, VM_MAP, PAGE_KERNEL);
+    if (*addr == NULL)
+        goto fail;
+
+    uvm_kvfree(vmas);
+    return NV_OK;
+
+fail:
+    if (*pages == NULL)
+        return status;
+
+    uvm_kvfree(vmas);
+
+    if (ret > 0)
+        put_user_pages(*pages, ret);
+    else if (ret < 0)
+        status = errno_to_nv_status(ret);
+
+    uvm_kvfree(*pages);
+    *pages = NULL;
+    return status;
 }
 
 static void insert_event_tracker(struct list_head *node,
-				 NvU32 list_count,
-				 NvU64 list_mask,
-				 NvU64 * subscribed_mask,
-				 struct list_head *lists)
-{
-	NvU32 i;
-	NvU64 insertable_lists = list_mask & ~*subscribed_mask;
-
-	for (i = 0; i < list_count; i++) {
-		if (insertable_lists & (1ULL << i))
-			list_add(node + i, lists + i);
-	}
+                                 NvU32 list_count,
+                                 NvU64 list_mask,
+                                 NvU64 *subscribed_mask,
+                                 struct list_head *lists)
+{
+    NvU32 i;
+    NvU64 insertable_lists = list_mask & ~*subscribed_mask;
+
+    for (i = 0; i < list_count; i++) {
+        if (insertable_lists & (1ULL << i))
+            list_add(node + i, lists + i);
+    }
 
-	*subscribed_mask |= list_mask;
+    *subscribed_mask |= list_mask;
 }
 
 static void remove_event_tracker(struct list_head *node,
-				 NvU32 list_count,
-				 NvU64 list_mask, NvU64 * subscribed_mask)
+                                 NvU32 list_count,
+                                 NvU64 list_mask,
+                                 NvU64 *subscribed_mask)
+{
+    NvU32 i;
+    NvU64 removable_lists = list_mask & *subscribed_mask;
+    for (i = 0; i < list_count; i++) {
+        if (removable_lists & (1ULL << i))
+            list_del(node + i);
+    }
+
+    *subscribed_mask &= ~list_mask;
+}
+
+static bool queue_needs_wakeup(uvm_tools_queue_t *queue, uvm_tools_queue_snapshot_t *sn)
 {
-	NvU32 i;
-	NvU64 removable_lists = list_mask & *subscribed_mask;
-	for (i = 0; i < list_count; i++) {
-		if (removable_lists & (1ULL << i))
-			list_del(node + i);
-	}
-
-	*subscribed_mask &= ~list_mask;
-}
-
-static bool queue_needs_wakeup(uvm_tools_queue_t * queue,
-			       uvm_tools_queue_snapshot_t * sn)
-{
-	NvU32 queue_mask = queue->queue_buffer_count - 1;
-
-	uvm_assert_spinlock_locked(&queue->lock);
-	return ((queue->queue_buffer_count + sn->put_behind -
-		 sn->get_ahead) & queue_mask) >= queue->notification_threshold;
-}
-
-static void destroy_event_tracker(uvm_tools_event_tracker_t * event_tracker)
-{
-	if (event_tracker->uvm_file != NULL) {
-		NV_STATUS status;
-		uvm_va_space_t *va_space =
-		    uvm_va_space_get(event_tracker->uvm_file);
-
-		uvm_down_write(&g_tools_va_space_list_lock);
-		uvm_down_write(&va_space->perf_events.lock);
-
-		if (event_tracker->is_queue) {
-			uvm_tools_queue_t *queue = &event_tracker->queue;
-
-			remove_event_tracker(queue->queue_nodes,
-					     UvmEventNumTypes,
-					     queue->subscribed_queues,
-					     &queue->subscribed_queues);
-
-			if (queue->queue != NULL) {
-				unmap_user_pages(queue->queue_buffer_pages,
-						 queue->queue,
-						 queue->queue_buffer_count *
-						 sizeof(UvmEventEntry));
-			}
-
-			if (queue->control != NULL) {
-				unmap_user_pages(queue->control_buffer_pages,
-						 queue->control,
-						 sizeof
-						 (UvmToolsEventControlData));
-			}
-		} else {
-			uvm_tools_counter_t *counters = &event_tracker->counter;
-
-			remove_event_tracker(counters->counter_nodes,
-					     UVM_TOTAL_COUNTERS,
-					     counters->subscribed_counters,
-					     &counters->subscribed_counters);
-
-			if (counters->counters != NULL) {
-				unmap_user_pages(counters->counter_buffer_pages,
-						 counters->counters,
-						 UVM_TOTAL_COUNTERS *
-						 sizeof(NvU64));
-			}
-		}
-
-		// de-registration should not fail
-		status = tools_update_status(va_space);
-		UVM_ASSERT(status == NV_OK);
-
-		uvm_up_write(&va_space->perf_events.lock);
-		uvm_up_write(&g_tools_va_space_list_lock);
-
-		fput(event_tracker->uvm_file);
-	}
-	kmem_cache_free(g_tools_event_tracker_cache, event_tracker);
-}
-
-static void enqueue_event(UvmEventEntry * entry, uvm_tools_queue_t * queue)
-{
-	UvmToolsEventControlData *ctrl = queue->control;
-	uvm_tools_queue_snapshot_t sn;
-	NvU32 queue_size = queue->queue_buffer_count;
-	NvU32 queue_mask = queue_size - 1;
-
-	uvm_spin_lock(&queue->lock);
-
-	// ctrl is mapped into user space with read and write permissions,
-	// so its values cannot be trusted.
-	sn.get_behind =
-	    atomic_read((atomic_t *) & ctrl->get_behind) & queue_mask;
-	sn.put_behind =
-	    atomic_read((atomic_t *) & ctrl->put_behind) & queue_mask;
-	sn.put_ahead = (sn.put_behind + 1) & queue_mask;
-
-	// one free element means that the queue is full
-	if (((queue_size + sn.get_behind - sn.put_behind) & queue_mask) == 1) {
-		atomic64_inc((atomic64_t *) & ctrl->dropped +
-			     entry->eventData.eventType);
-		goto unlock;
-	}
-
-	memcpy(queue->queue + sn.put_behind, entry, sizeof(*entry));
-
-	sn.put_behind = sn.put_ahead;
-	// put_ahead and put_behind will always be the same outside of queue->lock
-	// this allows the user-space consumer to choose either a 2 or 4 pointer synchronization approach
-	atomic_set((atomic_t *) & ctrl->put_ahead, sn.put_behind);
-	atomic_set((atomic_t *) & ctrl->put_behind, sn.put_behind);
-
-	sn.get_ahead = atomic_read((atomic_t *) & ctrl->get_ahead);
-	// if the queue needs to be woken up, only signal if we haven't signaled before for this value of get_ahead
-	if (queue_needs_wakeup(queue, &sn)
-	    && !(queue->is_wakeup_get_valid
-		 && queue->wakeup_get == sn.get_ahead)) {
-		queue->is_wakeup_get_valid = true;
-		queue->wakeup_get = sn.get_ahead;
-		wake_up_all(&queue->wait_queue);
-	}
-
- unlock:
-	uvm_spin_unlock(&queue->lock);
-}
-
-static void uvm_tools_record_event(uvm_va_space_t * va_space,
-				   UvmEventEntry * entry)
-{
-	NvU8 eventType = entry->eventData.eventType;
-	uvm_tools_queue_t *queue;
-
-	UVM_ASSERT(eventType < UvmEventNumTypes);
-
-	uvm_assert_rwsem_locked(&va_space->perf_events.lock);
-
-	list_for_each_entry(queue, va_space->tools.queues + eventType,
-			    queue_nodes[eventType])
-	    enqueue_event(entry, queue);
-}
-
-static void uvm_tools_broadcast_event(UvmEventEntry * entry)
-{
-	uvm_va_space_t *va_space;
-
-	uvm_down_read(&g_tools_va_space_list_lock);
-	list_for_each_entry(va_space, &g_tools_va_space_list, tools.node) {
-		uvm_down_read(&va_space->perf_events.lock);
-		uvm_tools_record_event(va_space, entry);
-		uvm_up_read(&va_space->perf_events.lock);
-	}
-	uvm_up_read(&g_tools_va_space_list_lock);
-}
-
-static bool counter_matches_processor(UvmCounterName counter,
-				      const NvProcessorUuid * processor)
-{
-	// For compatibility with older counters, CPU faults for memory with a preferred location are reported
-	// for their preferred location as well as for the CPU device itself.
-	// This check prevents double counting in the aggregate count.
-	if (counter == UvmCounterNameCpuPageFaultCount)
-		return uvm_processor_uuid_eq(processor,
-					     &NV_PROCESSOR_UUID_CPU_DEFAULT);
-	return true;
-}
-
-static void uvm_tools_inc_counter(uvm_va_space_t * va_space,
-				  UvmCounterName counter,
-				  NvU64 amount,
-				  const NvProcessorUuid * processor)
-{
-	UVM_ASSERT((NvU32) counter < UVM_TOTAL_COUNTERS);
-	uvm_assert_rwsem_locked(&va_space->perf_events.lock);
-
-	if (amount > 0) {
-		uvm_tools_counter_t *counters;
-
-		list_for_each_entry(counters,
-				    va_space->tools.counters + counter,
-				    counter_nodes[counter]) {
-			if ((counters->all_processors
-			     && counter_matches_processor(counter, processor))
-			    || uvm_processor_uuid_eq(&counters->processor,
-						     processor)) {
-				atomic64_add(amount,
-					     (atomic64_t *) (counters->
-							     counters +
-							     counter));
-			}
-		}
-	}
-}
-
-static bool tools_are_enabled(uvm_va_space_t * va_space)
-{
-	NvU32 i;
-
-	for (i = 0; i < ARRAY_SIZE(va_space->tools.counters); i++) {
-		if (!list_empty(va_space->tools.counters + i))
-			return true;
-	}
-	for (i = 0; i < ARRAY_SIZE(va_space->tools.queues); i++) {
-		if (!list_empty(va_space->tools.queues + i))
-			return true;
-	}
-	return false;
+    NvU32 queue_mask = queue->queue_buffer_count - 1;
+
+    uvm_assert_spinlock_locked(&queue->lock);
+    return ((queue->queue_buffer_count + sn->put_behind - sn->get_ahead) & queue_mask) >= queue->notification_threshold;
+}
+
+static void destroy_event_tracker(uvm_tools_event_tracker_t *event_tracker)
+{
+    if (event_tracker->uvm_file != NULL) {
+        NV_STATUS status;
+        uvm_va_space_t *va_space = uvm_va_space_get(event_tracker->uvm_file);
+
+        uvm_down_write(&g_tools_va_space_list_lock);
+        uvm_down_write(&va_space->perf_events.lock);
+
+        if (event_tracker->is_queue) {
+            uvm_tools_queue_t *queue = &event_tracker->queue;
+
+            remove_event_tracker(queue->queue_nodes,
+                                 UvmEventNumTypes,
+                                 queue->subscribed_queues,
+                                 &queue->subscribed_queues);
+
+            if (queue->queue != NULL) {
+                unmap_user_pages(queue->queue_buffer_pages,
+                                 queue->queue,
+                                 queue->queue_buffer_count * sizeof(UvmEventEntry));
+            }
+
+            if (queue->control != NULL) {
+                unmap_user_pages(queue->control_buffer_pages,
+                                 queue->control,
+                                 sizeof(UvmToolsEventControlData));
+            }
+        }
+        else {
+            uvm_tools_counter_t *counters = &event_tracker->counter;
+
+            remove_event_tracker(counters->counter_nodes,
+                                 UVM_TOTAL_COUNTERS,
+                                 counters->subscribed_counters,
+                                 &counters->subscribed_counters);
+
+            if (counters->counters != NULL) {
+                unmap_user_pages(counters->counter_buffer_pages,
+                                 counters->counters,
+                                 UVM_TOTAL_COUNTERS * sizeof(NvU64));
+            }
+        }
+
+        // de-registration should not fail
+        status = tools_update_status(va_space);
+        UVM_ASSERT(status == NV_OK);
+
+        uvm_up_write(&va_space->perf_events.lock);
+        uvm_up_write(&g_tools_va_space_list_lock);
+
+        fput(event_tracker->uvm_file);
+    }
+    kmem_cache_free(g_tools_event_tracker_cache, event_tracker);
+}
+
+static void enqueue_event(UvmEventEntry *entry, uvm_tools_queue_t *queue)
+{
+    UvmToolsEventControlData *ctrl = queue->control;
+    uvm_tools_queue_snapshot_t sn;
+    NvU32 queue_size = queue->queue_buffer_count;
+    NvU32 queue_mask = queue_size - 1;
+
+    uvm_spin_lock(&queue->lock);
+
+    // ctrl is mapped into user space with read and write permissions,
+    // so its values cannot be trusted.
+    sn.get_behind = atomic_read((atomic_t *)&ctrl->get_behind) & queue_mask;
+    sn.put_behind = atomic_read((atomic_t *)&ctrl->put_behind) & queue_mask;
+    sn.put_ahead = (sn.put_behind + 1) & queue_mask;
+
+    // one free element means that the queue is full
+    if (((queue_size + sn.get_behind - sn.put_behind) & queue_mask) == 1) {
+        atomic64_inc((atomic64_t *)&ctrl->dropped + entry->eventData.eventType);
+        goto unlock;
+    }
+
+    memcpy(queue->queue + sn.put_behind, entry, sizeof(*entry));
+
+    sn.put_behind = sn.put_ahead;
+    // put_ahead and put_behind will always be the same outside of queue->lock
+    // this allows the user-space consumer to choose either a 2 or 4 pointer synchronization approach
+    atomic_set((atomic_t *)&ctrl->put_ahead, sn.put_behind);
+    atomic_set((atomic_t *)&ctrl->put_behind, sn.put_behind);
+
+    sn.get_ahead = atomic_read((atomic_t *)&ctrl->get_ahead);
+    // if the queue needs to be woken up, only signal if we haven't signaled before for this value of get_ahead
+    if (queue_needs_wakeup(queue, &sn) && !(queue->is_wakeup_get_valid && queue->wakeup_get == sn.get_ahead)) {
+        queue->is_wakeup_get_valid = true;
+        queue->wakeup_get = sn.get_ahead;
+        wake_up_all(&queue->wait_queue);
+    }
+
+unlock:
+    uvm_spin_unlock(&queue->lock);
+}
+
+static void uvm_tools_record_event(uvm_va_space_t *va_space, UvmEventEntry *entry)
+{
+    NvU8 eventType = entry->eventData.eventType;
+    uvm_tools_queue_t *queue;
+
+    UVM_ASSERT(eventType < UvmEventNumTypes);
+
+    uvm_assert_rwsem_locked(&va_space->perf_events.lock);
+
+    list_for_each_entry(queue, va_space->tools.queues + eventType, queue_nodes[eventType])
+        enqueue_event(entry, queue);
+}
+
+static void uvm_tools_broadcast_event(UvmEventEntry *entry)
+{
+    uvm_va_space_t *va_space;
+
+    uvm_down_read(&g_tools_va_space_list_lock);
+    list_for_each_entry(va_space, &g_tools_va_space_list, tools.node) {
+        uvm_down_read(&va_space->perf_events.lock);
+        uvm_tools_record_event(va_space, entry);
+        uvm_up_read(&va_space->perf_events.lock);
+    }
+    uvm_up_read(&g_tools_va_space_list_lock);
+}
+
+static bool counter_matches_processor(UvmCounterName counter, const NvProcessorUuid *processor)
+{
+    // For compatibility with older counters, CPU faults for memory with a preferred location are reported
+    // for their preferred location as well as for the CPU device itself.
+    // This check prevents double counting in the aggregate count.
+    if (counter == UvmCounterNameCpuPageFaultCount)
+        return uvm_processor_uuid_eq(processor, &NV_PROCESSOR_UUID_CPU_DEFAULT);
+    return true;
+}
+
+static void uvm_tools_inc_counter(uvm_va_space_t *va_space,
+                                  UvmCounterName counter,
+                                  NvU64 amount,
+                                  const NvProcessorUuid *processor)
+{
+    UVM_ASSERT((NvU32)counter < UVM_TOTAL_COUNTERS);
+    uvm_assert_rwsem_locked(&va_space->perf_events.lock);
+
+    if (amount > 0) {
+        uvm_tools_counter_t *counters;
+
+        list_for_each_entry(counters, va_space->tools.counters + counter, counter_nodes[counter]) {
+            if ((counters->all_processors && counter_matches_processor(counter, processor)) ||
+                uvm_processor_uuid_eq(&counters->processor, processor)) {
+                atomic64_add(amount, (atomic64_t *)(counters->counters + counter));
+            }
+        }
+    }
+}
+
+static bool tools_are_enabled(uvm_va_space_t *va_space)
+{
+    NvU32 i;
+
+    for (i = 0; i < ARRAY_SIZE(va_space->tools.counters); i++) {
+        if (!list_empty(va_space->tools.counters + i))
+            return true;
+    }
+    for (i = 0; i < ARRAY_SIZE(va_space->tools.queues); i++) {
+        if (!list_empty(va_space->tools.queues + i))
+            return true;
+    }
+    return false;
 }
 
 static int uvm_tools_open(struct inode *inode, struct file *filp)
 {
-	filp->private_data = NULL;
-	return -nv_status_to_errno(uvm_global_get_status());
+    filp->private_data = NULL;
+    return -nv_status_to_errno(uvm_global_get_status());
 }
 
 static int uvm_tools_release(struct inode *inode, struct file *filp)
 {
-	uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
-	if (event_tracker != NULL) {
-		destroy_event_tracker(event_tracker);
-		filp->private_data = NULL;
-	}
-	return -nv_status_to_errno(uvm_global_get_status());
-}
-
-static long uvm_tools_unlocked_ioctl(struct file *filp, unsigned int cmd,
-				     unsigned long arg)
-{
-	switch (cmd) {
-		UVM_ROUTE_CMD_STACK(UVM_TOOLS_INIT_EVENT_TRACKER,
-				    uvm_api_tools_init_event_tracker);
-		UVM_ROUTE_CMD_STACK(UVM_TOOLS_SET_NOTIFICATION_THRESHOLD,
-				    uvm_api_tools_set_notification_threshold);
-		UVM_ROUTE_CMD_STACK(UVM_TOOLS_EVENT_QUEUE_ENABLE_EVENTS,
-				    uvm_api_tools_event_queue_enable_events);
-		UVM_ROUTE_CMD_STACK(UVM_TOOLS_EVENT_QUEUE_DISABLE_EVENTS,
-				    uvm_api_tools_event_queue_disable_events);
-		UVM_ROUTE_CMD_STACK(UVM_TOOLS_ENABLE_COUNTERS,
-				    uvm_api_tools_enable_counters);
-		UVM_ROUTE_CMD_STACK(UVM_TOOLS_DISABLE_COUNTERS,
-				    uvm_api_tools_disable_counters);
-	}
-	return -EINVAL;
-}
-
-static unsigned int uvm_tools_poll(struct file *filp, poll_table * wait)
-{
-	int flags = 0;
-	uvm_tools_queue_snapshot_t sn;
-	uvm_tools_event_tracker_t *event_tracker;
-	UvmToolsEventControlData *ctrl;
-
-	if (uvm_global_get_status() != NV_OK)
-		return POLLERR;
-
-	event_tracker = tools_event_tracker(filp);
-	if (!tracker_is_queue(event_tracker))
-		return POLLERR;
-
-	uvm_spin_lock(&event_tracker->queue.lock);
-
-	event_tracker->queue.is_wakeup_get_valid = false;
-	ctrl = event_tracker->queue.control;
-	sn.get_ahead = atomic_read((atomic_t *) & ctrl->get_ahead);
-	sn.put_behind = atomic_read((atomic_t *) & ctrl->put_behind);
+    uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
+    if (event_tracker != NULL) {
+        destroy_event_tracker(event_tracker);
+        filp->private_data = NULL;
+    }
+    return -nv_status_to_errno(uvm_global_get_status());
+}
+
+static long uvm_tools_unlocked_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+    switch (cmd) {
+        UVM_ROUTE_CMD_STACK(UVM_TOOLS_INIT_EVENT_TRACKER,         uvm_api_tools_init_event_tracker);
+        UVM_ROUTE_CMD_STACK(UVM_TOOLS_SET_NOTIFICATION_THRESHOLD, uvm_api_tools_set_notification_threshold);
+        UVM_ROUTE_CMD_STACK(UVM_TOOLS_EVENT_QUEUE_ENABLE_EVENTS,  uvm_api_tools_event_queue_enable_events);
+        UVM_ROUTE_CMD_STACK(UVM_TOOLS_EVENT_QUEUE_DISABLE_EVENTS, uvm_api_tools_event_queue_disable_events);
+        UVM_ROUTE_CMD_STACK(UVM_TOOLS_ENABLE_COUNTERS,            uvm_api_tools_enable_counters);
+        UVM_ROUTE_CMD_STACK(UVM_TOOLS_DISABLE_COUNTERS,           uvm_api_tools_disable_counters);
+    }
+    return -EINVAL;
+}
+
+static unsigned int uvm_tools_poll(struct file *filp, poll_table *wait)
+{
+    int flags = 0;
+    uvm_tools_queue_snapshot_t sn;
+    uvm_tools_event_tracker_t *event_tracker;
+    UvmToolsEventControlData *ctrl;
+
+    if (uvm_global_get_status() != NV_OK)
+        return POLLERR;
 
-	if (queue_needs_wakeup(&event_tracker->queue, &sn))
-		flags = POLLIN | POLLRDNORM;
+    event_tracker = tools_event_tracker(filp);
+    if (!tracker_is_queue(event_tracker))
+        return POLLERR;
 
-	uvm_spin_unlock(&event_tracker->queue.lock);
+    uvm_spin_lock(&event_tracker->queue.lock);
 
-	poll_wait(filp, &event_tracker->queue.wait_queue, wait);
-	return flags;
+    event_tracker->queue.is_wakeup_get_valid = false;
+    ctrl = event_tracker->queue.control;
+    sn.get_ahead = atomic_read((atomic_t *)&ctrl->get_ahead);
+    sn.put_behind = atomic_read((atomic_t *)&ctrl->put_behind);
+
+    if (queue_needs_wakeup(&event_tracker->queue, &sn))
+        flags = POLLIN | POLLRDNORM;
+
+    uvm_spin_unlock(&event_tracker->queue.lock);
+
+    poll_wait(filp, &event_tracker->queue.wait_queue, wait);
+    return flags;
 }
 
 static UvmEventFaultType g_hal_to_tools_fault_type_table[UVM_FAULT_TYPE_MAX] = {
-	[UVM_FAULT_TYPE_INVALID_PDE] = UvmFaultTypeInvalidPde,
-	[UVM_FAULT_TYPE_INVALID_PTE] = UvmFaultTypeInvalidPte,
-	[UVM_FAULT_TYPE_ATOMIC] = UvmFaultTypeAtomic,
-	[UVM_FAULT_TYPE_WRITE] = UvmFaultTypeWrite,
-	[UVM_FAULT_TYPE_PDE_SIZE] = UvmFaultTypeInvalidPdeSize,
-	[UVM_FAULT_TYPE_VA_LIMIT_VIOLATION] = UvmFaultTypeLimitViolation,
-	[UVM_FAULT_TYPE_UNBOUND_INST_BLOCK] = UvmFaultTypeUnboundInstBlock,
-	[UVM_FAULT_TYPE_PRIV_VIOLATION] = UvmFaultTypePrivViolation,
-	[UVM_FAULT_TYPE_PITCH_MASK_VIOLATION] = UvmFaultTypePitchMaskViolation,
-	[UVM_FAULT_TYPE_WORK_CREATION] = UvmFaultTypeWorkCreation,
-	[UVM_FAULT_TYPE_UNSUPPORTED_APERTURE] = UvmFaultTypeUnsupportedAperture,
-	[UVM_FAULT_TYPE_COMPRESSION_FAILURE] = UvmFaultTypeCompressionFailure,
-	[UVM_FAULT_TYPE_UNSUPPORTED_KIND] = UvmFaultTypeUnsupportedKind,
-	[UVM_FAULT_TYPE_REGION_VIOLATION] = UvmFaultTypeRegionViolation,
-	[UVM_FAULT_TYPE_POISONED] = UvmFaultTypePoison,
+    [UVM_FAULT_TYPE_INVALID_PDE]          = UvmFaultTypeInvalidPde,
+    [UVM_FAULT_TYPE_INVALID_PTE]          = UvmFaultTypeInvalidPte,
+    [UVM_FAULT_TYPE_ATOMIC]               = UvmFaultTypeAtomic,
+    [UVM_FAULT_TYPE_WRITE]                = UvmFaultTypeWrite,
+    [UVM_FAULT_TYPE_PDE_SIZE]             = UvmFaultTypeInvalidPdeSize,
+    [UVM_FAULT_TYPE_VA_LIMIT_VIOLATION]   = UvmFaultTypeLimitViolation,
+    [UVM_FAULT_TYPE_UNBOUND_INST_BLOCK]   = UvmFaultTypeUnboundInstBlock,
+    [UVM_FAULT_TYPE_PRIV_VIOLATION]       = UvmFaultTypePrivViolation,
+    [UVM_FAULT_TYPE_PITCH_MASK_VIOLATION] = UvmFaultTypePitchMaskViolation,
+    [UVM_FAULT_TYPE_WORK_CREATION]        = UvmFaultTypeWorkCreation,
+    [UVM_FAULT_TYPE_UNSUPPORTED_APERTURE] = UvmFaultTypeUnsupportedAperture,
+    [UVM_FAULT_TYPE_COMPRESSION_FAILURE]  = UvmFaultTypeCompressionFailure,
+    [UVM_FAULT_TYPE_UNSUPPORTED_KIND]     = UvmFaultTypeUnsupportedKind,
+    [UVM_FAULT_TYPE_REGION_VIOLATION]     = UvmFaultTypeRegionViolation,
+    [UVM_FAULT_TYPE_POISONED]             = UvmFaultTypePoison,
 };
 
-static UvmEventMemoryAccessType
-    g_hal_to_tools_fault_access_type_table[UVM_FAULT_ACCESS_TYPE_MAX] = {
-	[UVM_FAULT_ACCESS_TYPE_ATOMIC] = UvmEventMemoryAccessTypeAtomic,
-	[UVM_FAULT_ACCESS_TYPE_WRITE] = UvmEventMemoryAccessTypeWrite,
-	[UVM_FAULT_ACCESS_TYPE_READ] = UvmEventMemoryAccessTypeRead,
-	[UVM_FAULT_ACCESS_TYPE_PREFETCH] = UvmEventMemoryAccessTypePrefetch
+static UvmEventMemoryAccessType g_hal_to_tools_fault_access_type_table[UVM_FAULT_ACCESS_TYPE_MAX] = {
+    [UVM_FAULT_ACCESS_TYPE_ATOMIC]   = UvmEventMemoryAccessTypeAtomic,
+    [UVM_FAULT_ACCESS_TYPE_WRITE]    = UvmEventMemoryAccessTypeWrite,
+    [UVM_FAULT_ACCESS_TYPE_READ]     = UvmEventMemoryAccessTypeRead,
+    [UVM_FAULT_ACCESS_TYPE_PREFETCH] = UvmEventMemoryAccessTypePrefetch
 };
 
-static void uvm_tools_record_fault(uvm_perf_event_t event_id,
-				   uvm_perf_event_data_t * event_data)
+static void uvm_tools_record_fault(uvm_perf_event_t event_id, uvm_perf_event_data_t *event_data)
 {
-	UvmEventEntry entry;
-	uvm_va_space_t *va_space;
+    UvmEventEntry entry;
+    uvm_va_space_t *va_space;
 
-	UVM_ASSERT(event_id == UVM_PERF_EVENT_FAULT);
-	UVM_ASSERT(event_data->fault.space);
+    UVM_ASSERT(event_id == UVM_PERF_EVENT_FAULT);
+    UVM_ASSERT(event_data->fault.space);
 
-	va_space = event_data->fault.space;
-
-	uvm_assert_rwsem_locked(&va_space->lock);
-	uvm_assert_rwsem_locked(&va_space->perf_events.lock);
-	UVM_ASSERT(va_space->tools.enabled);
-
-	memset(&entry, 0, sizeof(entry));
-
-	if (event_data->fault.proc_id == UVM_CPU_ID) {
-		uvm_processor_id_t preferred_location;
-		UvmEventCpuFaultInfo *info = &entry.eventData.cpuFault;
-
-		info->eventType = UvmEventTypeCpuFault;
-		if (event_data->fault.cpu.is_write)
-			info->accessType = UvmEventMemoryAccessTypeWrite;
-		else
-			info->accessType = UvmEventMemoryAccessTypeRead;
-
-		info->address = event_data->fault.cpu.fault_va;
-		info->timeStamp = NV_GETTIME();
-		// assume that current owns va_space
-		info->pid = uvm_get_stale_process_id();
-		info->threadId = uvm_get_stale_thread_id();
-
-		// The UVM Lite tools interface did not represent the CPU as a UVM device.
-		// It reported CPU faults against the corresponding allocation's 'home location'.
-		// Though this driver's tools interface does include a CPU device, for compatibility,
-		// the driver still reports faults against a buffer's preferred location,
-		// in addition to the CPU.
-		uvm_tools_inc_counter(va_space, UvmCounterNameCpuPageFaultCount,
-				      1, &NV_PROCESSOR_UUID_CPU_DEFAULT);
-
-		preferred_location =
-		    event_data->fault.block->va_range->preferred_location;
-		if (preferred_location != UVM8_MAX_PROCESSORS
-		    && preferred_location != UVM_CPU_ID) {
-			uvm_gpu_t *gpu = uvm_gpu_get(preferred_location);
-			uvm_tools_inc_counter(va_space,
-					      UvmCounterNameCpuPageFaultCount,
-					      1, &gpu->uuid);
-		}
-	} else {
-		uvm_fault_buffer_entry_t *buffer_entry =
-		    event_data->fault.gpu.buffer_entry;
-		UvmEventGpuFaultInfo *info = &entry.eventData.gpuFault;
-		uvm_gpu_t *gpu = uvm_gpu_get(event_data->fault.proc_id);
-
-		UVM_ASSERT(gpu);
-
-		info->eventType = UvmEventTypeGpuFault;
-		info->gpuIndex = gpu->id;
-		info->faultType =
-		    g_hal_to_tools_fault_type_table[buffer_entry->fault_type];
-		info->accessType =
-		    g_hal_to_tools_fault_access_type_table[buffer_entry->
-							   fault_access_type];
-		info->gpcId = info->gpcId;
-		info->tpcId = info->tpcId;
-		info->address = buffer_entry->fault_address;
-		info->timeStamp = NV_GETTIME();
-		info->timeStampGpu = buffer_entry->timestamp;
-		info->batchId = event_data->fault.gpu.batch_id;
-
-		uvm_tools_inc_counter(va_space, UvmCounterNameGpuPageFaultCount,
-				      1, &gpu->uuid);
-	}
-
-	uvm_tools_record_event(va_space, &entry);
-}
-
-static void add_pending_event_for_channel(uvm_va_space_t * va_space,
-					  block_migration_data_t * block_mig)
-{
-	tools_channel_entry_t *channel_entry;
-
-	uvm_assert_spinlock_locked(&va_space->tools.channel_list_lock);
-
-	// If this channel already has pending events, just increment the count
-	list_for_each_entry(channel_entry, &va_space->tools.channel_list,
-			    channel_list_node) {
-		if (channel_entry->channel ==
-		    block_mig->self_channel_entry.channel)
-			goto done;
-	}
-
-	// otherwise, use the channel list from within the block migration
-	channel_entry = &block_mig->self_channel_entry;
-	list_add_tail(&channel_entry->channel_list_node,
-		      &va_space->tools.channel_list);
-
- done:
-	block_mig->channel_entry = channel_entry;
-	channel_entry->pending_event_count++;
-}
-
-static void remove_pending_event_for_channel(uvm_va_space_t * va_space,
-					     tools_channel_entry_t *
-					     channel_entry)
-{
-	uvm_assert_spinlock_locked(&va_space->tools.channel_list_lock);
-	UVM_ASSERT(channel_entry->pending_event_count > 0);
-	channel_entry->pending_event_count--;
-	if (channel_entry->pending_event_count == 0) {
-		list_del(&channel_entry->channel_list_node);
-
-		if (!channel_entry->parent_alive) {
-			block_migration_data_t *block_mig =
-			    container_of(channel_entry, block_migration_data_t,
-					 self_channel_entry);
-			kmem_cache_free(g_tools_block_migration_data_cache,
-					block_mig);
-		}
-	}
+    va_space = event_data->fault.space;
+
+    uvm_assert_rwsem_locked(&va_space->lock);
+    uvm_assert_rwsem_locked(&va_space->perf_events.lock);
+    UVM_ASSERT(va_space->tools.enabled);
+
+    memset(&entry, 0, sizeof(entry));
+
+    if (event_data->fault.proc_id == UVM_CPU_ID) {
+        uvm_processor_id_t preferred_location;
+        UvmEventCpuFaultInfo *info = &entry.eventData.cpuFault;
+
+        info->eventType = UvmEventTypeCpuFault;
+        if (event_data->fault.cpu.is_write)
+            info->accessType = UvmEventMemoryAccessTypeWrite;
+        else
+            info->accessType = UvmEventMemoryAccessTypeRead;
+
+        info->address = event_data->fault.cpu.fault_va;
+        info->timeStamp = NV_GETTIME();
+        // assume that current owns va_space
+        info->pid = uvm_get_stale_process_id();
+        info->threadId = uvm_get_stale_thread_id();
+
+        // The UVM Lite tools interface did not represent the CPU as a UVM device.
+        // It reported CPU faults against the corresponding allocation's 'home location'.
+        // Though this driver's tools interface does include a CPU device, for compatibility,
+        // the driver still reports faults against a buffer's preferred location,
+        // in addition to the CPU.
+        uvm_tools_inc_counter(va_space, UvmCounterNameCpuPageFaultCount, 1, &NV_PROCESSOR_UUID_CPU_DEFAULT);
+
+        preferred_location = event_data->fault.block->va_range->preferred_location;
+        if (preferred_location != UVM8_MAX_PROCESSORS && preferred_location != UVM_CPU_ID) {
+            uvm_gpu_t *gpu = uvm_gpu_get(preferred_location);
+            uvm_tools_inc_counter(va_space, UvmCounterNameCpuPageFaultCount, 1, &gpu->uuid);
+        }
+    }
+    else {
+        uvm_fault_buffer_entry_t *buffer_entry = event_data->fault.gpu.buffer_entry;
+        UvmEventGpuFaultInfo *info = &entry.eventData.gpuFault;
+        uvm_gpu_t *gpu = uvm_gpu_get(event_data->fault.proc_id);
+
+        UVM_ASSERT(gpu);
+
+        info->eventType    = UvmEventTypeGpuFault;
+        info->gpuIndex     = gpu->id;
+        info->faultType    = g_hal_to_tools_fault_type_table[buffer_entry->fault_type];
+        info->accessType   = g_hal_to_tools_fault_access_type_table[buffer_entry->fault_access_type];
+        info->gpcId        = info->gpcId;
+        info->tpcId        = info->tpcId;
+        info->address      = buffer_entry->fault_address;
+        info->timeStamp    = NV_GETTIME();
+        info->timeStampGpu = buffer_entry->timestamp;
+        info->batchId      = event_data->fault.gpu.batch_id;
+
+        uvm_tools_inc_counter(va_space, UvmCounterNameGpuPageFaultCount, 1, &gpu->uuid);
+    }
+
+    uvm_tools_record_event(va_space, &entry);
+}
+
+static void add_pending_event_for_channel(uvm_va_space_t *va_space, block_migration_data_t *block_mig)
+{
+    tools_channel_entry_t *channel_entry;
+
+    uvm_assert_spinlock_locked(&va_space->tools.channel_list_lock);
+
+    // If this channel already has pending events, just increment the count
+    list_for_each_entry(channel_entry, &va_space->tools.channel_list, channel_list_node) {
+        if (channel_entry->channel == block_mig->self_channel_entry.channel)
+            goto done;
+    }
+
+    // otherwise, use the channel list from within the block migration
+    channel_entry = &block_mig->self_channel_entry;
+    list_add_tail(&channel_entry->channel_list_node, &va_space->tools.channel_list);
+
+done:
+    block_mig->channel_entry = channel_entry;
+    channel_entry->pending_event_count++;
+}
+
+static void remove_pending_event_for_channel(uvm_va_space_t *va_space, tools_channel_entry_t *channel_entry)
+{
+    uvm_assert_spinlock_locked(&va_space->tools.channel_list_lock);
+    UVM_ASSERT(channel_entry->pending_event_count > 0);
+    channel_entry->pending_event_count--;
+    if (channel_entry->pending_event_count == 0) {
+        list_del(&channel_entry->channel_list_node);
+
+        if (!channel_entry->parent_alive) {
+            block_migration_data_t *block_mig = container_of(channel_entry, block_migration_data_t, self_channel_entry);
+            kmem_cache_free(g_tools_block_migration_data_cache, block_mig);
+        }
+    }
 }
 
+
 void record_migration_events(void *args)
 {
-	block_migration_data_t *block_mig = (block_migration_data_t *) args;
-	migration_data_t *mig;
-	migration_data_t *next;
-	UvmEventEntry entry;
-	UvmEventMigrationInfo *info = &entry.eventData.migration;
-	uvm_va_space_t *va_space = block_mig->va_space;
-
-	NvU64 gpu_timestamp = block_mig->start_timestamp_gpu;
-
-	UVM_ASSERT(block_mig->self_channel_entry.parent_alive);
-
-	// Initialize fields that are constant throughout the whole block
-	memset(&entry, 0, sizeof(entry));
-	info->srcIndex = block_mig->src;
-	info->dstIndex = block_mig->dst;
-	info->beginTimeStamp = block_mig->start_timestamp_cpu;
-	info->endTimeStamp = block_mig->end_timestamp_cpu;
-	info->rangeGroupId = block_mig->range_group_id;
-	info->migrationCause = block_mig->cause;
-
-	uvm_down_read(&va_space->perf_events.lock);
-	list_for_each_entry_safe(mig, next, &block_mig->events, events_node) {
-
-		UVM_ASSERT(mig->bytes > 0);
-		list_del(&mig->events_node);
-
-		info->eventType = UvmEventTypeMigration;
-		info->address = mig->address;
-		info->migratedBytes = mig->bytes;
-		info->beginTimeStampGpu = gpu_timestamp;
-		info->endTimeStampGpu = mig->end_timestamp_gpu;
-		gpu_timestamp = mig->end_timestamp_gpu;
-		kmem_cache_free(g_tools_migration_data_cache, mig);
-
-		uvm_tools_record_event(block_mig->va_space, &entry);
-	}
-	uvm_up_read(&va_space->perf_events.lock);
-
-	uvm_spin_lock(&va_space->tools.channel_list_lock);
-	block_mig->self_channel_entry.parent_alive = false;
-	if (block_mig->self_channel_entry.pending_event_count == 0)
-		kmem_cache_free(g_tools_block_migration_data_cache, block_mig);
+    block_migration_data_t *block_mig = (block_migration_data_t *)args;
+    migration_data_t *mig;
+    migration_data_t *next;
+    UvmEventEntry entry;
+    UvmEventMigrationInfo *info = &entry.eventData.migration;
+    uvm_va_space_t *va_space = block_mig->va_space;
+
+    NvU64 gpu_timestamp = block_mig->start_timestamp_gpu;
+
+    UVM_ASSERT(block_mig->self_channel_entry.parent_alive);
+
+    // Initialize fields that are constant throughout the whole block
+    memset(&entry, 0, sizeof(entry));
+    info->eventType = UvmEventTypeMigration;
+    info->srcIndex = block_mig->src;
+    info->dstIndex = block_mig->dst;
+    info->beginTimeStamp = block_mig->start_timestamp_cpu;
+    info->endTimeStamp = block_mig->end_timestamp_cpu;
+    info->rangeGroupId = block_mig->range_group_id;
+
+    uvm_down_read(&va_space->perf_events.lock);
+    list_for_each_entry_safe(mig, next, &block_mig->events, events_node) {
+
+        UVM_ASSERT(mig->bytes > 0);
+        list_del(&mig->events_node);
+
+        info->address = mig->address;
+        info->migratedBytes = mig->bytes;
+        info->beginTimeStampGpu = gpu_timestamp;
+        info->endTimeStampGpu = mig->end_timestamp_gpu;
+        info->migrationCause = mig->cause;
+        gpu_timestamp = mig->end_timestamp_gpu;
+        kmem_cache_free(g_tools_migration_data_cache, mig);
+
+        uvm_tools_record_event(block_mig->va_space, &entry);
+    }
+    uvm_up_read(&va_space->perf_events.lock);
+
+    uvm_spin_lock(&va_space->tools.channel_list_lock);
+    block_mig->self_channel_entry.parent_alive = false;
+    if (block_mig->self_channel_entry.pending_event_count == 0)
+        kmem_cache_free(g_tools_block_migration_data_cache, block_mig);
 
-	uvm_spin_unlock(&va_space->tools.channel_list_lock);
+    uvm_spin_unlock(&va_space->tools.channel_list_lock);
 }
 
 void on_block_migration_complete(void *ptr)
 {
-	migration_data_t *mig;
-	block_migration_data_t *block_mig = (block_migration_data_t *) ptr;
+    migration_data_t *mig;
+    block_migration_data_t *block_mig = (block_migration_data_t *)ptr;
 
-	block_mig->end_timestamp_cpu = NV_GETTIME();
-	block_mig->start_timestamp_gpu = *block_mig->start_timestamp_gpu_addr;
-	list_for_each_entry(mig, &block_mig->events, events_node)
-	    mig->end_timestamp_gpu = *mig->end_timestamp_gpu_addr;
-
-	nv_kthread_q_item_init(&block_mig->queue_item, record_migration_events,
-			       block_mig);
-
-	// The UVM driver may notice that work in a channel is complete in a variety of situations
-	// and the va_space lock is not always held in all of them, nor can it always be taken safely on them.
-	// Dispatching events requires the va_space lock to be held in at least read mode, so
-	// this callback simply enqueues the dispatching onto a queue, where the
-	// va_space lock is always safe to acquire.
-	uvm_spin_lock(&block_mig->va_space->tools.channel_list_lock);
-	remove_pending_event_for_channel(block_mig->va_space,
-					 block_mig->channel_entry);
-	nv_kthread_q_schedule_q_item(&g_tools_queue, &block_mig->queue_item);
-	uvm_spin_unlock(&block_mig->va_space->tools.channel_list_lock);
-}
-
-static void uvm_tools_record_migration(uvm_perf_event_t event_id,
-				       uvm_perf_event_data_t * event_data)
-{
-	uvm_va_block_t *va_block = event_data->migration.block;
-	uvm_va_space_t *va_space = va_block->va_range->va_space;
-
-	UVM_ASSERT(event_id == UVM_PERF_EVENT_MIGRATION);
-
-	uvm_assert_mutex_locked(&va_block->lock);
-	uvm_assert_rwsem_locked(&va_space->perf_events.lock);
-
-	if (va_space->tools.enabled) {
-		UvmEventEntry entry;
-		migration_data_t *mig;
-		uvm_push_info_t *push_info =
-		    uvm_push_info_from_push(event_data->migration.push);
-		block_migration_data_t *block_mig =
-		    (block_migration_data_t *) push_info->on_complete_data;
-		NvU64 begin_time_stamp = NV_GETTIME();
-		size_t page_index;
-		uvm_va_block_region_t region =
-		    uvm_va_block_region_from_start_size(va_block,
-							event_data->migration.
-							address,
-							event_data->migration.
-							bytes);
-
-		// Increment counters
-		if (event_data->migration.src == UVM_CPU_ID) {
-			uvm_gpu_t *gpu = uvm_gpu_get(event_data->migration.dst);
-			uvm_tools_inc_counter(va_space,
-					      UvmCounterNameBytesXferHtD,
-					      event_data->migration.bytes,
-					      &gpu->uuid);
-		} else if (event_data->migration.dst == UVM_CPU_ID) {
-			uvm_gpu_t *gpu = uvm_gpu_get(event_data->migration.src);
-			uvm_tools_inc_counter(va_space,
-					      UvmCounterNameBytesXferDtH,
-					      event_data->migration.bytes,
-					      &gpu->uuid);
-		}
-
-		if (push_info->on_complete != NULL) {
-			mig =
-			    kmem_cache_alloc(g_tools_migration_data_cache,
-					     NV_UVM_GFP_FLAGS);
-			if (mig == NULL)
-				return;
-
-			mig->address = event_data->migration.address;
-			mig->bytes = event_data->migration.bytes;
-			mig->end_timestamp_gpu_addr =
-			    uvm_push_timestamp(event_data->migration.push);
-
-			list_add_tail(&mig->events_node, &block_mig->events);
-		}
-		// Read-duplication events
-		if (event_data->migration.transfer_mode ==
-		    UVM_VA_BLOCK_TRANSFER_MODE_COPY) {
-			UvmEventReadDuplicateInfo *info_read_duplicate =
-			    &entry.eventData.readDuplicate;
-			memset(&entry, 0, sizeof(entry));
-
-			info_read_duplicate->eventType =
-			    UvmEventTypeReadDuplicate;
-			info_read_duplicate->size = PAGE_SIZE;
-			info_read_duplicate->timeStamp = begin_time_stamp;
-
-			for_each_va_block_page_in_region(page_index, region) {
-				uvm_processor_id_t id;
-				uvm_processor_mask_t resident_processors;
-
-				info_read_duplicate->address =
-				    va_block->start + page_index * PAGE_SIZE;
-				info_read_duplicate->processors = 0;
-
-				uvm_va_block_page_resident_processors(va_block,
-								      page_index,
-								      &resident_processors);
-				for_each_id_in_mask(id, &resident_processors)
-				    info_read_duplicate->processors |= (1 < id);
-
-				uvm_tools_record_event(va_space, &entry);
-			}
-		} else {
-			UvmEventReadDuplicateInvalidateInfo *info =
-			    &entry.eventData.readDuplicateInvalidate;
-			memset(&entry, 0, sizeof(entry));
-
-			info->eventType = UvmEventTypeReadDuplicateInvalidate;
-			info->residentIndex = event_data->migration.dst;
-			info->size = PAGE_SIZE;
-			info->timeStamp = begin_time_stamp;
-
-			for_each_va_block_page_in_region(page_index, region) {
-				if (test_bit
-				    (page_index,
-				     va_block->read_duplicated_pages)) {
-					info->address =
-					    va_block->start +
-					    page_index * PAGE_SIZE;
-
-					uvm_tools_record_event(va_space,
-							       &entry);
-				}
-			}
-		}
-	}
+    block_mig->end_timestamp_cpu = NV_GETTIME();
+    block_mig->start_timestamp_gpu = *block_mig->start_timestamp_gpu_addr;
+    list_for_each_entry(mig, &block_mig->events, events_node)
+        mig->end_timestamp_gpu = *mig->end_timestamp_gpu_addr;
+
+    nv_kthread_q_item_init(&block_mig->queue_item, record_migration_events, block_mig);
+
+    // The UVM driver may notice that work in a channel is complete in a variety of situations
+    // and the va_space lock is not always held in all of them, nor can it always be taken safely on them.
+    // Dispatching events requires the va_space lock to be held in at least read mode, so
+    // this callback simply enqueues the dispatching onto a queue, where the
+    // va_space lock is always safe to acquire.
+    uvm_spin_lock(&block_mig->va_space->tools.channel_list_lock);
+    remove_pending_event_for_channel(block_mig->va_space, block_mig->channel_entry);
+    nv_kthread_q_schedule_q_item(&g_tools_queue, &block_mig->queue_item);
+    uvm_spin_unlock(&block_mig->va_space->tools.channel_list_lock);
+}
+
+static void uvm_tools_record_migration(uvm_perf_event_t event_id, uvm_perf_event_data_t *event_data)
+{
+    uvm_va_block_t *va_block = event_data->migration.block;
+    uvm_va_space_t *va_space = va_block->va_range->va_space;
+
+    UVM_ASSERT(event_id == UVM_PERF_EVENT_MIGRATION);
+
+    uvm_assert_mutex_locked(&va_block->lock);
+    uvm_assert_rwsem_locked(&va_space->perf_events.lock);
+
+    if (va_space->tools.enabled) {
+        UvmEventEntry entry;
+        migration_data_t *mig;
+        uvm_push_info_t *push_info = uvm_push_info_from_push(event_data->migration.push);
+        block_migration_data_t *block_mig = (block_migration_data_t *)push_info->on_complete_data;
+        NvU64 begin_time_stamp = NV_GETTIME();
+        size_t page_index;
+        uvm_va_block_region_t region = uvm_va_block_region_from_start_size(va_block,
+                                                                           event_data->migration.address,
+                                                                           event_data->migration.bytes);
+
+        // Increment counters
+        if (event_data->migration.src == UVM_CPU_ID) {
+            uvm_gpu_t *gpu = uvm_gpu_get(event_data->migration.dst);
+            uvm_tools_inc_counter(va_space, UvmCounterNameBytesXferHtD, event_data->migration.bytes, &gpu->uuid);
+        }
+        else if (event_data->migration.dst == UVM_CPU_ID) {
+            uvm_gpu_t *gpu = uvm_gpu_get(event_data->migration.src);
+            uvm_tools_inc_counter(va_space, UvmCounterNameBytesXferDtH, event_data->migration.bytes, &gpu->uuid);
+        }
+
+        if (push_info->on_complete != NULL) {
+            mig = kmem_cache_alloc(g_tools_migration_data_cache, NV_UVM_GFP_FLAGS);
+            if (mig == NULL)
+                return;
+
+            mig->address = event_data->migration.address;
+            mig->bytes = event_data->migration.bytes;
+            mig->end_timestamp_gpu_addr = uvm_push_timestamp(event_data->migration.push);
+            mig->cause = event_data->migration.cause;
+
+            list_add_tail(&mig->events_node, &block_mig->events);
+        }
+
+        // Read-duplication events
+        if (event_data->migration.transfer_mode == UVM_VA_BLOCK_TRANSFER_MODE_COPY) {
+            UvmEventReadDuplicateInfo *info_read_duplicate = &entry.eventData.readDuplicate;
+            memset(&entry, 0, sizeof(entry));
+
+            info_read_duplicate->eventType = UvmEventTypeReadDuplicate;
+            info_read_duplicate->size      = PAGE_SIZE;
+            info_read_duplicate->timeStamp = begin_time_stamp;
+
+            for_each_va_block_page_in_region(page_index, region) {
+                uvm_processor_id_t id;
+                uvm_processor_mask_t resident_processors;
+
+                info_read_duplicate->address    = va_block->start + page_index * PAGE_SIZE;
+                info_read_duplicate->processors = 0;
+
+                uvm_va_block_page_resident_processors(va_block, page_index, &resident_processors);
+                for_each_id_in_mask(id, &resident_processors)
+                    info_read_duplicate->processors |= (1 < id);
+
+                uvm_tools_record_event(va_space, &entry);
+            }
+        }
+        else {
+            UvmEventReadDuplicateInvalidateInfo *info = &entry.eventData.readDuplicateInvalidate;
+            memset(&entry, 0, sizeof(entry));
+
+            info->eventType     = UvmEventTypeReadDuplicateInvalidate;
+            info->residentIndex = event_data->migration.dst;
+            info->size          = PAGE_SIZE;
+            info->timeStamp     = begin_time_stamp;
+
+            for_each_va_block_page_in_region(page_index, region) {
+                if (test_bit(page_index, va_block->read_duplicated_pages)) {
+                    info->address = va_block->start + page_index * PAGE_SIZE;
+
+                    uvm_tools_record_event(va_space, &entry);
+                }
+            }
+        }
+    }
 }
 
 void uvm_tools_broadcast_replay(uvm_gpu_id_t gpu_id, NvU32 batch_id)
 {
-	UvmEventEntry entry;
-	UvmEventGpuFaultReplayInfo *info = &entry.eventData.gpuFaultReplay;
-	memset(&entry, 0, sizeof(entry));
-
-	info->eventType = UvmEventTypeGpuFaultReplay;
-	info->gpuIndex = gpu_id;
-	info->batchId = batch_id;
-	info->timeStamp = NV_GETTIME();
-
-	uvm_tools_broadcast_event(&entry);
-}
-
-static void uvm_tools_record_block_migration_begin(uvm_perf_event_t event_id,
-						   uvm_perf_event_data_t *
-						   event_data)
-{
-	uvm_va_space_t *va_space;
-	uvm_range_group_range_t *range;
-	UVM_ASSERT(event_id == UVM_PERF_EVENT_BLOCK_MIGRATION_BEGIN);
-
-	va_space = event_data->migration.block->va_range->va_space;
-	uvm_assert_rwsem_locked(&va_space->perf_events.lock);
-
-	if (va_space->tools.enabled) {
-		block_migration_data_t *block_mig;
-		uvm_push_info_t *push_info =
-		    uvm_push_info_from_push(event_data->migration.push);
-
-		UVM_ASSERT(push_info->on_complete == NULL
-			   && push_info->on_complete_data == NULL);
-
-		block_mig =
-		    kmem_cache_alloc(g_tools_block_migration_data_cache,
-				     NV_UVM_GFP_FLAGS);
-		if (block_mig == NULL)
-			return;
-
-		block_mig->start_timestamp_gpu_addr =
-		    uvm_push_timestamp(event_data->migration.push);
-		block_mig->start_timestamp_cpu = NV_GETTIME();
-		block_mig->dst = event_data->migration.dst;
-		block_mig->src = event_data->migration.src;
-		block_mig->range_group_id = UVM_RANGE_GROUP_ID_NONE;
-
-		// During evictions, it is not safe to uvm_range_group_range_find() because the va_space lock is not held.
-		if (event_data->migration.cause !=
-		    UvmEventMigrationCauseEviction) {
-			range =
-			    uvm_range_group_range_find(va_space,
-						       event_data->migration.
-						       address);
-			if (range != NULL)
-				block_mig->range_group_id =
-				    range->range_group->id;
-		}
-		block_mig->cause = event_data->migration.cause;
-		block_mig->va_space = va_space;
-
-		INIT_LIST_HEAD(&block_mig->events);
-		push_info->on_complete_data = block_mig;
-		push_info->on_complete = on_block_migration_complete;
-
-		// Set-up channel-oriented state
-		block_mig->self_channel_entry.parent_alive = true;
-		block_mig->self_channel_entry.pending_event_count = 0;
-		block_mig->self_channel_entry.channel =
-		    event_data->migration.push->channel;
-
-		uvm_spin_lock(&va_space->tools.channel_list_lock);
-		add_pending_event_for_channel(va_space, block_mig);
-		uvm_spin_unlock(&va_space->tools.channel_list_lock);
-	}
-}
-
-void uvm_tools_schedule_completed_events(uvm_va_space_t * va_space)
-{
-	tools_channel_entry_t *channel_entry;
-	tools_channel_entry_t *next_channel_entry;
-	NvU64 channel_count = 0;
-	NvU64 i;
-
-	uvm_assert_rwsem_locked(&va_space->lock);
-
-	uvm_spin_lock(&va_space->tools.channel_list_lock);
-
-	// retain every channel list entry currently in the list and keep track of their count.
-	list_for_each_entry(channel_entry, &va_space->tools.channel_list,
-			    channel_list_node) {
-		channel_entry->pending_event_count++;
-		channel_count++;
-	}
-	uvm_spin_unlock(&va_space->tools.channel_list_lock);
-
-	if (channel_count == 0)
-		return;
-
-	// new entries always appear at the end, and all the entries seen in the first loop have been retained
-	// so it is safe to go through them
-	channel_entry =
-	    list_first_entry(&va_space->tools.channel_list,
-			     tools_channel_entry_t, channel_list_node);
-	for (i = 0; i < channel_count; i++) {
-		uvm_channel_update_progress_all(channel_entry->channel);
-		channel_entry =
-		    list_next_entry(channel_entry, channel_list_node);
-	}
-
-	// now release all the entries we retained in the beginning
-	i = 0;
-	uvm_spin_lock(&va_space->tools.channel_list_lock);
-	list_for_each_entry_safe(channel_entry, next_channel_entry,
-				 &va_space->tools.channel_list,
-				 channel_list_node) {
-		if (i++ == channel_count)
-			break;
-
-		remove_pending_event_for_channel(va_space, channel_entry);
-	}
-	uvm_spin_unlock(&va_space->tools.channel_list_lock);
+    UvmEventEntry entry;
+    UvmEventGpuFaultReplayInfo *info = &entry.eventData.gpuFaultReplay;
+    memset(&entry, 0, sizeof(entry));
+
+    info->eventType = UvmEventTypeGpuFaultReplay;
+    info->gpuIndex  = gpu_id;
+    info->batchId   = batch_id;
+    info->timeStamp = NV_GETTIME();
+
+    uvm_tools_broadcast_event(&entry);
+}
+
+void uvm_tools_record_block_migration_begin(uvm_va_block_t *va_block,
+                                            uvm_push_t *push,
+                                            uvm_processor_id_t dst_id,
+                                            uvm_processor_id_t src_id,
+                                            NvU64 start,
+                                            UvmEventMigrationCause cause)
+{
+    uvm_va_space_t *va_space;
+    uvm_range_group_range_t *range;
+
+    va_space = va_block->va_range->va_space;
+    // During evictions the va_space lock is not held.
+    if (cause != UvmEventMigrationCauseEviction)
+        uvm_assert_rwsem_locked(&va_space->lock);
+
+    if (va_space->tools.enabled) {
+        block_migration_data_t *block_mig;
+        uvm_push_info_t *push_info = uvm_push_info_from_push(push);
+
+        UVM_ASSERT(push_info->on_complete == NULL && push_info->on_complete_data == NULL);
+
+        block_mig = kmem_cache_alloc(g_tools_block_migration_data_cache, NV_UVM_GFP_FLAGS);
+        if (block_mig == NULL)
+            return;
+
+        block_mig->start_timestamp_gpu_addr = uvm_push_timestamp(push);
+        block_mig->start_timestamp_cpu = NV_GETTIME();
+        block_mig->dst = dst_id;
+        block_mig->src = src_id;
+        block_mig->range_group_id = UVM_RANGE_GROUP_ID_NONE;
+
+        // During evictions, it is not safe to uvm_range_group_range_find() because the va_space lock is not held.
+        if (cause != UvmEventMigrationCauseEviction) {
+            range = uvm_range_group_range_find(va_space, start);
+            if (range != NULL)
+                block_mig->range_group_id = range->range_group->id;
+        }
+        block_mig->va_space = va_space;
+
+        INIT_LIST_HEAD(&block_mig->events);
+        push_info->on_complete_data = block_mig;
+        push_info->on_complete = on_block_migration_complete;
+
+        // Set-up channel-oriented state
+        block_mig->self_channel_entry.parent_alive = true;
+        block_mig->self_channel_entry.pending_event_count = 0;
+        block_mig->self_channel_entry.channel = push->channel;
+
+        uvm_spin_lock(&va_space->tools.channel_list_lock);
+        add_pending_event_for_channel(va_space, block_mig);
+        uvm_spin_unlock(&va_space->tools.channel_list_lock);
+    }
+}
+
+void uvm_tools_schedule_completed_events(uvm_va_space_t *va_space)
+{
+    tools_channel_entry_t *channel_entry;
+    tools_channel_entry_t *next_channel_entry;
+    NvU64 channel_count = 0;
+    NvU64 i;
+
+    uvm_assert_rwsem_locked(&va_space->lock);
+
+    uvm_spin_lock(&va_space->tools.channel_list_lock);
+
+    // retain every channel list entry currently in the list and keep track of their count.
+    list_for_each_entry(channel_entry, &va_space->tools.channel_list, channel_list_node) {
+        channel_entry->pending_event_count++;
+        channel_count++;
+    }
+    uvm_spin_unlock(&va_space->tools.channel_list_lock);
+
+    if (channel_count == 0)
+        return;
+
+    // new entries always appear at the end, and all the entries seen in the first loop have been retained
+    // so it is safe to go through them
+    channel_entry = list_first_entry(&va_space->tools.channel_list, tools_channel_entry_t, channel_list_node);
+    for (i = 0; i < channel_count; i++) {
+        uvm_channel_update_progress_all(channel_entry->channel);
+        channel_entry = list_next_entry(channel_entry, channel_list_node);
+    }
+
+    // now release all the entries we retained in the beginning
+    i = 0;
+    uvm_spin_lock(&va_space->tools.channel_list_lock);
+    list_for_each_entry_safe(channel_entry, next_channel_entry, &va_space->tools.channel_list, channel_list_node) {
+        if (i++ == channel_count)
+            break;
+
+        remove_pending_event_for_channel(va_space, channel_entry);
+    }
+    uvm_spin_unlock(&va_space->tools.channel_list_lock);
 }
 
 // TODO: Bug 1760246: Temporary workaround to start recording replay events. The
 //       final implementation should provide a VA space broadcast mechanism.
-void uvm_tools_record_replay(uvm_gpu_id_t gpu_id, uvm_va_space_t * va_space,
-			     NvU32 batch_id)
+void uvm_tools_record_replay(uvm_gpu_id_t gpu_id, uvm_va_space_t *va_space, NvU32 batch_id)
 {
-	uvm_assert_rwsem_locked(&va_space->lock);
+    uvm_assert_rwsem_locked(&va_space->lock);
+
+    if (va_space->tools.enabled) {
+        UvmEventEntry entry;
+        UvmEventGpuFaultReplayInfo *info = &entry.eventData.gpuFaultReplay;
+
+        memset(&entry, 0, sizeof(entry));
 
-	if (va_space->tools.enabled) {
-		UvmEventEntry entry;
-		UvmEventGpuFaultReplayInfo *info =
-		    &entry.eventData.gpuFaultReplay;
-
-		memset(&entry, 0, sizeof(entry));
-
-		info->eventType = UvmEventTypeGpuFaultReplay;
-		info->gpuIndex = gpu_id;
-		info->batchId = batch_id;
-		info->timeStamp = NV_GETTIME();
-
-		uvm_down_read(&va_space->perf_events.lock);
-		uvm_tools_record_event(va_space, &entry);
-		uvm_up_read(&va_space->perf_events.lock);
-	}
-}
-
-void uvm_tools_record_cpu_fatal_fault(uvm_va_space_t * va_space, NvU64 address,
-				      bool is_write, UvmEventFatalReason reason)
-{
-	uvm_assert_rwsem_locked(&va_space->lock);
-
-	if (va_space->tools.enabled) {
-		UvmEventEntry entry;
-		UvmEventFatalFaultInfo *info = &entry.eventData.fatalFault;
-		memset(&entry, 0, sizeof(entry));
-
-		info->eventType = UvmEventTypeFatalFault;
-		info->processorIndex = UVM_CPU_ID;
-		info->timeStamp = NV_GETTIME();
-		info->address = address;
-		info->accessType =
-		    is_write ? UvmEventMemoryAccessTypeWrite :
-		    UvmEventMemoryAccessTypeRead;
-		// info->faultType is not valid for cpu faults
-		info->reason = reason;
-
-		uvm_down_read(&va_space->perf_events.lock);
-		uvm_tools_record_event(va_space, &entry);
-		uvm_up_read(&va_space->perf_events.lock);
-	}
+        info->eventType = UvmEventTypeGpuFaultReplay;
+        info->gpuIndex  = gpu_id;
+        info->batchId   = batch_id;
+        info->timeStamp = NV_GETTIME();
+
+        uvm_down_read(&va_space->perf_events.lock);
+        uvm_tools_record_event(va_space, &entry);
+        uvm_up_read(&va_space->perf_events.lock);
+    }
+}
+
+void uvm_tools_record_cpu_fatal_fault(uvm_va_space_t *va_space, NvU64 address, bool is_write,
+                                      UvmEventFatalReason reason)
+{
+    uvm_assert_rwsem_locked(&va_space->lock);
+
+    if (va_space->tools.enabled) {
+        UvmEventEntry entry;
+        UvmEventFatalFaultInfo *info = &entry.eventData.fatalFault;
+        memset(&entry, 0, sizeof(entry));
+
+        info->eventType = UvmEventTypeFatalFault;
+        info->processorIndex = UVM_CPU_ID;
+        info->timeStamp = NV_GETTIME();
+        info->address = address;
+        info->accessType = is_write? UvmEventMemoryAccessTypeWrite: UvmEventMemoryAccessTypeRead;
+        // info->faultType is not valid for cpu faults
+        info->reason = reason;
+
+        uvm_down_read(&va_space->perf_events.lock);
+        uvm_tools_record_event(va_space, &entry);
+        uvm_up_read(&va_space->perf_events.lock);
+    }
 }
 
 void uvm_tools_record_gpu_fatal_fault(uvm_gpu_id_t gpu_id,
-				      uvm_va_space_t * va_space,
-				      uvm_fault_buffer_entry_t * buffer_entry,
-				      UvmEventFatalReason reason)
+                                      uvm_va_space_t *va_space,
+                                      uvm_fault_buffer_entry_t *buffer_entry,
+                                      UvmEventFatalReason reason)
 {
-	uvm_assert_rwsem_locked(&va_space->lock);
+    uvm_assert_rwsem_locked(&va_space->lock);
 
-	if (va_space->tools.enabled) {
-		UvmEventEntry entry;
-		UvmEventFatalFaultInfo *info = &entry.eventData.fatalFault;
-		memset(&entry, 0, sizeof(entry));
+    if (va_space->tools.enabled) {
+        UvmEventEntry entry;
+        UvmEventFatalFaultInfo *info = &entry.eventData.fatalFault;
+        memset(&entry, 0, sizeof(entry));
 
-		info->eventType = UvmEventTypeFatalFault;
-		info->processorIndex = gpu_id;
-		info->timeStamp = NV_GETTIME();
-		info->address = buffer_entry->fault_address;
-		info->accessType =
-		    g_hal_to_tools_fault_access_type_table[buffer_entry->
-							   fault_access_type];
-		info->faultType =
-		    g_hal_to_tools_fault_type_table[buffer_entry->fault_type];
-		info->reason = reason;
+        info->eventType = UvmEventTypeFatalFault;
+        info->processorIndex = gpu_id;
+        info->timeStamp = NV_GETTIME();
+        info->address = buffer_entry->fault_address;
+        info->accessType = g_hal_to_tools_fault_access_type_table[buffer_entry->fault_access_type];
+        info->faultType = g_hal_to_tools_fault_type_table[buffer_entry->fault_type];
+        info->reason = reason;
 
-		uvm_down_read(&va_space->perf_events.lock);
-		uvm_tools_record_event(va_space, &entry);
-		uvm_up_read(&va_space->perf_events.lock);
-	}
+        uvm_down_read(&va_space->perf_events.lock);
+        uvm_tools_record_event(va_space, &entry);
+        uvm_up_read(&va_space->perf_events.lock);
+    }
 }
 
-void uvm_tools_record_thrashing(uvm_va_block_t * va_block, NvU64 address,
-				size_t region_size,
-				const uvm_processor_mask_t * processors)
+void uvm_tools_record_thrashing(uvm_va_block_t *va_block, NvU64 address, size_t region_size,
+                                const uvm_processor_mask_t *processors)
 {
-	uvm_va_space_t *va_space = va_block->va_range->va_space;
+    uvm_va_space_t *va_space = va_block->va_range->va_space;
 
-	UVM_ASSERT(address);
-	UVM_ASSERT(PAGE_ALIGNED(address));
-	UVM_ASSERT(region_size > 0);
+    UVM_ASSERT(address);
+    UVM_ASSERT(PAGE_ALIGNED(address));
+    UVM_ASSERT(region_size > 0);
 
-	uvm_assert_rwsem_locked(&va_space->lock);
-	uvm_assert_rwsem_locked(&va_space->perf_events.lock);
+    uvm_assert_rwsem_locked(&va_space->lock);
+    uvm_assert_rwsem_locked(&va_space->perf_events.lock);
 
-	if (va_space->tools.enabled) {
-		UvmEventEntry entry;
-		UvmEventThrashingDetectedInfo *info =
-		    &entry.eventData.thrashing;
-		memset(&entry, 0, sizeof(entry));
+    if (va_space->tools.enabled) {
+        UvmEventEntry entry;
+        UvmEventThrashingDetectedInfo *info = &entry.eventData.thrashing;
+        memset(&entry, 0, sizeof(entry));
 
-		info->eventType = UvmEventTypeThrashingDetected;
-		info->address = address;
-		info->size = region_size;
-		info->timeStamp = NV_GETTIME();
-		bitmap_copy((long unsigned *)&info->processors,
-			    processors->bitmap, UVM8_MAX_PROCESSORS);
+        info->eventType = UvmEventTypeThrashingDetected;
+        info->address   = address;
+        info->size      = region_size;
+        info->timeStamp = NV_GETTIME();
+        bitmap_copy((long unsigned *)&info->processors, processors->bitmap, UVM8_MAX_PROCESSORS);
 
-		uvm_tools_record_event(va_space, &entry);
-	}
+        uvm_tools_record_event(va_space, &entry);
+    }
 }
 
-void uvm_tools_record_throttling_start(uvm_va_block_t * va_block, NvU64 address,
-				       uvm_processor_id_t processor)
+void uvm_tools_record_throttling_start(uvm_va_block_t *va_block, NvU64 address, uvm_processor_id_t processor)
 {
-	uvm_va_space_t *va_space = va_block->va_range->va_space;
+    uvm_va_space_t *va_space = va_block->va_range->va_space;
 
-	UVM_ASSERT(address);
-	UVM_ASSERT(PAGE_ALIGNED(address));
-	UVM_ASSERT(processor < UVM8_MAX_PROCESSORS);
+    UVM_ASSERT(address);
+    UVM_ASSERT(PAGE_ALIGNED(address));
+    UVM_ASSERT(processor < UVM8_MAX_PROCESSORS);
 
-	uvm_assert_rwsem_locked(&va_space->lock);
+    uvm_assert_rwsem_locked(&va_space->lock);
 
-	if (va_space->tools.enabled) {
-		UvmEventEntry entry;
-		UvmEventThrottlingStartInfo *info =
-		    &entry.eventData.throttlingStart;
-		memset(&entry, 0, sizeof(entry));
+    if (va_space->tools.enabled) {
+        UvmEventEntry entry;
+        UvmEventThrottlingStartInfo *info = &entry.eventData.throttlingStart;
+        memset(&entry, 0, sizeof(entry));
 
-		info->eventType = UvmEventTypeThrottlingStart;
-		info->processorIndex = processor;
-		info->address = address;
-		info->timeStamp = NV_GETTIME();
+        info->eventType      = UvmEventTypeThrottlingStart;
+        info->processorIndex = processor;
+        info->address        = address;
+        info->timeStamp      = NV_GETTIME();
 
-		uvm_down_read(&va_space->perf_events.lock);
-		uvm_tools_record_event(va_space, &entry);
-		uvm_up_read(&va_space->perf_events.lock);
-	}
+        uvm_down_read(&va_space->perf_events.lock);
+        uvm_tools_record_event(va_space, &entry);
+        uvm_up_read(&va_space->perf_events.lock);
+    }
 }
 
-void uvm_tools_record_throttling_end(uvm_va_block_t * va_block, NvU64 address,
-				     uvm_processor_id_t processor)
+void uvm_tools_record_throttling_end(uvm_va_block_t *va_block, NvU64 address, uvm_processor_id_t processor)
 {
-	uvm_va_space_t *va_space = va_block->va_range->va_space;
+    uvm_va_space_t *va_space = va_block->va_range->va_space;
 
-	UVM_ASSERT(address);
-	UVM_ASSERT(PAGE_ALIGNED(address));
-	UVM_ASSERT(processor < UVM8_MAX_PROCESSORS);
+    UVM_ASSERT(address);
+    UVM_ASSERT(PAGE_ALIGNED(address));
+    UVM_ASSERT(processor < UVM8_MAX_PROCESSORS);
 
-	uvm_assert_rwsem_locked(&va_space->lock);
+    uvm_assert_rwsem_locked(&va_space->lock);
 
-	if (va_space->tools.enabled) {
-		UvmEventEntry entry;
-		UvmEventThrottlingEndInfo *info =
-		    &entry.eventData.throttlingEnd;
-		memset(&entry, 0, sizeof(entry));
+    if (va_space->tools.enabled) {
+        UvmEventEntry entry;
+        UvmEventThrottlingEndInfo *info = &entry.eventData.throttlingEnd;
+        memset(&entry, 0, sizeof(entry));
 
-		info->eventType = UvmEventTypeThrottlingEnd;
-		info->processorIndex = processor;
-		info->address = address;
-		info->timeStamp = NV_GETTIME();
+        info->eventType      = UvmEventTypeThrottlingEnd;
+        info->processorIndex = processor;
+        info->address        = address;
+        info->timeStamp      = NV_GETTIME();
 
-		uvm_down_read(&va_space->perf_events.lock);
-		uvm_tools_record_event(va_space, &entry);
-		uvm_up_read(&va_space->perf_events.lock);
-	}
+        uvm_down_read(&va_space->perf_events.lock);
+        uvm_tools_record_event(va_space, &entry);
+        uvm_up_read(&va_space->perf_events.lock);
+    }
 }
 
-void uvm_tools_record_map_remote(uvm_va_block_t * va_block,
-				 uvm_processor_id_t processor,
-				 uvm_processor_id_t residency, NvU64 address,
-				 size_t region_size,
-				 UvmEventMapRemoteCause cause)
+void uvm_tools_record_map_remote(uvm_va_block_t *va_block, uvm_processor_id_t processor, uvm_processor_id_t residency,
+                                 NvU64 address, size_t region_size, UvmEventMapRemoteCause cause)
 {
-	uvm_va_space_t *va_space = va_block->va_range->va_space;
+    uvm_va_space_t *va_space = va_block->va_range->va_space;
 
-	UVM_ASSERT(processor < UVM8_MAX_PROCESSORS);
-	UVM_ASSERT(residency < UVM8_MAX_PROCESSORS);
+    UVM_ASSERT(processor < UVM8_MAX_PROCESSORS);
+    UVM_ASSERT(residency < UVM8_MAX_PROCESSORS);
 
-	uvm_assert_rwsem_locked(&va_space->lock);
+    uvm_assert_rwsem_locked(&va_space->lock);
 
-	if (va_space->tools.enabled) {
-		UvmEventEntry entry;
-		UvmEventMapRemoteInfo *info = &entry.eventData.mapRemote;
-		memset(&entry, 0, sizeof(entry));
+    if (va_space->tools.enabled) {
+        UvmEventEntry entry;
+        UvmEventMapRemoteInfo *info = &entry.eventData.mapRemote;
+        memset(&entry, 0, sizeof(entry));
 
-		info->eventType = UvmEventTypeMapRemote;
-		info->srcIndex = processor;
-		info->dstIndex = residency;
-		info->address = address;
-		info->mapRemoteCause = cause;
-		info->size = region_size;
-		info->timeStamp = NV_GETTIME();
-		// TODO: Bug 200194638: compute GPU time stamp reliably
-		info->timeStampGpu = 0;
+        info->eventType      = UvmEventTypeMapRemote;
+        info->srcIndex       = processor;
+        info->dstIndex       = residency;
+        info->address        = address;
+        info->mapRemoteCause = cause;
+        info->size           = region_size;
+        info->timeStamp      = NV_GETTIME();
+        // TODO: Bug 200194638: compute GPU time stamp reliably
+        info->timeStampGpu   = 0;
 
-		uvm_down_read(&va_space->perf_events.lock);
-		uvm_tools_record_event(va_space, &entry);
-		uvm_up_read(&va_space->perf_events.lock);
-	}
+        uvm_down_read(&va_space->perf_events.lock);
+        uvm_tools_record_event(va_space, &entry);
+        uvm_up_read(&va_space->perf_events.lock);
+    }
 }
 
-NV_STATUS uvm_api_tools_init_event_tracker(UVM_TOOLS_INIT_EVENT_TRACKER_PARAMS *
-					   params, struct file *filp)
+NV_STATUS uvm_api_tools_init_event_tracker(UVM_TOOLS_INIT_EVENT_TRACKER_PARAMS *params, struct file *filp)
 {
-	NV_STATUS status = NV_OK;
-	uvm_tools_event_tracker_t *event_tracker;
+    NV_STATUS status = NV_OK;
+    uvm_tools_event_tracker_t *event_tracker;
 
-	event_tracker =
-	    kmem_cache_zalloc(g_tools_event_tracker_cache, NV_UVM_GFP_FLAGS);
-	if (event_tracker == NULL)
-		return NV_ERR_NO_MEMORY;
+    event_tracker = kmem_cache_zalloc(g_tools_event_tracker_cache, NV_UVM_GFP_FLAGS);
+    if (event_tracker == NULL)
+        return NV_ERR_NO_MEMORY;
 
-	event_tracker->uvm_file = fget(params->uvmFd);
-	if (event_tracker->uvm_file == NULL) {
-		status = NV_ERR_INSUFFICIENT_PERMISSIONS;
-		goto fail;
-	}
+    event_tracker->uvm_file = fget(params->uvmFd);
+    if (event_tracker->uvm_file == NULL) {
+        status = NV_ERR_INSUFFICIENT_PERMISSIONS;
+        goto fail;
+    }
 
-	if (!file_is_nvidia_uvm(event_tracker->uvm_file)) {
-		fput(event_tracker->uvm_file);
-		event_tracker->uvm_file = NULL;
-		status = NV_ERR_INSUFFICIENT_PERMISSIONS;
-		goto fail;
-	}
+    if (!file_is_nvidia_uvm(event_tracker->uvm_file)) {
+        fput(event_tracker->uvm_file);
+        event_tracker->uvm_file = NULL;
+        status = NV_ERR_INSUFFICIENT_PERMISSIONS;
+        goto fail;
+    }
 
-	event_tracker->is_queue = params->queueBufferSize != 0;
-	if (event_tracker->is_queue) {
-		uvm_tools_queue_t *queue = &event_tracker->queue;
-		uvm_spin_lock_init(&queue->lock, UVM_LOCK_ORDER_LEAF);
-		init_waitqueue_head(&queue->wait_queue);
+    event_tracker->is_queue = params->queueBufferSize != 0;
+    if (event_tracker->is_queue) {
+        uvm_tools_queue_t *queue = &event_tracker->queue;
+        uvm_spin_lock_init(&queue->lock, UVM_LOCK_ORDER_LEAF);
+        init_waitqueue_head(&queue->wait_queue);
 
-		if (params->queueBufferSize > UINT_MAX) {
-			status = NV_ERR_INVALID_ARGUMENT;
-			goto fail;
-		}
+        if (params->queueBufferSize > UINT_MAX) {
+            status = NV_ERR_INVALID_ARGUMENT;
+            goto fail;
+        }
 
-		queue->queue_buffer_count = (NvU32) params->queueBufferSize;
-		queue->notification_threshold = queue->queue_buffer_count / 2;
+        queue->queue_buffer_count = (NvU32)params->queueBufferSize;
+        queue->notification_threshold = queue->queue_buffer_count / 2;
 
-		// queue_buffer_count must be a power of 2, of at least 2
-		if (!is_power_of_2(queue->queue_buffer_count)
-		    || queue->queue_buffer_count < 2) {
-			status = NV_ERR_INVALID_ARGUMENT;
-			goto fail;
-		}
+        // queue_buffer_count must be a power of 2, of at least 2
+        if (!is_power_of_2(queue->queue_buffer_count) || queue->queue_buffer_count < 2) {
+            status = NV_ERR_INVALID_ARGUMENT;
+            goto fail;
+        }
 
-		status = map_user_pages(params->queueBuffer,
-					queue->queue_buffer_count *
-					sizeof(UvmEventEntry),
-					(void **)&queue->queue,
-					&queue->queue_buffer_pages);
-		if (status != NV_OK)
-			goto fail;
+        status = map_user_pages(params->queueBuffer,
+                                queue->queue_buffer_count * sizeof(UvmEventEntry),
+                                (void **)&queue->queue,
+                                &queue->queue_buffer_pages);
+        if (status != NV_OK)
+            goto fail;
 
-		status = map_user_pages(params->controlBuffer,
-					sizeof(UvmToolsEventControlData),
-					(void **)&queue->control,
-					&queue->control_buffer_pages);
+        status = map_user_pages(params->controlBuffer,
+                                sizeof(UvmToolsEventControlData),
+                                (void **)&queue->control,
+                                &queue->control_buffer_pages);
 
-		if (status != NV_OK)
-			goto fail;
-	} else {
-		uvm_tools_counter_t *counter = &event_tracker->counter;
-		counter->all_processors = params->allProcessors;
-		counter->processor = params->processor;
-		status = map_user_pages(params->controlBuffer,
-					sizeof(NvU64) * UVM_TOTAL_COUNTERS,
-					(void **)&counter->counters,
-					&counter->counter_buffer_pages);
-		if (status != NV_OK)
-			goto fail;
-	}
+        if (status != NV_OK)
+            goto fail;
+    }
+    else {
+        uvm_tools_counter_t *counter = &event_tracker->counter;
+        counter->all_processors = params->allProcessors;
+        counter->processor = params->processor;
+        status = map_user_pages(params->controlBuffer,
+                                sizeof(NvU64) * UVM_TOTAL_COUNTERS,
+                                (void **)&counter->counters,
+                                &counter->counter_buffer_pages);
+        if (status != NV_OK)
+            goto fail;
+    }
 
-	if (nv_atomic_long_cmpxchg
-	    ((atomic_long_t *) & filp->private_data, 0,
-	     (long)event_tracker) != 0) {
-		status = NV_ERR_INVALID_ARGUMENT;
-		goto fail;
-	}
+    if (nv_atomic_long_cmpxchg((atomic_long_t *)&filp->private_data, 0, (long)event_tracker) != 0) {
+        status = NV_ERR_INVALID_ARGUMENT;
+        goto fail;
+    }
 
-	return NV_OK;
+    return NV_OK;
 
- fail:
-	destroy_event_tracker(event_tracker);
-	return status;
+fail:
+    destroy_event_tracker(event_tracker);
+    return status;
 }
 
-NV_STATUS
-uvm_api_tools_set_notification_threshold
-(UVM_TOOLS_SET_NOTIFICATION_THRESHOLD_PARAMS * params, struct file * filp)
+NV_STATUS uvm_api_tools_set_notification_threshold(UVM_TOOLS_SET_NOTIFICATION_THRESHOLD_PARAMS *params, struct file *filp)
 {
-	UvmToolsEventControlData *ctrl;
-	uvm_tools_queue_snapshot_t sn;
-	uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
+    UvmToolsEventControlData *ctrl;
+    uvm_tools_queue_snapshot_t sn;
+    uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
 
-	if (!tracker_is_queue(event_tracker))
-		return NV_ERR_INVALID_ARGUMENT;
+    if (!tracker_is_queue(event_tracker))
+        return NV_ERR_INVALID_ARGUMENT;
 
-	uvm_spin_lock(&event_tracker->queue.lock);
+    uvm_spin_lock(&event_tracker->queue.lock);
 
-	event_tracker->queue.notification_threshold =
-	    params->notificationThreshold;
+    event_tracker->queue.notification_threshold = params->notificationThreshold;
 
-	ctrl = event_tracker->queue.control;
-	sn.put_behind = atomic_read((atomic_t *) & ctrl->put_behind);
-	sn.get_ahead = atomic_read((atomic_t *) & ctrl->get_ahead);
+    ctrl = event_tracker->queue.control;
+    sn.put_behind = atomic_read((atomic_t *)&ctrl->put_behind);
+    sn.get_ahead = atomic_read((atomic_t *)&ctrl->get_ahead);
 
-	if (queue_needs_wakeup(&event_tracker->queue, &sn))
-		wake_up_all(&event_tracker->queue.wait_queue);
+    if (queue_needs_wakeup(&event_tracker->queue, &sn))
+        wake_up_all(&event_tracker->queue.wait_queue);
 
-	uvm_spin_unlock(&event_tracker->queue.lock);
+    uvm_spin_unlock(&event_tracker->queue.lock);
 
-	return NV_OK;
+    return NV_OK;
 }
 
-static NV_STATUS tools_register_perf_events(uvm_va_space_t * va_space)
+static NV_STATUS tools_register_perf_events(uvm_va_space_t *va_space)
 {
-	NV_STATUS status;
-
-	uvm_assert_rwsem_locked_write(&va_space->perf_events.lock);
+    NV_STATUS status;
 
-	status = uvm_perf_register_event_callback_locked(&va_space->perf_events,
-							 UVM_PERF_EVENT_FAULT,
-							 uvm_tools_record_fault);
-	if (status != NV_OK)
-		return status;
+    uvm_assert_rwsem_locked_write(&va_space->perf_events.lock);
 
-	status = uvm_perf_register_event_callback_locked(&va_space->perf_events,
-							 UVM_PERF_EVENT_MIGRATION,
-							 uvm_tools_record_migration);
-	if (status != NV_OK)
-		return status;
+    status = uvm_perf_register_event_callback_locked(&va_space->perf_events,
+                                                     UVM_PERF_EVENT_FAULT,
+                                                     uvm_tools_record_fault);
+    if (status != NV_OK)
+        return status;
 
-	status = uvm_perf_register_event_callback_locked(&va_space->perf_events,
-							 UVM_PERF_EVENT_BLOCK_MIGRATION_BEGIN,
-							 uvm_tools_record_block_migration_begin);
+    status = uvm_perf_register_event_callback_locked(&va_space->perf_events,
+                                                     UVM_PERF_EVENT_MIGRATION,
+                                                     uvm_tools_record_migration);
+    if (status != NV_OK)
+        return status;
 
-	if (status != NV_OK)
-		return status;
-
-	return NV_OK;
+    return NV_OK;
 }
 
-static void tools_unregister_perf_events(uvm_va_space_t * va_space)
+static void tools_unregister_perf_events(uvm_va_space_t *va_space)
 {
-	uvm_assert_rwsem_locked_write(&va_space->perf_events.lock);
-
-	uvm_perf_unregister_event_callback_locked(&va_space->perf_events,
-						  UVM_PERF_EVENT_FAULT,
-						  uvm_tools_record_fault);
+    uvm_assert_rwsem_locked_write(&va_space->perf_events.lock);
 
-	uvm_perf_unregister_event_callback_locked(&va_space->perf_events,
-						  UVM_PERF_EVENT_MIGRATION,
-						  uvm_tools_record_migration);
-
-	uvm_perf_unregister_event_callback_locked(&va_space->perf_events,
-						  UVM_PERF_EVENT_BLOCK_MIGRATION_BEGIN,
-						  uvm_tools_record_block_migration_begin);
+    uvm_perf_unregister_event_callback_locked(&va_space->perf_events,
+                                              UVM_PERF_EVENT_FAULT,
+                                              uvm_tools_record_fault);
 
+    uvm_perf_unregister_event_callback_locked(&va_space->perf_events,
+                                              UVM_PERF_EVENT_MIGRATION,
+                                              uvm_tools_record_migration);
 }
 
-static NV_STATUS tools_update_status(uvm_va_space_t * va_space)
+static NV_STATUS tools_update_status(uvm_va_space_t *va_space)
 {
-	bool should_be_enabled;
-	uvm_assert_rwsem_locked_write(&g_tools_va_space_list_lock);
-	uvm_assert_rwsem_locked_write(&va_space->perf_events.lock);
+    bool should_be_enabled;
+    uvm_assert_rwsem_locked_write(&g_tools_va_space_list_lock);
+    uvm_assert_rwsem_locked_write(&va_space->perf_events.lock);
 
-	should_be_enabled = tools_are_enabled(va_space);
-	if (should_be_enabled != va_space->tools.enabled) {
-		if (should_be_enabled) {
-			NV_STATUS status = tools_register_perf_events(va_space);
-			if (status != NV_OK)
-				return status;
+    should_be_enabled = tools_are_enabled(va_space);
+    if (should_be_enabled != va_space->tools.enabled) {
+        if (should_be_enabled) {
+            NV_STATUS status = tools_register_perf_events(va_space);
+            if (status != NV_OK)
+                return status;
 
-			list_add(&va_space->tools.node, &g_tools_va_space_list);
-		} else {
-			tools_unregister_perf_events(va_space);
-			list_del(&va_space->tools.node);
-		}
-		va_space->tools.enabled = should_be_enabled;
-	}
+            list_add(&va_space->tools.node, &g_tools_va_space_list);
+        }
+        else {
+            tools_unregister_perf_events(va_space);
+            list_del(&va_space->tools.node);
+        }
+        va_space->tools.enabled = should_be_enabled;
+    }
 
-	return NV_OK;
+    return NV_OK;
 }
 
-NV_STATUS
-uvm_api_tools_event_queue_enable_events
-(UVM_TOOLS_EVENT_QUEUE_ENABLE_EVENTS_PARAMS * params, struct file * filp)
+NV_STATUS uvm_api_tools_event_queue_enable_events(UVM_TOOLS_EVENT_QUEUE_ENABLE_EVENTS_PARAMS *params, struct file *filp)
 {
-	uvm_va_space_t *va_space;
-	uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
-	NV_STATUS status = NV_OK;
+    uvm_va_space_t *va_space;
+    uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
+    NV_STATUS status = NV_OK;
 
-	if (!tracker_is_queue(event_tracker))
-		return NV_ERR_INVALID_ARGUMENT;
+    if (!tracker_is_queue(event_tracker))
+        return NV_ERR_INVALID_ARGUMENT;
 
-	va_space = uvm_va_space_get(event_tracker->uvm_file);
+    va_space = uvm_va_space_get(event_tracker->uvm_file);
 
-	uvm_down_write(&g_tools_va_space_list_lock);
-	uvm_down_write(&va_space->perf_events.lock);
-	insert_event_tracker(event_tracker->queue.queue_nodes,
-			     UvmEventNumTypes,
-			     params->eventTypeFlags,
-			     &event_tracker->queue.subscribed_queues,
-			     va_space->tools.queues);
+    uvm_down_write(&g_tools_va_space_list_lock);
+    uvm_down_write(&va_space->perf_events.lock);
+    insert_event_tracker(event_tracker->queue.queue_nodes,
+                         UvmEventNumTypes,
+                         params->eventTypeFlags,
+                         &event_tracker->queue.subscribed_queues,
+                         va_space->tools.queues);
 
-	// perform any necessary registration
-	status = tools_update_status(va_space);
+    // perform any necessary registration
+    status = tools_update_status(va_space);
 
-	uvm_up_write(&va_space->perf_events.lock);
-	uvm_up_write(&g_tools_va_space_list_lock);
+    uvm_up_write(&va_space->perf_events.lock);
+    uvm_up_write(&g_tools_va_space_list_lock);
 
-	return status;
+    return status;
 }
 
-NV_STATUS
-uvm_api_tools_event_queue_disable_events
-(UVM_TOOLS_EVENT_QUEUE_DISABLE_EVENTS_PARAMS * params, struct file * filp)
+NV_STATUS uvm_api_tools_event_queue_disable_events(UVM_TOOLS_EVENT_QUEUE_DISABLE_EVENTS_PARAMS *params, struct file *filp)
 {
-	NV_STATUS status;
-	uvm_va_space_t *va_space;
-	uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
+    NV_STATUS status;
+    uvm_va_space_t *va_space;
+    uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
 
-	if (!tracker_is_queue(event_tracker))
-		return NV_ERR_INVALID_ARGUMENT;
+    if (!tracker_is_queue(event_tracker))
+        return NV_ERR_INVALID_ARGUMENT;
 
-	va_space = uvm_va_space_get(event_tracker->uvm_file);
+    va_space = uvm_va_space_get(event_tracker->uvm_file);
 
-	uvm_down_write(&g_tools_va_space_list_lock);
-	uvm_down_write(&va_space->perf_events.lock);
-	remove_event_tracker(event_tracker->queue.queue_nodes,
-			     UvmEventNumTypes,
-			     params->eventTypeFlags,
-			     &event_tracker->queue.subscribed_queues);
+    uvm_down_write(&g_tools_va_space_list_lock);
+    uvm_down_write(&va_space->perf_events.lock);
+    remove_event_tracker(event_tracker->queue.queue_nodes,
+                         UvmEventNumTypes,
+                         params->eventTypeFlags,
+                         &event_tracker->queue.subscribed_queues);
 
-	// de-registration should not fail
-	status = tools_update_status(va_space);
-	UVM_ASSERT(status == NV_OK);
+    // de-registration should not fail
+    status = tools_update_status(va_space);
+    UVM_ASSERT(status == NV_OK);
 
-	uvm_up_write(&va_space->perf_events.lock);
-	uvm_up_write(&g_tools_va_space_list_lock);
-	return NV_OK;
+    uvm_up_write(&va_space->perf_events.lock);
+    uvm_up_write(&g_tools_va_space_list_lock);
+    return NV_OK;
 }
 
-NV_STATUS uvm_api_tools_enable_counters(UVM_TOOLS_ENABLE_COUNTERS_PARAMS *
-					params, struct file * filp)
+NV_STATUS uvm_api_tools_enable_counters(UVM_TOOLS_ENABLE_COUNTERS_PARAMS *params, struct file *filp)
 {
-	uvm_va_space_t *va_space;
-	uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
-	NV_STATUS status = NV_OK;
+    uvm_va_space_t *va_space;
+    uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
+    NV_STATUS status = NV_OK;
 
-	if (!tracker_is_counter(event_tracker))
-		return NV_ERR_INVALID_ARGUMENT;
+    if (!tracker_is_counter(event_tracker))
+        return NV_ERR_INVALID_ARGUMENT;
 
-	va_space = uvm_va_space_get(event_tracker->uvm_file);
+    va_space = uvm_va_space_get(event_tracker->uvm_file);
 
-	uvm_down_write(&g_tools_va_space_list_lock);
-	uvm_down_write(&va_space->perf_events.lock);
-	insert_event_tracker(event_tracker->counter.counter_nodes,
-			     UVM_TOTAL_COUNTERS,
-			     params->counterTypeFlags,
-			     &event_tracker->counter.subscribed_counters,
-			     va_space->tools.counters);
+    uvm_down_write(&g_tools_va_space_list_lock);
+    uvm_down_write(&va_space->perf_events.lock);
+    insert_event_tracker(event_tracker->counter.counter_nodes,
+                         UVM_TOTAL_COUNTERS,
+                         params->counterTypeFlags,
+                         &event_tracker->counter.subscribed_counters,
+                         va_space->tools.counters);
 
-	status = tools_update_status(va_space);
+    status = tools_update_status(va_space);
 
-	uvm_up_write(&va_space->perf_events.lock);
-	uvm_up_write(&g_tools_va_space_list_lock);
+    uvm_up_write(&va_space->perf_events.lock);
+    uvm_up_write(&g_tools_va_space_list_lock);
 
-	return status;
+    return status;
 }
 
-NV_STATUS uvm_api_tools_disable_counters(UVM_TOOLS_DISABLE_COUNTERS_PARAMS *
-					 params, struct file * filp)
+NV_STATUS uvm_api_tools_disable_counters(UVM_TOOLS_DISABLE_COUNTERS_PARAMS *params, struct file *filp)
 {
-	NV_STATUS status;
-	uvm_va_space_t *va_space;
-	uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
+    NV_STATUS status;
+    uvm_va_space_t *va_space;
+    uvm_tools_event_tracker_t *event_tracker = tools_event_tracker(filp);
 
-	if (!tracker_is_counter(event_tracker))
-		return NV_ERR_INVALID_ARGUMENT;
+    if (!tracker_is_counter(event_tracker))
+        return NV_ERR_INVALID_ARGUMENT;
 
-	va_space = uvm_va_space_get(event_tracker->uvm_file);
+    va_space = uvm_va_space_get(event_tracker->uvm_file);
 
-	uvm_down_write(&g_tools_va_space_list_lock);
-	uvm_down_write(&va_space->perf_events.lock);
-	remove_event_tracker(event_tracker->counter.counter_nodes,
-			     UVM_TOTAL_COUNTERS,
-			     params->counterTypeFlags,
-			     &event_tracker->counter.subscribed_counters);
+    uvm_down_write(&g_tools_va_space_list_lock);
+    uvm_down_write(&va_space->perf_events.lock);
+    remove_event_tracker(event_tracker->counter.counter_nodes,
+                         UVM_TOTAL_COUNTERS,
+                         params->counterTypeFlags,
+                         &event_tracker->counter.subscribed_counters);
 
-	// de-registration should not fail
-	status = tools_update_status(va_space);
-	UVM_ASSERT(status == NV_OK);
+    // de-registration should not fail
+    status = tools_update_status(va_space);
+    UVM_ASSERT(status == NV_OK);
 
-	uvm_up_write(&va_space->perf_events.lock);
-	uvm_up_write(&g_tools_va_space_list_lock);
+    uvm_up_write(&va_space->perf_events.lock);
+    uvm_up_write(&g_tools_va_space_list_lock);
 
-	return NV_OK;
+    return NV_OK;
 }
 
-static NV_STATUS tools_access_va_block(uvm_va_block_t * va_block,
-				       NvU64 target_va,
-				       NvU64 size, bool is_write, void *stage)
+static NV_STATUS tools_access_va_block(uvm_va_block_t *va_block,
+                                       NvU64 target_va,
+                                       NvU64 size,
+                                       bool is_write,
+                                       void *stage)
 {
-	NV_STATUS status;
+    NV_STATUS status;
 
-	if (is_write)
-		status = UVM_VA_BLOCK_LOCK_RETRY(va_block, NULL,
-						 uvm_va_block_write_from_cpu
-						 (va_block, target_va, stage,
-						  size));
-	else
-		status = UVM_VA_BLOCK_LOCK_RETRY(va_block, NULL,
-						 uvm_va_block_read_to_cpu
-						 (va_block, stage, target_va,
-						  size));
+    if (is_write)
+        status = UVM_VA_BLOCK_LOCK_RETRY(va_block, NULL,
+                     uvm_va_block_write_from_cpu(va_block, target_va, stage, size));
+    else
+        status = UVM_VA_BLOCK_LOCK_RETRY(va_block, NULL,
+                     uvm_va_block_read_to_cpu(va_block, stage, target_va, size));
 
-	return status;
+    return status;
 }
 
-static NV_STATUS tools_access_process_memory(uvm_va_space_t * va_space,
-					     NvU64 target_va,
-					     NvU64 size,
-					     NvU64 user_va,
-					     NvU64 * bytes, bool is_write)
+static NV_STATUS tools_access_process_memory(uvm_va_space_t *va_space,
+                                             NvU64 target_va,
+                                             NvU64 size,
+                                             NvU64 user_va,
+                                             NvU64 *bytes,
+                                             bool is_write)
 {
-	NV_STATUS status = NV_OK;
-	uvm_va_block_t *block;
-	uvm_mem_t *stage_mem;
-	void *stage_addr;
+    NV_STATUS status = NV_OK;
+    uvm_va_block_t *block;
+    uvm_mem_t *stage_mem;
+    void *stage_addr;
+
+    status = uvm_mem_alloc_sysmem_and_map_cpu_kernel(PAGE_SIZE, &stage_mem);
+    if (status != NV_OK)
+        return status;
 
-	status = uvm_mem_alloc_sysmem_and_map_cpu_kernel(PAGE_SIZE, &stage_mem);
-	if (status != NV_OK)
-		return status;
+    stage_addr = uvm_mem_get_cpu_addr_kernel(stage_mem);
+    *bytes = 0;
 
-	stage_addr = uvm_mem_get_cpu_addr_kernel(stage_mem);
-	*bytes = 0;
+    while (*bytes < size) {
+        NvU64 user_va_start = user_va + *bytes;
+        NvU64 target_va_start = target_va + *bytes;
+        NvU64 bytes_left = size - *bytes;
+        NvU64 page_offset = target_va_start & (PAGE_SIZE - 1);
+        NvU64 bytes_now = min(bytes_left, (NvU64)(PAGE_SIZE - page_offset));
 
-	while (*bytes < size) {
-		NvU64 user_va_start = user_va + *bytes;
-		NvU64 target_va_start = target_va + *bytes;
-		NvU64 bytes_left = size - *bytes;
-		NvU64 page_offset = target_va_start & (PAGE_SIZE - 1);
-		NvU64 bytes_now =
-		    min(bytes_left, (NvU64) (PAGE_SIZE - page_offset));
+        if (is_write) {
+            NvU64 remaining = copy_from_user(stage_addr, (void *)user_va_start, bytes_now);
+            if (remaining != 0)  {
+                status = NV_ERR_INVALID_ARGUMENT;
+                break;
+            }
 
-		if (is_write) {
-			NvU64 remaining =
-			    copy_from_user(stage_addr, (void *)user_va_start,
-					   bytes_now);
-			if (remaining != 0) {
-				status = NV_ERR_INVALID_ARGUMENT;
-				break;
-			}
+        }
 
-		}
-		// The RM flavor of the lock is needed to perform ECC checks.
-		uvm_va_space_down_read_rm(va_space);
-		status =
-		    uvm_va_block_find_create(va_space, target_va_start, &block);
-		if (status != NV_OK) {
-			uvm_va_space_up_read_rm(va_space);
-			break;
-		}
-		status = tools_access_va_block(block,
-					       target_va_start,
-					       bytes_now, is_write, stage_mem);
+        // The RM flavor of the lock is needed to perform ECC checks.
+        uvm_va_space_down_read_rm(va_space);
+        status = uvm_va_block_find_create(va_space, target_va_start, &block);
+        if (status != NV_OK) {
+            uvm_va_space_up_read_rm(va_space);
+            break;
+        }
+        status = tools_access_va_block(block,
+                                       target_va_start,
+                                       bytes_now,
+                                       is_write,
+                                       stage_mem);
 
-		// For simplicity, check for ECC errors on all GPUs registered in the VA
-		// space as tools read/write is not on a perf critical path.
-		if (status == NV_OK)
-			status =
-			    uvm_gpu_check_ecc_error_mask(&va_space->
-							 registered_gpus);
+        // For simplicity, check for ECC errors on all GPUs registered in the VA
+        // space as tools read/write is not on a perf critical path.
+        if (status == NV_OK)
+            status = uvm_gpu_check_ecc_error_mask(&va_space->registered_gpus);
 
-		uvm_va_space_up_read_rm(va_space);
-		if (status != NV_OK)
-			break;
+        uvm_va_space_up_read_rm(va_space);
+        if (status != NV_OK)
+            break;
 
-		if (!is_write) {
-			NvU64 remaining =
-			    copy_to_user((void *)user_va_start, stage_addr,
-					 bytes_now);
-			if (remaining > 0) {
-				status = NV_ERR_INVALID_ARGUMENT;
-				break;
-			}
-		}
+        if (!is_write) {
+            NvU64 remaining = copy_to_user((void *)user_va_start, stage_addr, bytes_now);
+            if (remaining > 0) {
+                status = NV_ERR_INVALID_ARGUMENT;
+                break;
+            }
+        }
 
-		*bytes += bytes_now;
-	}
+        *bytes += bytes_now;
+    }
 
-	uvm_mem_free(stage_mem);
+    uvm_mem_free(stage_mem);
 
-	return status;
+    return status;
 }
 
-NV_STATUS uvm_api_tools_read_process_memory(UVM_TOOLS_READ_PROCESS_MEMORY_PARAMS
-					    * params, struct file * filp)
+NV_STATUS uvm_api_tools_read_process_memory(UVM_TOOLS_READ_PROCESS_MEMORY_PARAMS *params, struct file *filp)
 {
-	return tools_access_process_memory(uvm_va_space_get(filp),
-					   params->targetVa,
-					   params->size,
-					   params->buffer,
-					   &params->bytesRead, false);
+    return tools_access_process_memory(uvm_va_space_get(filp),
+                                       params->targetVa,
+                                       params->size,
+                                       params->buffer,
+                                       &params->bytesRead,
+                                       false);
 }
 
-NV_STATUS
-uvm_api_tools_write_process_memory(UVM_TOOLS_WRITE_PROCESS_MEMORY_PARAMS *
-				   params, struct file * filp)
+NV_STATUS uvm_api_tools_write_process_memory(UVM_TOOLS_WRITE_PROCESS_MEMORY_PARAMS *params, struct file *filp)
 {
-	return tools_access_process_memory(uvm_va_space_get(filp),
-					   params->targetVa,
-					   params->size,
-					   params->buffer,
-					   &params->bytesWritten, true);
+    return tools_access_process_memory(uvm_va_space_get(filp),
+                                       params->targetVa,
+                                       params->size,
+                                       params->buffer,
+                                       &params->bytesWritten,
+                                       true);
 }
 
-NV_STATUS uvm8_test_inject_tools_event(UVM_TEST_INJECT_TOOLS_EVENT_PARAMS *
-				       params, struct file * filp)
+NV_STATUS uvm8_test_inject_tools_event(UVM_TEST_INJECT_TOOLS_EVENT_PARAMS *params, struct file *filp)
 {
-	NvU32 i;
-	uvm_va_space_t *va_space = uvm_va_space_get(filp);
+    NvU32 i;
+    uvm_va_space_t *va_space = uvm_va_space_get(filp);
 
-	if (params->entry.eventData.eventType >= UvmEventNumTypes)
-		return NV_ERR_INVALID_ARGUMENT;
+    if (params->entry.eventData.eventType >= UvmEventNumTypes)
+        return NV_ERR_INVALID_ARGUMENT;
 
-	uvm_down_read(&va_space->perf_events.lock);
-	for (i = 0; i < params->count; i++)
-		uvm_tools_record_event(va_space, &params->entry);
-	uvm_up_read(&va_space->perf_events.lock);
-	return NV_OK;
+    uvm_down_read(&va_space->perf_events.lock);
+    for (i = 0; i < params->count; i++)
+        uvm_tools_record_event(va_space, &params->entry);
+    uvm_up_read(&va_space->perf_events.lock);
+    return NV_OK;
 }
 
-NV_STATUS
-uvm8_test_increment_tools_counter(UVM_TEST_INCREMENT_TOOLS_COUNTER_PARAMS *
-				  params, struct file * filp)
+NV_STATUS uvm8_test_increment_tools_counter(UVM_TEST_INCREMENT_TOOLS_COUNTER_PARAMS *params, struct file *filp)
 {
-	NvU32 i;
-	uvm_va_space_t *va_space = uvm_va_space_get(filp);
+    NvU32 i;
+    uvm_va_space_t *va_space = uvm_va_space_get(filp);
 
-	if (params->counter >= UVM_TOTAL_COUNTERS)
-		return NV_ERR_INVALID_ARGUMENT;
+    if (params->counter >= UVM_TOTAL_COUNTERS)
+        return NV_ERR_INVALID_ARGUMENT;
 
-	uvm_down_read(&va_space->perf_events.lock);
-	for (i = 0; i < params->count; i++)
-		uvm_tools_inc_counter(va_space, params->counter, params->amount,
-				      &params->processor);
-	uvm_up_read(&va_space->perf_events.lock);
+    uvm_down_read(&va_space->perf_events.lock);
+    for (i = 0; i < params->count; i++)
+        uvm_tools_inc_counter(va_space, params->counter, params->amount, &params->processor);
+    uvm_up_read(&va_space->perf_events.lock);
 
-	return NV_OK;
+    return NV_OK;
 }
 
-NV_STATUS
-uvm_api_tools_get_processor_uuid_table(UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_PARAMS
-				       * params, struct file * filp)
+NV_STATUS uvm_api_tools_get_processor_uuid_table(UVM_TOOLS_GET_PROCESSOR_UUID_TABLE_PARAMS *params, struct file *filp)
 {
-	NvProcessorUuid *uuids;
-	NvU64 remaining;
-	uvm_gpu_t *gpu;
-	uvm_va_space_t *va_space = uvm_va_space_get(filp);
+    NvProcessorUuid *uuids;
+    NvU64 remaining;
+    uvm_gpu_t *gpu;
+    uvm_va_space_t *va_space = uvm_va_space_get(filp);
 
-	uuids = uvm_kvmalloc_zero(sizeof(NvProcessorUuid) * UVM_MAX_PROCESSORS);
-	if (uuids == NULL)
-		return NV_ERR_NO_MEMORY;
+    uuids = uvm_kvmalloc_zero(sizeof(NvProcessorUuid) * UVM_MAX_PROCESSORS);
+    if (uuids == NULL)
+        return NV_ERR_NO_MEMORY;
 
-	uvm_processor_uuid_copy(&uuids[UVM_CPU_ID],
-				&NV_PROCESSOR_UUID_CPU_DEFAULT);
-	params->count = 1;
+    uvm_processor_uuid_copy(&uuids[UVM_CPU_ID], &NV_PROCESSOR_UUID_CPU_DEFAULT);
+    params->count = 1;
 
-	uvm_va_space_down_read(va_space);
-	for_each_va_space_gpu(gpu, va_space) {
-		uvm_processor_uuid_copy(&uuids[gpu->id], &gpu->uuid);
-		if (gpu->id + 1 > params->count)
-			params->count = gpu->id + 1;
-	}
-	uvm_va_space_up_read(va_space);
+    uvm_va_space_down_read(va_space);
+    for_each_va_space_gpu(gpu, va_space) {
+       uvm_processor_uuid_copy(&uuids[gpu->id], &gpu->uuid);
+       if (gpu->id + 1 > params->count)
+           params->count = gpu->id + 1;
+    }
+    uvm_va_space_up_read(va_space);
 
-	remaining =
-	    copy_to_user((void *)params->tablePtr, uuids,
-			 sizeof(NvProcessorUuid) * params->count);
-	uvm_kvfree(uuids);
+    remaining = copy_to_user((void *)params->tablePtr, uuids, sizeof(NvProcessorUuid) * params->count);
+    uvm_kvfree(uuids);
 
-	if (remaining != 0)
-		return NV_ERR_INVALID_ADDRESS;
+    if (remaining != 0)
+        return NV_ERR_INVALID_ADDRESS;
 
-	return NV_OK;
+    return NV_OK;
 }
 
-void uvm_tools_flush_events(uvm_va_space_t * va_space)
+void uvm_tools_flush_events(uvm_va_space_t *va_space)
 {
-	uvm_va_space_down_read(va_space);
-	uvm_tools_schedule_completed_events(va_space);
-	uvm_va_space_up_read(va_space);
+    uvm_va_space_down_read(va_space);
+    uvm_tools_schedule_completed_events(va_space);
+    uvm_va_space_up_read(va_space);
 
-	nv_kthread_q_flush(&g_tools_queue);
+    nv_kthread_q_flush(&g_tools_queue);
 }
 
-NV_STATUS uvm_api_tools_flush_events(UVM_TOOLS_FLUSH_EVENTS_PARAMS * params,
-				     struct file *filp)
+NV_STATUS uvm_api_tools_flush_events(UVM_TOOLS_FLUSH_EVENTS_PARAMS *params, struct file *filp)
 {
-	uvm_tools_flush_events(uvm_va_space_get(filp));
-	return NV_OK;
+    uvm_tools_flush_events(uvm_va_space_get(filp));
+    return NV_OK;
 }
 
-static const struct file_operations uvm_tools_fops = {
-	.open = uvm_tools_open,
-	.release = uvm_tools_release,
-	.unlocked_ioctl = uvm_tools_unlocked_ioctl,
+static const struct file_operations uvm_tools_fops =
+{
+    .open            = uvm_tools_open,
+    .release         = uvm_tools_release,
+    .unlocked_ioctl  = uvm_tools_unlocked_ioctl,
 #if NVCPU_IS_X86_64 && defined(NV_FILE_OPERATIONS_HAS_COMPAT_IOCTL)
-	.compat_ioctl = uvm_tools_unlocked_ioctl,
+    .compat_ioctl    = uvm_tools_unlocked_ioctl,
 #endif
-	.poll = uvm_tools_poll,
-	.owner = THIS_MODULE,
+    .poll            = uvm_tools_poll,
+    .owner           = THIS_MODULE,
 };
 
 // on failure, the caller should call uvm_tools_exit()
 int uvm_tools_init(dev_t uvm_base_dev)
 {
-	dev_t uvm_tools_dev =
-	    MKDEV(MAJOR(uvm_base_dev), NVIDIA_UVM_TOOLS_MINOR_NUMBER);
-	int ret;
-
-	uvm_init_rwsem(&g_tools_va_space_list_lock,
-		       UVM_LOCK_ORDER_TOOLS_VA_SPACE_LIST);
-
-	g_tools_event_tracker_cache =
-	    NV_KMEM_CACHE_CREATE("uvm_tools_event_tracker_t",
-				 uvm_tools_event_tracker_t);
-	if (!g_tools_event_tracker_cache)
-		return -ENOMEM;
-
-	g_tools_block_migration_data_cache =
-	    NV_KMEM_CACHE_CREATE("uvm_tools_block_migration_data_t",
-				 block_migration_data_t);
-	if (!g_tools_block_migration_data_cache)
-		return -ENOMEM;
-
-	g_tools_migration_data_cache =
-	    NV_KMEM_CACHE_CREATE("uvm_tools_migration_data_t",
-				 migration_data_t);
-	if (!g_tools_migration_data_cache)
-		return -ENOMEM;
-
-	ret = nv_kthread_q_init(&g_tools_queue, "UVM Tools Event Queue");
-	if (ret < 0)
-		return ret;
-
-	uvm_init_character_device(&g_uvm_tools_cdev, &uvm_tools_fops);
-	ret = cdev_add(&g_uvm_tools_cdev, uvm_tools_dev, 1);
-	if (ret != 0)
-		UVM_ERR_PRINT("cdev_add (major %u, minor %u) failed: %d\n",
-			      MAJOR(uvm_tools_dev), MINOR(uvm_tools_dev), ret);
+    dev_t uvm_tools_dev = MKDEV(MAJOR(uvm_base_dev), NVIDIA_UVM_TOOLS_MINOR_NUMBER);
+    int ret;
+
+    uvm_init_rwsem(&g_tools_va_space_list_lock, UVM_LOCK_ORDER_TOOLS_VA_SPACE_LIST);
+
+    g_tools_event_tracker_cache = NV_KMEM_CACHE_CREATE("uvm_tools_event_tracker_t",
+                                                        uvm_tools_event_tracker_t);
+    if (!g_tools_event_tracker_cache)
+        return -ENOMEM;
+
+    g_tools_block_migration_data_cache = NV_KMEM_CACHE_CREATE("uvm_tools_block_migration_data_t",
+                                                              block_migration_data_t);
+    if (!g_tools_block_migration_data_cache)
+        return -ENOMEM;
+
+    g_tools_migration_data_cache = NV_KMEM_CACHE_CREATE("uvm_tools_migration_data_t",
+                                                        migration_data_t);
+    if (!g_tools_migration_data_cache)
+        return -ENOMEM;
+
+    ret = nv_kthread_q_init(&g_tools_queue, "UVM Tools Event Queue");
+    if (ret < 0)
+        return ret;
+
+    uvm_init_character_device(&g_uvm_tools_cdev, &uvm_tools_fops);
+    ret = cdev_add(&g_uvm_tools_cdev, uvm_tools_dev, 1);
+    if (ret != 0)
+        UVM_ERR_PRINT("cdev_add (major %u, minor %u) failed: %d\n", MAJOR(uvm_tools_dev),
+                      MINOR(uvm_tools_dev), ret);
 
-	return ret;
+    return ret;
 }
 
 void uvm_tools_exit(void)
 {
-	cdev_del(&g_uvm_tools_cdev);
+    cdev_del(&g_uvm_tools_cdev);
 
-	nv_kthread_q_stop(&g_tools_queue);
+    nv_kthread_q_stop(&g_tools_queue);
 
-	kmem_cache_destroy_safe(&g_tools_event_tracker_cache);
-	kmem_cache_destroy_safe(&g_tools_block_migration_data_cache);
-	kmem_cache_destroy_safe(&g_tools_migration_data_cache);
+    kmem_cache_destroy_safe(&g_tools_event_tracker_cache);
+    kmem_cache_destroy_safe(&g_tools_block_migration_data_cache);
+    kmem_cache_destroy_safe(&g_tools_migration_data_cache);
 }
Только в 367.57/kernel/nvidia-uvm: uvm8_tools.c~
diff -ur 367.57/kernel/nvidia-uvm/uvm8_tools.h 375.20/kernel/nvidia-uvm/uvm8_tools.h
--- 367.57/kernel/nvidia-uvm/uvm8_tools.h	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_tools.h	2016-11-16 02:53:31.000000000 +0300
@@ -77,6 +77,13 @@
 void uvm_tools_record_map_remote(uvm_va_block_t *va_block, uvm_processor_id_t processor, uvm_processor_id_t residency,
                                  NvU64 address, size_t region_size, UvmEventMapRemoteCause cause);
 
+void uvm_tools_record_block_migration_begin(uvm_va_block_t *va_block,
+                                            uvm_push_t *push,
+                                            uvm_processor_id_t dst_id,
+                                            uvm_processor_id_t src_id,
+                                            NvU64 start,
+                                            UvmEventMigrationCause cause);
+
 void uvm_tools_broadcast_replay(uvm_gpu_id_t gpu_id, NvU32 batch_id);
 // Invokes the pushbuffer reclamation for the VA space
 void uvm_tools_schedule_completed_events(uvm_va_space_t *va_space);
diff -ur 367.57/kernel/nvidia-uvm/uvm8_va_block.c 375.20/kernel/nvidia-uvm/uvm8_va_block.c
--- 367.57/kernel/nvidia-uvm/uvm8_va_block.c	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_va_block.c	2016-11-16 02:53:31.000000000 +0300
@@ -540,9 +540,45 @@
     return status;
 }
 
+static void block_gpu_unmap_phys_all_cpu_pages(uvm_va_block_t *block, uvm_gpu_t *gpu)
+{
+    size_t page_index;
+    uvm_va_block_gpu_state_t *gpu_state = block->gpus[gpu->id - 1];
+
+    for (page_index = 0; page_index < uvm_va_block_num_cpu_pages(block); ++page_index) {
+        if (gpu_state->cpu_pages_dma_addrs[page_index] == 0)
+            continue;
+        uvm_gpu_unmap_cpu_page(gpu, gpu_state->cpu_pages_dma_addrs[page_index]);
+        gpu_state->cpu_pages_dma_addrs[page_index] = 0;
+    }
+}
+
+static NV_STATUS block_gpu_map_phys_all_cpu_pages(uvm_va_block_t *block, uvm_gpu_t *gpu)
+{
+    NV_STATUS status;
+    size_t page_index;
+    uvm_va_block_gpu_state_t *gpu_state = block->gpus[gpu->id - 1];
+
+    for (page_index = 0; page_index < uvm_va_block_num_cpu_pages(block); ++page_index) {
+        if (!block->cpu.pages[page_index])
+            continue;
+
+        status = uvm_gpu_map_cpu_page(gpu, block->cpu.pages[page_index], &gpu_state->cpu_pages_dma_addrs[page_index]);
+        if (status != NV_OK)
+            goto error;
+    }
+
+    return NV_OK;
+
+error:
+    block_gpu_unmap_phys_all_cpu_pages(block, gpu);
+    return status;
+}
+
 // Retrieves the gpu_state for the given GPU, allocating it if it doesn't exist
 static uvm_va_block_gpu_state_t *block_gpu_state_get_alloc(uvm_va_block_t *block, uvm_gpu_t *gpu)
 {
+    NV_STATUS status;
     uvm_va_block_gpu_state_t *gpu_state = block->gpus[gpu->id - 1];
 
     if (gpu_state)
@@ -556,20 +592,77 @@
     if (!gpu_state->chunks)
         goto error;
 
+    gpu_state->cpu_pages_dma_addrs = uvm_kvmalloc_zero(uvm_va_block_num_cpu_pages(block) * sizeof(gpu_state->cpu_pages_dma_addrs[0]));
+    if (!gpu_state->cpu_pages_dma_addrs)
+        goto error;
+
     block->gpus[gpu->id - 1] = gpu_state;
 
+    status = block_gpu_map_phys_all_cpu_pages(block, gpu);
+    if (status != NV_OK)
+        goto error;
+
     return gpu_state;
 
 error:
-    if (gpu_state)
+    if (gpu_state) {
+        if (gpu_state->chunks)
+            uvm_kvfree(gpu_state->chunks);
         kmem_cache_free(g_uvm_va_block_gpu_state_cache, gpu_state);
+    }
+    block->gpus[gpu->id - 1] = NULL;
 
     return NULL;
 }
 
+static void block_unmap_phys_cpu_page_on_gpus(uvm_va_block_t *block, size_t page_index)
+{
+    uvm_gpu_id_t id;
+
+    for_each_gpu_id(id) {
+        uvm_va_block_gpu_state_t *gpu_state = block->gpus[id - 1];
+        if (!gpu_state)
+            continue;
+
+        if (gpu_state->cpu_pages_dma_addrs[page_index] == 0)
+            continue;
+
+        uvm_gpu_unmap_cpu_page(uvm_gpu_get(id), gpu_state->cpu_pages_dma_addrs[page_index]);
+        gpu_state->cpu_pages_dma_addrs[page_index] = 0;
+    }
+}
+
+static NV_STATUS block_map_phys_cpu_page_on_gpus(uvm_va_block_t *block, size_t page_index, struct page *page)
+{
+    NV_STATUS status;
+    uvm_gpu_id_t id;
+
+    for_each_gpu_id(id) {
+        uvm_va_block_gpu_state_t *gpu_state = block->gpus[id - 1];
+        if (!gpu_state)
+            continue;
+
+        UVM_ASSERT(gpu_state->cpu_pages_dma_addrs[page_index] == 0);
+
+        status = uvm_gpu_map_cpu_page(uvm_gpu_get(id), page, &gpu_state->cpu_pages_dma_addrs[page_index]);
+        if (status != NV_OK)
+            goto error;
+    }
+
+    return NV_OK;
+
+error:
+    block_unmap_phys_cpu_page_on_gpus(block, page_index);
+    return status;
+}
+
 // Allocates the input page in the block, if it doesn't already exist
+//
+// Also maps the page for physical access by all GPUs used by the block, which
+// is required for IOMMU support.
 static NV_STATUS block_populate_page_cpu(uvm_va_block_t *block, size_t page_index, bool zero)
 {
+    NV_STATUS status;
     struct page *page;
     gfp_t gfp_flags;
 
@@ -590,8 +683,16 @@
     if (zero)
         SetPageDirty(page);
 
+    status = block_map_phys_cpu_page_on_gpus(block, page_index, page);
+    if (status != NV_OK)
+        goto error;
+
     block->cpu.pages[page_index] = page;
     return NV_OK;
+
+error:
+    __free_page(page);
+    return status;
 }
 
 // Try allocating a chunk. If eviction was required,
@@ -1548,16 +1649,23 @@
 static uvm_gpu_phys_address_t block_phys_page_address(uvm_va_block_t *block, block_phys_page_t block_page, uvm_gpu_t *gpu)
 {
     uvm_va_block_gpu_state_t *gpu_state;
+    uvm_va_block_gpu_state_t *accessing_gpu_state = block->gpus[gpu->id - 1];
     uvm_aperture_t aperture;
     size_t chunk_index, page_offset;
     uvm_gpu_t *owning_gpu;
     uvm_gpu_chunk_t *chunk;
     uvm_chunk_size_t chunk_size;
 
+    UVM_ASSERT(accessing_gpu_state);
+
     if (block_page.processor == UVM_CPU_ID) {
-        // TODO: Bug 1765195: IOMMU support
-        NvU64 phys = page_to_phys(block->cpu.pages[block_page.page_index]);
-        return uvm_gpu_phys_address(UVM_APERTURE_SYS, phys);
+        NvU64 dma_addr = accessing_gpu_state->cpu_pages_dma_addrs[block_page.page_index];
+
+        // The page should be mapped for physical access already as we do that
+        // eagerly on CPU page population and GPU state alloc.
+        UVM_ASSERT(dma_addr != 0);
+
+        return uvm_gpu_phys_address(UVM_APERTURE_SYS, dma_addr);
     }
 
     owning_gpu = uvm_gpu_get(block_page.processor);
@@ -1759,6 +1867,7 @@
                                                    uvm_processor_id_t src_id,
                                                    uvm_va_block_region_t region,
                                                    const unsigned long *page_mask,
+                                                   const unsigned long *prefetch_page_mask,
                                                    UvmEventMigrationCause cause,
                                                    block_transfer_mode_internal_t transfer_mode,
                                                    unsigned long *migrated_pages,
@@ -1778,6 +1887,8 @@
     unsigned long *src_move_mask = block_context->make_resident.copy_resident_pages_between_mask;
     uvm_range_group_range_t *rgr = NULL;
     bool rgr_has_changed = false;
+    UvmEventMigrationCause contig_cause = cause;
+    bool may_prefetch = (cause == UvmEventMigrationCauseCoherence) && !!prefetch_page_mask;
 
     *copied_pages = 0;
 
@@ -1800,6 +1911,9 @@
 
     for_each_va_block_page_in_mask(page_index, src_move_mask, region) {
         NvU64 page_start = block->start + PAGE_SIZE * page_index;
+        UvmEventMigrationCause page_cause = (may_prefetch && test_bit(page_index, prefetch_page_mask))?
+                                                UvmEventMigrationCausePrefetch:
+                                                cause;
 
         if (dst_id == UVM_CPU_ID) {
             // To support staging through CPU, populate CPU pages on demand.
@@ -1815,7 +1929,11 @@
         // If we're not evicting and we're migrating away from the preferred location, then we should add the range
         // group range to the list of migrated ranges in the range group. It's safe to skip this because the use of
         // range_group's migrated_ranges list is a UVM-Lite optimization - eviction is not supported on UVM-Lite GPUs.
-        if (cause != UvmEventMigrationCauseEviction && src_id == block->va_range->preferred_location ) {
+        if (cause != UvmEventMigrationCauseEviction && src_id == block->va_range->preferred_location) {
+            // rgr_has_changed is used to minimize the number of times the migrated_ranges_lock is taken. It is set to
+            // false when the range group range pointed by rgr is added to the migrated_ranges list, and it is just set
+            // back to true when we move to a different range group range.
+
             // The current page could be after the end of rgr. Iterate over the range group ranges until rgr's
             // end location is greater than or equal to the current page.
             while (rgr && rgr->node.end < page_start) {
@@ -1833,6 +1951,8 @@
                 if (list_empty(&rgr->range_group_migrated_list_node))
                     list_move_tail(&rgr->range_group_migrated_list_node, &rgr->range_group->migrated_ranges);
                 uvm_spin_unlock(&rgr->range_group->migrated_ranges_lock);
+
+                rgr_has_changed = false;
             }
         }
 
@@ -1841,19 +1961,6 @@
             goto update_bits;
 
         if (!copying_gpu) {
-            uvm_perf_event_data_t event_data =
-                {
-                    .migration =
-                        {
-                            .push = &push,
-                            .block = block,
-                            .src = src_id,
-                            .dst = dst_id,
-                            .address = page_start,
-                            .bytes = 0,
-                            .cause = cause
-                        }
-                };
             status = block_copy_begin_push(block, dst_id, src_id, &block->tracker, &push);
             if (status != NV_OK)
                 break;
@@ -1864,7 +1971,9 @@
             uvm_processor_mask_set(&block_context->make_resident.all_involved_processors, dst_id);
             uvm_processor_mask_set(&block_context->make_resident.all_involved_processors, src_id);
 
-            uvm_perf_event_notify(&block->va_range->va_space->perf_events, UVM_PERF_EVENT_BLOCK_MIGRATION_BEGIN, &event_data);
+            // This function is called just once per VA block and needs to receive the "main" cause for the migration
+            // (it mainly checks if we are in the eviction path). Therefore, we pass cause instead of contig_cause
+            uvm_tools_record_block_migration_begin(block, &push, dst_id, src_id, page_start, cause);
         }
         else {
             uvm_push_set_flag(&push, UVM_PUSH_FLAG_CE_NEXT_PIPELINED);
@@ -1877,8 +1986,9 @@
 
         if (last_index == region.outer) {
             contig_start_index = page_index;
+            contig_cause = page_cause;
         }
-        else if (page_index != last_index + 1) {
+        else if ((page_index != last_index + 1) || contig_cause != page_cause) {
             uvm_va_block_region_t contig_region = uvm_va_block_region(contig_start_index, last_index + 1);
             uvm_perf_event_data_t event_data =
                 {
@@ -1890,7 +2000,7 @@
                             .dst           = dst_id,
                             .address       = uvm_va_block_region_start(block, contig_region),
                             .bytes         = uvm_va_block_region_size(contig_region),
-                            .cause         = cause,
+                            .cause         = contig_cause,
                             .transfer_mode = transfer_mode
                         }
                 };
@@ -1898,6 +2008,7 @@
             UVM_ASSERT(uvm_va_block_region_contains_region(region, contig_region));
             uvm_perf_event_notify(&block->va_range->va_space->perf_events, UVM_PERF_EVENT_MIGRATION, &event_data);
             contig_start_index = page_index;
+            contig_cause = page_cause;
         }
 
         uvm_push_set_flag(&push, UVM_PUSH_FLAG_CE_NEXT_MEMBAR_NONE);
@@ -1907,10 +2018,9 @@
 
 update_bits:
         *copied_pages += 1;
+        __set_bit(page_index, migrated_pages);
 
         UVM_ASSERT(block_check_resident_proximity(block, page_index, dst_id));
-
-        __set_bit(page_index, migrated_pages);
         __set_bit(page_index, dst_resident_mask);
 
         if (transfer_mode == BLOCK_TRANSFER_MODE_INTERNAL_COPY ||
@@ -1920,8 +2030,6 @@
         // If we are staging the copy due to read duplication, we keep the copy there
         if (transfer_mode == BLOCK_TRANSFER_MODE_INTERNAL_MOVE_FROM_STAGE)
             __clear_bit(page_index, src_resident_mask);
-
-        rgr_has_changed = false;
     }
 
     if (*copied_pages) {
@@ -1947,7 +2055,7 @@
                         .dst           = dst_id,
                         .address       = uvm_va_block_region_start(block, contig_region),
                         .bytes         = uvm_va_block_region_size(contig_region),
-                        .cause         = cause,
+                        .cause         = contig_cause,
                         .transfer_mode = transfer_mode
 
                     }
@@ -1972,6 +2080,7 @@
                                                 uvm_processor_mask_t *src_processor_mask,
                                                 uvm_va_block_region_t region,
                                                 const unsigned long *page_mask,
+                                                const unsigned long *prefetch_page_mask,
                                                 UvmEventMigrationCause cause,
                                                 block_transfer_mode_internal_t transfer_mode,
                                                 NvU32 max_pages_to_copy,
@@ -1995,6 +2104,7 @@
                                                    src_id,
                                                    region,
                                                    page_mask,
+                                                   prefetch_page_mask,
                                                    cause,
                                                    transfer_mode,
                                                    migrated_pages,
@@ -2055,6 +2165,7 @@
                                            uvm_processor_id_t dst_id,
                                            uvm_va_block_region_t region,
                                            const unsigned long *page_mask,
+                                           const unsigned long *prefetch_page_mask,
                                            UvmEventMigrationCause cause,
                                            uvm_va_block_transfer_mode_t transfer_mode)
 {
@@ -2109,6 +2220,7 @@
                                                 &src_processor_mask,
                                                 region,
                                                 page_mask,
+                                                prefetch_page_mask,
                                                 cause,
                                                 transfer_mode == UVM_VA_BLOCK_TRANSFER_MODE_COPY?
                                                     BLOCK_TRANSFER_MODE_INTERNAL_COPY:
@@ -2152,6 +2264,7 @@
                                             &src_processor_mask,
                                             region,
                                             page_mask,
+                                            prefetch_page_mask,
                                             cause,
                                             transfer_mode_internal,
                                             missing_pages_count,
@@ -2183,6 +2296,7 @@
                                                UVM_CPU_ID,
                                                region,
                                                staged_pages,
+                                               prefetch_page_mask,
                                                cause,
                                                transfer_mode == UVM_VA_BLOCK_TRANSFER_MODE_COPY?
                                                    BLOCK_TRANSFER_MODE_INTERNAL_COPY_FROM_STAGE:
@@ -2204,6 +2318,7 @@
                                                UVM_CPU_ID,
                                                region,
                                                page_mask,
+                                               prefetch_page_mask,
                                                cause,
                                                transfer_mode == UVM_VA_BLOCK_TRANSFER_MODE_COPY?
                                                    BLOCK_TRANSFER_MODE_INTERNAL_COPY:
@@ -2264,6 +2379,7 @@
                                      uvm_processor_id_t dest_id,
                                      uvm_va_block_region_t region,
                                      const unsigned long *page_mask,
+                                     const unsigned long *prefetch_page_mask,
                                      UvmEventMigrationCause cause)
 {
     NV_STATUS status;
@@ -2272,6 +2388,9 @@
     unsigned long *unmap_page_mask = va_block_context->make_resident.page_mask;
     unsigned long *resident_mask;
 
+    if (prefetch_page_mask)
+        UVM_ASSERT(cause == UvmEventMigrationCauseCoherence);
+
     uvm_assert_mutex_locked(&va_block->lock);
     UVM_ASSERT(va_block->va_range);
     UVM_ASSERT(va_block->va_range->type == UVM_VA_RANGE_TYPE_MANAGED);
@@ -2318,6 +2437,7 @@
                                        dest_id,
                                        region,
                                        page_mask,
+                                       prefetch_page_mask,
                                        cause,
                                        UVM_VA_BLOCK_TRANSFER_MODE_MOVE);
     if (status != NV_OK)
@@ -2337,6 +2457,7 @@
                                                     uvm_processor_id_t dest_id,
                                                     uvm_va_block_region_t region,
                                                     const unsigned long *page_mask,
+                                                    const unsigned long *prefetch_page_mask,
                                                     UvmEventMigrationCause cause)
 {
     NV_STATUS status = NV_OK;
@@ -2350,6 +2471,9 @@
     uvm_processor_id_t src_id;
     uvm_tracker_t local_tracker = UVM_TRACKER_INIT();
 
+    if (prefetch_page_mask)
+        UVM_ASSERT(cause == UvmEventMigrationCauseCoherence);
+
     uvm_assert_mutex_locked(&va_block->lock);
     UVM_ASSERT(va_block->va_range);
     UVM_ASSERT(va_block->va_range->type == UVM_VA_RANGE_TYPE_MANAGED);
@@ -2421,7 +2545,13 @@
     if (status != NV_OK)
         return status;
 
-    status = block_copy_resident_pages(va_block, va_block_context, dest_id, region, page_mask, cause,
+    status = block_copy_resident_pages(va_block,
+                                       va_block_context,
+                                       dest_id,
+                                       region,
+                                       page_mask,
+                                       prefetch_page_mask,
+                                       cause,
                                        UVM_VA_BLOCK_TRANSFER_MODE_COPY);
     if (status != NV_OK)
         return status;
@@ -2765,6 +2895,14 @@
     uvm_gpu_t *resident_gpu;
     uvm_gpu_chunk_t *chunk;
 
+    if (!gpu_state->page_table_range_4k.table)
+        UVM_ASSERT(!gpu_state->activated_4k);
+
+    if (!gpu_state->page_table_range_big.table) {
+        UVM_ASSERT(!gpu_state->initialized_big);
+        UVM_ASSERT(!gpu_state->activated_big);
+    }
+
     // It's only safe to check the PTE mappings if we have page tables. See
     // uvm_va_block_get_gpu_va_space.
     if (!block_gpu_has_page_tables(block, gpu)) {
@@ -6266,6 +6404,11 @@
         UVM_ASSERT(!uvm_processor_mask_test(&block->resident, id));
     }
 
+    if (gpu_state->cpu_pages_dma_addrs) {
+        block_gpu_unmap_phys_all_cpu_pages(block, gpu);
+        uvm_kvfree(gpu_state->cpu_pages_dma_addrs);
+    }
+
     kmem_cache_free(g_uvm_va_block_gpu_state_cache, gpu_state);
     block->gpus[id - 1] = NULL;
 }
@@ -6362,7 +6505,12 @@
     block_put_ptes_safe(&gpu_va_space->page_tables, &gpu_state->page_table_range_2m);
 
     gpu_state->pte_is_2m = false;
+    gpu_state->initialized_big = false;
+    gpu_state->activated_big = false;
+    gpu_state->activated_4k = false;
     bitmap_zero(gpu_state->big_ptes, MAX_BIG_PAGES_PER_UVM_VA_BLOCK);
+
+    UVM_ASSERT(block_check_mappings(va_block));
 }
 
 void uvm_va_block_disable_peer(uvm_va_block_t *va_block, uvm_gpu_t *gpu0, uvm_gpu_t *gpu1)
@@ -7194,6 +7342,7 @@
     size_t new_pages = uvm_va_block_num_cpu_pages(new);
     size_t existing_pages, existing_pages_4k, existing_pages_big, new_pages_big;
     uvm_pte_bits_gpu_t pte_bit;
+    NvU64 *shrunk_dma_addrs;
 
     if (!existing_gpu_state)
         return;
@@ -7205,6 +7354,18 @@
     UVM_ASSERT(PAGE_ALIGNED(existing->start));
     existing_pages = (new->start - existing->start) / PAGE_SIZE;
 
+    // Move DMA addresses from the top of existing down into new
+    memcpy(&new_gpu_state->cpu_pages_dma_addrs[0],
+           &existing_gpu_state->cpu_pages_dma_addrs[existing_pages],
+           uvm_va_block_num_cpu_pages(new) * sizeof(new_gpu_state->cpu_pages_dma_addrs[0]));
+
+    // Attempt to shrink existing's allocation. If the realloc fails, just keep
+    // on using the old larger one.
+    shrunk_dma_addrs = uvm_kvrealloc(existing_gpu_state->cpu_pages_dma_addrs,
+                                   existing_pages * sizeof(existing_gpu_state->cpu_pages_dma_addrs[0]));
+    if (shrunk_dma_addrs)
+        existing_gpu_state->cpu_pages_dma_addrs = shrunk_dma_addrs;
+
     uvm_va_block_copy_split_gpu_chunks(existing, new, gpu);
 
     block_split_page_mask(existing_gpu_state->resident, existing_pages,
@@ -7263,6 +7424,8 @@
                                    existing_gpu_state->big_ptes,
                                    uvm_va_block_num_big_pages(existing, big_page_size) - new_pages_big,
                                    MAX_BIG_PAGES_PER_UVM_VA_BLOCK);
+
+                new_gpu_state->initialized_big = existing_gpu_state->initialized_big;
             }
 
             // Drop existing's references on the big PTEs it no longer covers
@@ -7273,14 +7436,14 @@
                                         &existing_gpu_state->page_table_range_big,
                                         existing_pages_big);
 
-            if (existing_pages_big == 0)
+            if (existing_pages_big == 0) {
                 memset(&existing_gpu_state->page_table_range_big, 0, sizeof(existing_gpu_state->page_table_range_big));
+                existing_gpu_state->initialized_big = false;
+            }
 
             bitmap_clear(existing_gpu_state->big_ptes,
                          existing_pages_big,
                          MAX_BIG_PAGES_PER_UVM_VA_BLOCK - existing_pages_big);
-
-            new_gpu_state->initialized_big = existing_gpu_state->initialized_big;
         }
 
         if (existing_gpu_state->page_table_range_4k.table) {
@@ -7311,6 +7474,8 @@
         // references it has. We've taken the necessary references on the lower
         // tables above.
         block_put_ptes_safe(&gpu_va_space->page_tables, &existing_gpu_state->page_table_range_2m);
+        existing_gpu_state->activated_big = false;
+        existing_gpu_state->activated_4k = false;
     }
 }
 
@@ -7430,6 +7595,28 @@
     return status;
 }
 
+static bool block_region_might_read_duplicate(uvm_va_block_t *va_block,
+                                              uvm_va_block_region_t region)
+{
+    uvm_va_range_t *va_range;
+    uvm_va_space_t *va_space;
+
+    va_range = va_block->va_range;
+    va_space = va_range->va_space;
+
+    if (!uvm_va_space_can_read_duplicate(va_space, NULL))
+        return false;
+
+    if (va_range->read_duplication == UVM_READ_DUPLICATION_DISABLED)
+        return false;
+
+    if (va_range->read_duplication == UVM_READ_DUPLICATION_UNSET
+        && uvm_page_mask_region_weight(va_block->read_duplicated_pages, region) == 0)
+        return false;
+
+    return true;
+}
+
 uvm_prot_t uvm_va_block_compute_new_permission_after_fault(uvm_va_block_t *va_block,
                                                            uvm_va_block_region_t region,
                                                            uvm_processor_id_t fault_processor_id,
@@ -7453,15 +7640,12 @@
 
     UVM_ASSERT(logical_prot >= new_prot);
 
-    // TODO: Bug 1765189: this is currently relying on the fact that
-    //       uvm_va_block_service_faults_locked is servicing only one page at a
-    //       time. This could be changed in the future to optimize multiple
-    //       faults on contiguous pages.
-    if (((va_range->read_duplication == UVM_READ_DUPLICATION_DISABLED &&
-          uvm_va_space_can_read_duplicate(va_space, NULL)) ||
-         (va_range->read_duplication == UVM_READ_DUPLICATION_UNSET &&
-          uvm_page_mask_region_weight(va_block->read_duplicated_pages, region) == 0)) &&
-        logical_prot > UVM_PROT_READ_ONLY && new_prot == UVM_PROT_READ_ONLY) {
+    // TODO: Bug 1766424: this is currently relying on the fact that
+    //       uvm_va_block_service_faults_locked calls us with a single-page
+    //       region at a time. This could be changed in the future to optimize
+    //       multiple faults on contiguous pages.
+    if (logical_prot > UVM_PROT_READ_ONLY && new_prot == UVM_PROT_READ_ONLY &&
+        !block_region_might_read_duplicate(va_block, region)) {
         uvm_processor_mask_t processors_with_atomic_mapping;
         uvm_processor_mask_t revoke_processors;
 
@@ -7801,6 +7985,7 @@
     uvm_processor_id_t closest_resident_processor;
     uvm_va_range_t *va_range = va_block->va_range;
     uvm_va_space_t *va_space = va_range->va_space;
+    bool is_atomic;
 
     if (is_uvm_fault_force_sysmem_set())
         return UVM_CPU_ID;
@@ -7820,43 +8005,50 @@
         return processor_id;
 
     if (thrashing_hint->type == UVM_PERF_THRASHING_HINT_TYPE_PIN) {
-        UVM_ASSERT(uvm_processor_mask_test(&va_range->va_space->accessible_from[thrashing_hint->pin.residency],
-                                           processor_id));
+        UVM_ASSERT(uvm_processor_mask_test(&va_space->accessible_from[thrashing_hint->pin.residency], processor_id));
         return thrashing_hint->pin.residency;
     }
 
-    // TODO: Bug 1765189: this is currently relying on the fact that
-    //       uvm_va_block_service_faults_locked is servicing only one page at a
-    //       time. This could be changed in the future to optimize multiple
-    //       faults on contiguous pages.
+    // TODO: Bug 1766424: this is currently relying on the fact that
+    //       uvm_va_block_service_faults_locked calls us with a single-page
+    //       region at a time. This could be changed in the future to optimize
+    //       multiple faults on contiguous pages.
     closest_resident_processor = uvm_va_block_page_get_closest_resident(va_block, region.first, processor_id);
 
-    // If the block is not resident or is resident on a processor other than the preferred location,
-    // we select the faulting GPU as the new residency, unless the faulting GPU has support for native
-    // atomics to the current location and the fault was due to an atomic access. In the later case we
-    // keep the current residency
-    //
-    // TODO: Bug id 1716025: UVM needs to implement performance heuristics to provide transparent data
-    //                       transfer optimizations
-    //
-    // This is a short-term solution to exercise remote atomics over NVLINK when possible (not only when
-    // preferred location is set to the remote GPU) as they are much faster than relying on page faults
-    // and permission downgrades, which cause thrashing. In the future, the thrashing
-    // detection/prevention heuristics will detect and handle this case.
-    if (closest_resident_processor != UVM_MAX_PROCESSORS &&
-        access_type == UVM_FAULT_ACCESS_TYPE_ATOMIC &&
-        uvm_processor_mask_test(&va_space->has_native_atomics[closest_resident_processor], processor_id))
-        return closest_resident_processor;
+    // If the page is not resident anywhere, select the preferred location as
+    // long as the preferred location is accessible from the faulting processor.
+    // Otherwise select the faulting processor.
+    if (closest_resident_processor == UVM8_MAX_PROCESSORS) {
+        if (va_range->preferred_location != UVM8_MAX_PROCESSORS &&
+            uvm_processor_mask_test(&va_space->accessible_from[va_range->preferred_location], processor_id)) {
+            return va_range->preferred_location;
+        }
 
-    if (closest_resident_processor == UVM_MAX_PROCESSORS ||
-        closest_resident_processor != va_range->preferred_location)
         return processor_id;
+    }
 
-    // If the faulting GPU can't access the current residency, we migrate the VA range
-    if (!uvm_processor_mask_test(&va_range->va_space->accessible_from[va_range->preferred_location], processor_id))
+    // TODO: Bug 1827400: If the faulting GPU has support for native atomics to
+    //       the current location and the fault was due to an atomic access, we
+    //       keep the current residency. This is a short-term solution to
+    //       exercise remote atomics over NVLINK when possible (not only when
+    //       preferred location is set to the remote GPU) as they are much
+    //       faster than relying on page faults and permission downgrades, which
+    //       cause thrashing. In the future, the thrashing detection/prevention
+    //       heuristics should detect and handle this case.
+    is_atomic = (access_type == UVM_FAULT_ACCESS_TYPE_ATOMIC);
+
+    if (is_atomic && uvm_processor_mask_test(&va_space->has_native_atomics[closest_resident_processor], processor_id))
+        return closest_resident_processor;
+
+    // If the page is resident on a processor other than the preferred location,
+    // or the faulting GPU can't access the preferred location, we select the
+    // faulting GPU as the new residency.
+    if (closest_resident_processor != va_range->preferred_location ||
+        !uvm_processor_mask_test(&va_space->accessible_from[closest_resident_processor], processor_id))
         return processor_id;
 
-    // If the faulting GPU can access the current residency, keep the VA range on its preferred location
+    // The page is on the preferred location and the faulting GPU can access it.
+    // Keep it there.
     return va_range->preferred_location;
 }
 
@@ -7889,7 +8081,7 @@
         uvm_assert_rwsem_locked_read(&va_space->lock);
 
     // Performance heuristics policy: we only consider prefetching when faults
-    // trigger migratons to a single processor.
+    // trigger migrations to a single processor.
     if (uvm_processor_mask_get_count(&service_context->resident_processors) == 1) {
         size_t page_index;
         uvm_va_block_region_t block_region = uvm_va_block_region_from_block(va_block);
@@ -7944,78 +8136,48 @@
         uvm_page_mask_zero(did_migrate_mask);
         uvm_processor_mask_zero(&service_context->block_context.make_resident.all_involved_processors);
 
-        if (service_context->read_duplicate_count == 0) {
+        if (prefetch_hint.residency != UVM8_MAX_PROCESSORS) {
+            UVM_ASSERT(prefetch_hint.residency == new_residency);
+            UVM_ASSERT(prefetch_hint.prefetch_pages_mask != NULL);
+
+            uvm_page_mask_or(service_context->per_processor_masks[new_residency].new_residency,
+                             service_context->per_processor_masks[new_residency].new_residency,
+                             prefetch_hint.prefetch_pages_mask);
+        }
+
+        if (service_context->read_duplicate_count == 0 ||
+            uvm_page_mask_andnot(service_context->block_context.caller_page_mask,
+                                 service_context->per_processor_masks[new_residency].new_residency,
+                                 service_context->read_duplicate_mask)) {
             status = uvm_va_block_make_resident(va_block,
                                                 block_retry,
                                                 &service_context->block_context,
                                                 new_residency,
                                                 service_context->fault_region,
-                                                service_context->per_processor_masks[new_residency].new_residency,
-                                                UvmEventMigrationCauseCoherence);
-        }
-        else {
-            bool do_migration;
-            do_migration = uvm_page_mask_andnot(service_context->block_context.caller_page_mask,
-                                                service_context->per_processor_masks[new_residency].new_residency,
-                                                service_context->read_duplicate_mask);
-            if (do_migration) {
-                status = uvm_va_block_make_resident(va_block,
-                                                    block_retry,
-                                                    &service_context->block_context,
-                                                    new_residency,
-                                                    service_context->fault_region,
+                                                service_context->read_duplicate_count == 0?
+                                                    service_context->per_processor_masks[new_residency].new_residency:
                                                     service_context->block_context.caller_page_mask,
-                                                    UvmEventMigrationCauseCoherence);
-
-                if (status != NV_OK)
-                    goto done;
-            }
-
-            do_migration = uvm_page_mask_and(service_context->block_context.caller_page_mask,
-                                             service_context->per_processor_masks[new_residency].new_residency,
-                                             service_context->read_duplicate_mask);
-            if (do_migration) {
-                status = uvm_va_block_make_resident_read_duplicate(va_block,
-                                                                   block_retry,
-                                                                   &service_context->block_context,
-                                                                   new_residency,
-                                                                   service_context->fault_region,
-                                                                   service_context->block_context.caller_page_mask,
-                                                                   UvmEventMigrationCauseCoherence);
-            }
+                                                prefetch_hint.prefetch_pages_mask,
+                                                UvmEventMigrationCauseCoherence);
+            if (status != NV_OK)
+                goto done;
         }
-        if (status != NV_OK)
-            goto done;
 
-        if (prefetch_hint.residency != UVM8_MAX_PROCESSORS) {
-            UVM_ASSERT(prefetch_hint.residency == new_residency);
-            UVM_ASSERT(prefetch_hint.prefetch_pages_mask != NULL);
+        if (service_context->read_duplicate_count != 0 &&
+            uvm_page_mask_and(service_context->block_context.caller_page_mask,
+                              service_context->per_processor_masks[new_residency].new_residency,
+                              service_context->read_duplicate_mask)) {
+            status = uvm_va_block_make_resident_read_duplicate(va_block,
+                                                               block_retry,
+                                                               &service_context->block_context,
+                                                               new_residency,
+                                                               service_context->fault_region,
+                                                               service_context->block_context.caller_page_mask,
+                                                               prefetch_hint.prefetch_pages_mask,
+                                                               UvmEventMigrationCauseCoherence);
 
-            // All prefetched pages in a block use the same read-duplicate policy
-            if (service_context->read_duplicate_count == 0) {
-                status = uvm_va_block_make_resident(va_block,
-                                                    block_retry,
-                                                    &service_context->block_context,
-                                                    new_residency,
-                                                    service_context->fault_region,
-                                                    prefetch_hint.prefetch_pages_mask,
-                                                    UvmEventMigrationCausePrefetch);
-            }
-            else {
-                status = uvm_va_block_make_resident_read_duplicate(va_block,
-                                                                   block_retry,
-                                                                   &service_context->block_context,
-                                                                   new_residency,
-                                                                   service_context->fault_region,
-                                                                   prefetch_hint.prefetch_pages_mask,
-                                                                   UvmEventMigrationCausePrefetch);
-            }
             if (status != NV_OK)
                 goto done;
-
-            uvm_page_mask_or(service_context->per_processor_masks[new_residency].new_residency,
-                             service_context->per_processor_masks[new_residency].new_residency,
-                             prefetch_hint.prefetch_pages_mask);
         }
 
         if (new_residency == UVM_CPU_ID) {
@@ -8565,6 +8727,7 @@
                                         proc,
                                         region,
                                         NULL,
+                                        NULL,
                                         UvmEventMigrationCauseInvalid);
 
     uvm_va_block_context_free(block_context);
@@ -8588,6 +8751,10 @@
     }
 
     gpu = uvm_gpu_get(proc);
+    status = uvm_mem_map_gpu_phys(src_mem, gpu);
+    if (status != NV_OK)
+        return status;
+
     dst_gpu_address = block_phys_page_copy_address(va_block, block_phys_page(proc, page_index), gpu);
     dst_gpu_address.address += page_offset;
     src_gpu_address = uvm_mem_gpu_address_physical(src_mem, gpu, 0, PAGE_SIZE);
@@ -8639,6 +8806,10 @@
     }
 
     gpu = uvm_gpu_get(proc);
+    status = uvm_mem_map_gpu_phys(dst_mem, gpu);
+    if (status != NV_OK)
+        return status;
+
     src_gpu_address = block_phys_page_copy_address(va_block, block_phys_page(proc, page_index), gpu);
     src_gpu_address.address += page_offset;
     dst_gpu_address = uvm_mem_gpu_address_physical(dst_mem, gpu, 0, PAGE_SIZE);
@@ -8780,6 +8951,7 @@
                                         UVM_CPU_ID,
                                         uvm_va_block_region_from_block(va_block),
                                         pages_to_evict,
+                                        NULL,
                                         UvmEventMigrationCauseEviction);
     if (status != NV_OK)
         goto out;
diff -ur 367.57/kernel/nvidia-uvm/uvm8_va_block.h 375.20/kernel/nvidia-uvm/uvm8_va_block.h
--- 367.57/kernel/nvidia-uvm/uvm8_va_block.h	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_va_block.h	2016-11-16 02:53:31.000000000 +0300
@@ -119,6 +119,15 @@
     // See big_ptes for the layout of these PTEs within the block.
     DECLARE_BITMAP(big_pages_swizzled, MAX_BIG_PAGES_PER_UVM_VA_BLOCK);
 
+    // Array of addresses of CPU pages mapped for physical access by the GPU.
+    //
+    // The CPU pages are mapped eagerly on the GPU whenever a new CPU page is
+    // allocated and whenever a new GPU state is allocated.
+    //
+    // The size of this array is always the same as the uvm_va_block::cpu.pages
+    // array.
+    NvU64 *cpu_pages_dma_addrs;
+
     // Array of naturally-aligned chunks. Each chunk has the largest possible
     // size which can fit within the block, so they are not uniform size.
     //
@@ -328,9 +337,9 @@
         // Per-page array of physical pages. This array scales dynamically with
         // the block size.
         //
-        // TODO: Bug 1765195: These should be some UVM CPU page data structure,
-        // complete with per-GPU IOMMU mappings when applicable. We shouldn't
-        // track CPU PFNs for the same reason.
+        // Note that the pages need to be physically mapped on each GPU before
+        // being accessed and these mappings are tracked in the GPU state,
+        // uvm_va_block_gpu_state_t::cpu_pages_dma_addrs.
         struct page **pages;
 
         // Per-page mapping bit vectors, one per bit we need to track. These are
@@ -474,6 +483,12 @@
 // If page_mask is non-NULL, the movement is further restricted to only those
 // pages in the region which are present in the mask.
 //
+// prefetch_page_mask may be passed as a subset of page_mask when cause is
+// UvmEventMigrationCauseCoherence to indicate pages that have been pulled due
+// to automatic page prefetching heuristics. For pages in this mask,
+// UvmEventMigrationCausePrefetch will be reported in migration events,
+// instead.
+//
 // This function breaks read duplication for all given pages even if they
 // don't migrate. Pages which are not resident on the destination processor
 // will also be unmapped from all existing processors, be populated in the
@@ -516,6 +531,7 @@
                                      uvm_processor_id_t dest_id,
                                      uvm_va_block_region_t region,
                                      const unsigned long *page_mask,
+                                     const unsigned long *prefetch_page_mask,
                                      UvmEventMigrationCause cause);
 
 // Similar to uvm_va_block_make_resident (read documentation there). The main
@@ -533,6 +549,7 @@
                                                     uvm_processor_id_t dest_id,
                                                     uvm_va_block_region_t region,
                                                     const unsigned long *page_mask,
+                                                    const unsigned long *prefetch_page_mask,
                                                     UvmEventMigrationCause cause);
 
 // Creates or upgrades a mapping from the input processor to the given virtual
diff -ur 367.57/kernel/nvidia-uvm/uvm8_va_range.c 375.20/kernel/nvidia-uvm/uvm8_va_range.c
--- 367.57/kernel/nvidia-uvm/uvm8_va_range.c	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_va_range.c	2016-11-16 02:53:31.000000000 +0300
@@ -807,6 +807,9 @@
     uvm_mutex_unlock(&va_range->semaphore_pool.tracker_lock);
     if (status != NV_OK)
         UVM_ASSERT(status == uvm_global_get_status());
+
+    uvm_mem_unmap_gpu_phys(va_range->semaphore_pool.mem, gpu);
+
     va_range->semaphore_pool.gpu_attrs[gpu->id - 1] = va_range->semaphore_pool.default_gpu_attrs;
     if (va_range->semaphore_pool.owner == gpu)
         va_range->semaphore_pool.owner = NULL;
diff -ur 367.57/kernel/nvidia-uvm/uvm8_va_range.h 375.20/kernel/nvidia-uvm/uvm8_va_range.h
--- 367.57/kernel/nvidia-uvm/uvm8_va_range.h	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_va_range.h	2016-11-16 02:53:31.000000000 +0300
@@ -530,11 +530,16 @@
          (va_range);                                                            \
          (va_range) = uvm_va_space_iter_next_contig((va_range), (end)))
 
+#define uvm_for_each_va_range_in_contig_from(va_range, va_space, first_va_range, end) \
+    for ((va_range) = (first_va_range);                                               \
+         (va_range);                                                                  \
+         (va_range) = uvm_va_space_iter_next_contig((va_range), (end)))
+
 // Like uvm_for_each_va_range_in_contig but also stops iteration if any VA range
 // has a type other than UVM_VA_RANGE_TYPE_MANAGED.
-#define uvm_for_each_managed_va_range_in_contig(va_range, va_space, start, end)    \
+#define uvm_for_each_managed_va_range_in_contig(va_range, va_space, start, end) \
     for ((va_range) = uvm_va_space_iter_first((va_space), (start), (start));    \
-         (va_range) && (va_range)->type == UVM_VA_RANGE_TYPE_MANAGED;              \
+         (va_range) && (va_range)->type == UVM_VA_RANGE_TYPE_MANAGED;           \
          (va_range) = uvm_va_space_iter_next_contig((va_range), (end)))
 
 #define uvm_for_each_va_range_in_vma(va_range, vma)             \
diff -ur 367.57/kernel/nvidia-uvm/uvm8_va_space.c 375.20/kernel/nvidia-uvm/uvm8_va_space.c
--- 367.57/kernel/nvidia-uvm/uvm8_va_space.c	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_va_space.c	2016-11-16 02:53:31.000000000 +0300
@@ -34,6 +34,7 @@
 #include "uvm_common.h"
 #include "nv_uvm_interface.h"
 #include "nv-kthread-q.h"
+#include "uvm8_next_decl.h"
 
 static void disable_peers(uvm_va_space_t *va_space,
                           uvm_gpu_t *gpu0,
@@ -66,6 +67,7 @@
     uvm_init_rwsem(&va_space->lock, UVM_LOCK_ORDER_VA_SPACE);
     uvm_mutex_init(&va_space->serialize_writers_lock, UVM_LOCK_ORDER_VA_SPACE_SERIALIZE_WRITERS);
     uvm_range_tree_init(&va_space->va_range_tree);
+    uvm_next_va_space_init(va_space);
 
     // By default all struct files on the same inode share the same
     // address_space structure (the inode's) across all processes. This means
diff -ur 367.57/kernel/nvidia-uvm/uvm8_va_space.h 375.20/kernel/nvidia-uvm/uvm8_va_space.h
--- 367.57/kernel/nvidia-uvm/uvm8_va_space.h	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm8_va_space.h	2016-11-16 02:53:31.000000000 +0300
@@ -36,6 +36,15 @@
 #include "uvm8_perf_module.h"
 #include "uvm8_va_block_types.h"
 
+#if UVM_IS_NEXT()
+    #include "uvm8_va_space_next.h"
+#else
+    struct uvm_va_space_next_data_struct
+    {
+        NvU64 not_used;
+    };
+#endif
+
 // uvm_deferred_free_object provides a mechanism for building and later freeing
 // a list of objects which are owned by a VA space, but can't be freed while the
 // VA space lock is held.
@@ -214,6 +223,9 @@
     // Mask of processors that are participating in system-wide atomics
     uvm_processor_mask_t system_wide_atomics_enabled_processors;
 
+    // Nextchip specific data structure
+    uvm_va_space_next_data_t next_data;
+
     // Array of GPU VA spaces
     uvm_gpu_va_space_t *gpu_va_spaces[UVM8_MAX_GPUS];
 
diff -ur 367.57/kernel/nvidia-uvm/uvm_common.h 375.20/kernel/nvidia-uvm/uvm_common.h
--- 367.57/kernel/nvidia-uvm/uvm_common.h	2016-10-04 05:37:31.000000000 +0300
+++ 375.20/kernel/nvidia-uvm/uvm_common.h	2016-11-16 02:53:32.000000000 +0300
@@ -187,6 +187,7 @@
         ((x) + _a - 1) & ~(_a - 1);     \
     })
 
+#define UVM_PAGE_ALIGN_UP(value) UVM_ALIGN_UP(value, PAGE_SIZE)
 #define UVM_PAGE_ALIGN_DOWN(value) UVM_ALIGN_DOWN(value, PAGE_SIZE)
 
 // These macros provide a convenient way to string-ify enum values.
